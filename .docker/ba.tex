
% !TeX program = pdflatex
\documentclass[12pt,a4paper,oneside]{scrreprt}

% ---------- Packages ----------
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{lmodern}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{tabularx}
\newcolumntype{Y}{>{\raggedright\arraybackslash}X}
\usepackage{array}
\usepackage{enumitem}
\usepackage{csquotes}
\usepackage[printonlyused]{acronym}
\usepackage{tocbasic} % better control of ToC lists
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{titling}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{float}
\usepackage{pgf-pie}
\setlength{\parindent}{0pt} 
\setlength{\parskip}{6pt}

% ---------- Page Setup ----------
\geometry{left=30mm,right=20mm,top=20mm,bottom=30mm}
\onehalfspacing
\setkomafont{sectioning}{\normalfont\bfseries}
\setkomafont{disposition}{\normalfont\bfseries}
\renewcommand{\arraystretch}{1.2}

% ---------- Header / Footer ----------
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[C]{\thepage\ / \pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}

% --- Seitenränder ---
\usepackage[a4paper, left=3cm, right=2cm, top=2cm, bottom=3cm]{geometry}

% --- Kopf- und Fußzeilen ---
\usepackage{fancyhdr}
\usepackage{graphicx} % Für das Logo
\usepackage{lastpage} % Für "Seite X von Y" falls benötigt

% --- Stil für alle Seiten ---
\pagestyle{fancy}

% Kopfzeile leeren
\fancyhead{}
\fancyfoot{}

% --- Kopfzeile: links = Kapitelname, rechts = Logo ---
\fancyhead[L]{\nouppercase{\leftmark}} % aktueller Kapitelname
\fancyhead[R]{\includegraphics[height=1cm]{Bilder/TH_Nuernberg_Logo.png}} % passe den Pfad an!

% --- Fußzeile: Seitenzahlen zentriert ---
\fancyfoot[C]{\thepage}

% --- Linien unter/über Kopf- und Fußzeile ---
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

% --- Optional: Stil für Kapitelanfangsseiten (ohne Logo) ---
\fancypagestyle{plain}{
  \fancyhead{}
  \fancyfoot[C]{\thepage}
  \renewcommand{\headrulewidth}{0pt}
}

% ---------- Metadata ----------
\newcommand{\arbeitstitel}{Entwicklung und Evaluierung von Algorithmen zur Umfelderkennung auf Basis hochauflösender 360°-LiDAR-Sensordaten}
\newcommand{\autor}{Ingrid Nodem Lambou}
\newcommand{\erstpruefer}{Prof. Dr. Christina Singer}
\newcommand{\zweitpruefer}{Prof. Dr. Christian Pfitzner}
\newcommand{\ort}{Nürnberg}
\newcommand{\abgabedatum}{30.08.2025}
\newcommand{\studiengang}{Mechatronik/Feinwerktechnik (B.\,Eng.)}
\newcommand{\matrikelnr}{3584699}
\newcommand{\studienschwerpunkt}{\textit{(eintragen)}} % Placeholder
\newcommand{\firma}{\textit{(eintragen)}} % Placeholder
\newcommand{\firmenbetreuer}{\textit{(Name, Abt., Tel.-Nr.)}} % Placeholder
\newcommand{\ausgabedatum}{\textit{(eintragen)}} % Placeholder

%-----------Literatur--------------
%-----------Literatur--------------
\usepackage[backend=biber,style=authoryear,sorting=nyt,maxbibnames=99]{biblatex}
\addbibresource{literatur.bib} % nom exact du .bib

%--Document can have many numberedd subsection---
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3} % (optionnel : pour qu'elles apparaissent aussi dans la table des matières)

% --- TikZ für Flussdiagramme ---
\usepackage[utf8]{inputenc}  % encodage
\usepackage[T1]{fontenc}     % caractères accentués
\usepackage[ngerman]{babel}  % ou [french] selon la langue
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, positioning}

% --- Stile für Flowchart-Knoten und Pfeile ---
\tikzset{
  startstop/.style = {rectangle, rounded corners, minimum width=3.2cm,
    minimum height=1.0cm, text centered, draw=black, fill=red!30},
  process/.style   = {rectangle, minimum width=3.6cm, minimum height=1.0cm,
    text centered, draw=black, fill=orange!30},
  arrow/.style     = {thick,->,>=stealth}
}

% ---------- Document ----------
\begin{document}
\pagenumbering{Roman}

% ====== Offizielles Deckblatt (Hochschule) ======
\begin{titlepage}
    \centering
    \vspace*{1cm}
    {\Large\textbf{Bachelorarbeit}}\\[1.5cm]
    {\LARGE\textbf{\arbeitstitel}}\\[1.5cm]
    \begin{tabular}{@{}ll@{}}
        \textbf{Autor:} & \autor \\
        \textbf{Erstgutachter:} & \erstpruefer \\
        \textbf{Zweitgutachter:} & \zweitpruefer \\
        \textbf{Ort, Abgabetermin:} & \ort, \abgabedatum \\
    \end{tabular}
    \vfill
\end{titlepage}
\cleardoublepage

% ====== Offizielles Deckblatt (Formular) ======
\begin{titlepage}
    \centering
    {\Large\textbf{Offizielles Deckblatt Bachelorarbeit}}\\[1.2cm]
    \begin{tabular}{@{}p{5cm}p{9cm}@{}}
        \textbf{Bearbeiter:} & \autor \dotfill \\[0.3cm]
        \textbf{Matrikel-Nr.:} & \matrikelnr \dotfill \\[0.3cm]
        \textbf{Studiengang:} & \studiengang \dotfill \\[0.3cm]
        \textbf{Studienschwerpunkt:} & \studienschwerpunkt \dotfill \\[0.3cm]
        \textbf{Erstprüfer:} & \erstpruefer \dotfill \\[0.3cm]
        \textbf{Zweitprüfer:} & \zweitpruefer \dotfill \\[0.3cm]
        \textbf{Durchgeführt bei der Firma:} & \firma \dotfill \\[0.3cm]
        \textbf{Betreuer innerhalb der Firma:} & \firmenbetreuer \dotfill \\[0.3cm]
        \textbf{Ausgabedatum:} & \ausgabedatum \dotfill \\[0.3cm]
        \textbf{Abgabedatum:} & \abgabedatum \dotfill \\[0.8cm]
        \textbf{Thema der Arbeit:} & \arbeitstitel \\[0.8cm]
        \textbf{Die Arbeit ist frei einsehbar:} & $\Box$ Ja \quad $\Box$ Nein \\[0.8cm]
        \textbf{Die Arbeit darf nur mit Zustimmung von:} & \textit{(bei Firmenarbeiten Name, Abt., Tel.-Nr.)} \dotfill \\
    \end{tabular}
    \vfill
\end{titlepage}
\cleardoublepage

% ====== Prüfungsrechtliche ErklÃ¤rung ======
\chapter*{Prüfungsrechtliche Erklärung}
Hiermit versichere ich, dass ich die vorliegende Bachelorarbeit selbstständig verfasst und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt habe. Alle Stellen, die wörtlich oder sinngemäß aus veröffentlichten oder nicht veröffentlichten Quellen entnommen wurden, sind als solche kenntlich gemacht. Diese Arbeit wurde in gleicher oder ähnlicher Form noch keiner anderen Prüfungsbehörde vorgelegt.\\[1.2cm]
\textbf{Nürnberg, \abgabedatum}\\[0.8cm]
\autor \dotfill
\cleardoublepage

% ====== Kurzfassung / Zusammenfassung ======
\chapter*{Kurzfassung}
\addcontentsline{toc}{chapter}{Kurzfassung}
\noindent
\textit{Hier Kurzzusammenfassung in deutscher Sprache einfügen.} % Placeholder

\cleardoublepage

% ====== Inhalts- und Verzeichnisse ======
\tableofcontents
\cleardoublepage

\listoffigures
\addcontentsline{toc}{chapter}{Abbildungsverzeichnis}
\cleardoublepage

\listoftables
\addcontentsline{toc}{chapter}{Tabellenverzeichnis}
\cleardoublepage

\chapter*{Abkürzungsverzeichnis}
\addcontentsline{toc}{chapter}{Abkürzungsverzeichnis}
\begin{acronym}[LiDAR]
    \acro{LiDAR}{Light Detection and Ranging}
    \acro{ROS}{Robot Operating System}
    \acro{ToF}{Time of Flight}
    % \acro{PCL}{Point Cloud Library}
    % weitere AbkÃ¼rzungen hier ergÃ¤nzen
\end{acronym}
\cleardoublepage

\pagenumbering{arabic}

% ====== 1 Einleitung ======
\chapter{Einleitung}
\section{Motivation}
Die Anforderungen an automatisierte und autonome Systeme in Fahrzeugen und mobilen Robotern steigen kontinuierlich. Insbesondere in urbanen Gebieten wird eine präzise und zuverlässige Umfelderkennung benötigt, um die Sicherheit im Straßenverkehr zu erhöhen und den Verkehrsfluss zu optimieren. LiDAR-Sensoren bieten hierbei einen entscheidenden Vorteil, da sie unabhängig von den Lichtverhältnissen eine hochauflösende, dreidimensionale Erfassung der Umgebung ermöglichen.

Die fortschreitende Entwicklung von LiDAR-Technologien hat dazu geführt, dass moderne Sensoren wie der Ouster OS1 eine 360°-Rundumsicht mit hoher Winkelauflösung und großer Reichweite bereitstellen. Diese Eigenschaften eröffnen neue Möglichkeiten für die präzise Erkennung und Klassifizierung von Objekten sowie für die robuste Segmentierung von Fahrbahnen und Hindernissen.

Trotz dieser technologischen Fortschritte sind die effiziente Verarbeitung großer Datenmengen und die Entwicklung robuster Algorithmen nach wie vor zentrale Herausforderungen. Die steigende Komplexität der Verkehrssituationen, die zunehmende Verkehrsdichte und die hohen Anforderungen an die Echtzeitfähigkeit erfordern kontinuierliche Forschungs- und Entwicklungsarbeit in diesem Bereich.

Aus diesen Gründen wurde das Thema \enquote{\arbeitstitel} gewählt. Ziel ist es, einen Beitrag zur Verbesserung der Sicherheit und Zuverlässigkeit automatisierter Systeme zu leisten und aktuelle Ansätze weiterzuentwickeln, um den steigenden Anforderungen moderner Mobilitätskonzepte gerecht zu werden.

\section{Zielsetzung}
Aufbauend auf der Motivation verfolgt diese Arbeit das Ziel, Algorithmen zur effizienten und robusten Umfelderkennung auf Basis hochauflösender 360\textdegree{}-LiDAR-Daten zu entwickeln und zu evaluieren. Ein besonderer Schwerpunkt liegt dabei auf der Vorverarbeitung der Sensordaten, um eine möglichst präzise Segmentierung des Bodens und eine zuverlässige Erkennung relevanter Objekte zu ermöglichen.

Die Implementierung und Validierung der Algorithmen erfolgt unter Verwendung des \ac{ROS}~2, um eine modulare und echtzeitfähige Datenverarbeitung zu gewährleisten.

\section{Forschungsfragen}
Im Rahmen dieser Arbeit sollen insbesondere folgende Forschungsfragen untersucht werden:
\begin{enumerate}[label=\textbf{F\arabic*})]
    \item Wie können die Rohdaten eines Ouster-OS1-LiDAR-Sensors effizient vorverarbeitet werden, um Rauschen zu reduzieren und die Datenqualität für nachfolgende Verarbeitungsschritte zu erhöhen?
    \item Welche Verfahren eignen sich am besten zur robusten Bodensegmentierung in unterschiedlichen Umgebungsbedingungen und wie wirken sich verschiedene Ansätze auf die Gesamteffizienz des Systems aus?
    \item Wie kann die Objekterkennung auf Basis der vorverarbeiteten und segmentierten LiDAR-Daten optimiert werden, um relevante Objekte sicher zu erkennen und zu klassifizieren?
    \item Wie lassen sich die entwickelten Algorithmen innerhalb einer \ac{ROS}~2-Architektur effizient integrieren und in Echtzeit ausführen?
    \item Wie unterscheiden sich die Ergebnisse der Umfelderkennung bei unterschiedlichen Szenarien und welche Grenzen bestehen hinsichtlich der Übertragbarkeit auf reale Anwendungen?
\end{enumerate}

\section{Methodische Vorgehensweise}
Zur Beantwortung der Forschungsfragen und Erreichung der definierten Ziele wird folgendes methodisches Vorgehen verfolgt:
\begin{enumerate}
    \item \textbf{Literaturrecherche:} Analyse aktueller wissenschaftlicher Veröffentlichungen und technischer Berichte zu LiDAR-basierten Verfahren der Datenvorverarbeitung, Bodensegmentierung und Objekterkennung.
    \item \textbf{Anforderungsanalyse:} Ableitung technischer und funktionaler Anforderungen unter Berücksichtigung der Spezifikationen des Ouster OS1 und der Einsatzumgebung.
    \item \textbf{Datenaufnahme:} Durchführung eigener Messungen mit dem Ouster OS1 in unterschiedlichen Umgebungen zur Erzeugung repräsentativer Datensätze.
    \item \textbf{Vorverarbeitung:} Implementierung und Evaluierung von Methoden zur Filterung, Rauschunterdrückung und Formatierung der Rohdaten.
    \item \textbf{Bodensegmentierung:} Auswahl, Anpassung und Vergleich geeigneter Algorithmen zur robusten Trennung von Boden- und Nicht-Bodenpunkten.
    \item \textbf{Objekterkennung:} Entwicklung und Integration von Verfahren zur Detektion und Klassifikation relevanter Objekte.
    \item \textbf{Integration \& Evaluation:} Einbindung der Module in eine \ac{ROS}~2-Architektur, Durchführung von Testszenarien und Bewertung hinsichtlich Genauigkeit, Robustheit und Echtzeitfähigkeit.
\end{enumerate}

% ====== 2 Wissenschaftliche Grundlagen ======
\chapter{Stand der Technik und Wissenschaft}

Die Umfeldwahrnehmung eines Fahrzeugs basiert in modernen 
Fahrerassistenzsystemen und im autonomen Fahren auf einer Vielzahl 
von Sensortechnologien. Neben Radar und Kamerasystemen 
hat sich insbesondere die LiDAR-Sensorik (\emph{Light Detection and Ranging}) 
als Schlüsseltechnologie etabliert.

\noindent Die Entwicklung von LiDAR begann in den 1960er-Jahren, zunächst für 
Anwendungen in der Geodäsie, Fernerkundung und Atmosphärenforschung. 
Mit dem technologischen Fortschritt in der Lasertechnik sowie der 
Miniaturisierung optoelektronischer Komponenten fand die LiDAR-Sensorik 
in den 2000er-Jahren zunehmend Eingang in die Automobilindustrie. 
Heute gilt sie als eine der zentralen Technologien für Fahrerassistenzsysteme 
und hochautomatisiertes Fahren.

\noindent Kameras liefern hochauflösende Bildinformationen, die für die 
Objekterkennung und Klassifizierung besonders geeignet sind, zeigen jedoch 
Schwächen bei schlechten Lichtverhältnissen oder starker Blendung. 
Radarsensoren dagegen arbeiten robust unter verschiedenen Wetterbedingungen 
und liefern präzise Informationen über die Relativgeschwindigkeit, verfügen 
jedoch nur über eine eingeschränkte Winkelauflösung. LiDAR kombiniert die 
Vorteile beider Systeme teilweise: Es liefert hochgenaue 
Distanzinformationen mit einer hohen Winkelauflösung und erzeugt 
dadurch eine dreidimensionale Punktwolke der Fahrzeugumgebung.

\noindent Im Kontext des autonomen Fahrens ermöglicht LiDAR eine 
präzise Abbildung des Umfelds, die sowohl für die Hinderniserkennung 
als auch für die simultane Lokalisierung und Kartierung (SLAM) genutzt 
werden kann. Aufgrund seiner Eigenschaften wird LiDAR häufig als 
unverzichtbare Ergänzung zu Kamera und Radar betrachtet, da es zusätzliche 
Redundanz und Genauigkeit im Sensor-Stack schafft.

\noindent Ziel dieses Kapitels ist es, die wissenschaftlich-technischen Grundlagen von LiDAR darzustellen und den Bezug zu seiner praktischen Anwendung im Fahrzeug herzustellen. Nach einer Beschreibung des Sensoraufbaus (Abschnitt~\ref{sec:sensoraufbau}) werden die Wellenlängenbereiche der Laserquelle erläutert (Abschnitt~\ref{sec:wellenlaenge}). Anschließend folgt das Messprinzip (Abschnitt~\ref{sec:messprinzip}), bevor typische Anwendungsszenarien im Fahrzeugumfeld (Abschnitt~\ref{sec:anwendung}) 
behandelt werden. Abschließend werden die spezifischen Eigenschaften des in dieser Arbeit verwendeten Sensors, des Ouster OS1, vorgestellt (Abschnitt~\ref{sec:ouster}).

\section{Sensoraufbau}
\label{sec:sensoraufbau}

Der Aufbau eines LiDAR-Sensors lässt sich in mehrere funktionale 
Komponenten gliedern, die im Signalpfad von der Lichtquelle bis 
zur Auswerteeinheit angeordnet sind (vgl. Abb.~\ref{fig:sensoraufbau}). 

\subsection {Sendeoptik und Strahlablenkung}
Vom Ausgang der Laserquelle gelangt das Licht zunächst in die Sendeoptik. 
Diese formt das Intensitätsprofil des Strahls und bestimmt somit indirekt 
die Auflösung des Sensors. Häufig werden refraktive Linsen oder diffraktive 
optische Elemente (DOE) eingesetzt, welche die gewünschte Strahlform erzeugen. 
Für rasternde Systeme übernehmen Spiegel oder Prismen die Strahlablenkung durch 
rotierende oder oszillierende Bewegungen. Im Gegensatz dazu nutzen sogenannte 
Flash-Systeme DOEs oder Diffusoren, um die Szene pro Aufnahme vollständig auszuleuchten.  

Die Strahlablenkung ist ein zentrales Element zur Erzeugung der Winkelinformation. 
Sie legt fest, welcher Raumwinkel in einem bestimmten Zeitpunkt erfasst wird, 
und ermöglicht somit die sukzessive Abtastung der Umgebung.  

\subsection {Empfangsoptik und Lichtsensor}
Der ausgesendete Strahl wird auf dem Weg durch die Atmosphäre beeinflusst, 
am Objekt reflektiert und anschließend von der Empfangsoptik aufgenommen. 
Ihre Größe und Transmission haben direkten Einfluss auf die Signalstärke 
gemäß der LiDAR-Gleichung. Auch auf dem Empfangspfad können Strahlablenkungen 
auftreten, wie sie beispielsweise bei Drehspiegelsystemen üblich sind.  

Um Reflexionen zu minimieren und die Transmission zu erhöhen, sind optische 
Komponenten in der Regel mit Antireflexbeschichtungen versehen. Die 
Durchlässigkeit wird auf den Wellenlängenbereich der Quelle abgestimmt, 
um Rauschen durch Umgebungslicht zu reduzieren.  

Das aufgefangene Signal wird schließlich vom Lichtsensor detektiert. 
Typischerweise kommen Avalanche-Photodioden (APD) zum Einsatz, die durch 
ihren internen Verstärkungsmechanismus eine hohe Empfindlichkeit für 
schwache Signale bieten.  

\subsection {Auslese- und Steuerungselektronik}
Die vom Detektor erzeugten elektrischen Signale werden durch die 
Ausleseelektronik verarbeitet. Sie umfasst Verstärker, Analog-Digital-Wandler (ADC), 
Zeit-zu-Digital-Wandler (TDC) und Multiplexer. Ihre Aufgabe besteht darin, 
die Signale zu digitalisieren und die Laufzeitinformationen für jeden 
Messpunkt zu bestimmen. Moderne Systeme berechnen auf der Sensorebene 
bereits Distanzwerte und können weitergehende Vorverarbeitungen wie die 
Bildung von Objektboxen oder sogar erste Klassifikationen durchführen.  

Eine zentrale Rolle spielt die Steuerungselektronik, welche die Synchronisation 
aller Komponenten sicherstellt. Sie sorgt dafür, dass der Lichtsensor nur während 
des Aussendens eines Laserpulses aktiv ist, um unnötige Rauschsignale durch 
Umgebungslicht zu vermeiden. Ebenso steuert sie die Strahlablenkung, sodass 
jeder Messpunkt dem korrekten Raumwinkel zugeordnet werden kann.  

Der LiDAR-Sensoraufbau vereint optische, elektronische und algorithmische 
Bausteine zu einem Gesamtsystem, das die präzise Abbildung der Fahrzeugumgebung 
in Echtzeit ermöglicht.  

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.95\textwidth]{Bilder/Sensoraufbau.png}
    \caption{Schematischer Aufbau eines LiDAR-Sensors mit Sendeoptik, 
    Empfangsoptik, Strahlablenkung, Steuer- und Auswerteeinheit.}
    \label{fig:sensoraufbau}
\end{figure}


\section{Wellenlängenbereiche}
\label{sec:wellenlaenge}
Zur Umsetzung dieses Prinzips werden überwiegend Laser im nahen Infrarotbereich 
(NIR) eingesetzt, typischerweise mit Wellenlängen zwischen $870\,\text{nm}$ und 
$950\,\text{nm}$. Dieser Bereich ist besonders geeignet, da die spektrale 
Strahlungsintensität des Sonnenlichts dort vergleichsweise gering ist und somit 
ein günstiges Signal-Rausch-Verhältnis erzielt werden kann. Gleichzeitig stehen in diesem Wellenlängenbereich sowohl kosteneffiziente Halbleiterlaser als auch empfindliche Siliziumdetektoren zur Verfügung. Für spezielle Anwendungen werden 
auch Wellenlängen um $1064\,\text{nm}$ oder $1550\,\text{nm}$ eingesetzt, da hier 
höhere Laserleistungen innerhalb der Augensicherheitsgrenzen möglich sind. Diese 
erfordern jedoch alternative Detektormaterialien, was den technischen Aufwand und 
die Kosten erhöht.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{Bilder/Spektrum der elektromagnetischen Strahlung.png}
    \caption{Spektrum der elektromagnetischen Strahlung}
    \label{fig:ouster}
\end{figure}

\section{Messprinzip}
\label{sec:messprinzip}

Das Messprinzip moderner LiDAR-Systeme basiert überwiegend auf dem sogenannten 
\emph{Time-of-Flight} (ToF)-Verfahren, das sich aufgrund seiner Robustheit und 
hohen Genauigkeit als Standard etabliert hat. Beim ToF-Verfahren wird ein kurzer 
Laserimpuls ausgesendet, der sich mit Lichtgeschwindigkeit 
$c \approx 3 \cdot 10^{8}\,\text{m/s}$ ausbreitet, von einem Objekt reflektiert wird und schließlich den Empfänger erreicht. Aus der gemessenen Zeitdifferenz $\Delta t$ zwischen Emission und Detektion ergibt sich die Entfernung $d$ nach folgender Gleichung:

\begin{equation}
    d = \frac{c \cdot \Delta t}{2}
\end{equation}

\noindent Der Faktor $\tfrac{1}{2}$ berücksichtigt, dass der Lichtimpuls den Weg zweimal zurücklegt – vom Sender zum Objekt und wieder zurück zum Empfänger.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{Bilder/Schematisches Beispiel eines direct ToF.png}
    \caption{Schematisches Beispiel eines direct Time-Of-Flight (dToF)}
    \label{fig:ouster}
\end{figure}

\section{Typische Anwendungsszenarien im Fahrzeugumfeld}
\label{sec:anwendung}

\section{Ouster OS1 am Opel Astra der TH Nürnberg}
\subsection{Spezifische Eigenschaften des Ouster OS1}
\label{sec:ouster}

Der in dieser Arbeit verwendete Sensor ist der Ouster OS1, ein 
360°-LiDAR der mittleren Reichweitenklasse. Der Sensor wurde 
ausgewählt, da er durch seine Konfiguration eine hohe Punktdichte 
bereitstellt und damit eine geeignete Datengrundlage für die 
Umfelderkennung liefert. Die erzeugten Punktwolken bilden die Basis 
für die in dieser Arbeit entwickelten Algorithmen zur Objekterkennung 
und -verfolgung.  

Der OS1 arbeitet mit einer Laserwellenlänge von 865\,nm und einem 
horizontalen Sichtfeld von 360° sowie einem vertikalen Sichtfeld 
von 42,4°. Je nach Konfiguration kann der Sensor mit 32, 64 oder 
128 vertikalen Kanälen betrieben werden. Pro Umdrehung stehen 
horizontal 512, 1024 oder 2048 Messpunkte zur Verfügung. In 
Abhängigkeit von der Konfiguration werden zwischen 1,3 und 
5,2 Millionen Messpunkte pro Sekunde erzeugt.  

Die Reichweite beträgt bis zu 170\,m bei Zielen mit hoher 
Reflektivität und etwa 90\,m bei dunklen Oberflächen unter 
starker Sonneneinstrahlung. Die Distanzmessung weist eine Genauigkeit 
von etwa ±2,5\,cm auf, mit einer typischen Präzision zwischen ±0,5 
und ±3\,cm. Darüber hinaus kann der Sensor bis zu zwei Echos pro Puls 
registrieren, wodurch auch teilweise transparente Objekte erfasst werden.  

Die vom Sensor ausgegebenen Daten enthalten neben der Distanz 
auch Intensität, Reflexivität, Kanalnummer, Azimutwinkel sowie 
einen Zeitstempel. Zusätzlich ist eine inertiale Messeinheit (IMU) 
integriert, die Bewegungsdaten mit 100\,Hz bereitstellt.  
Die Ausgabe erfolgt über Gigabit-Ethernet.  

Das Gehäuse des Sensors hat einen Durchmesser von 87\,mm, eine Höhe 
von 58--74\,mm und ein Gewicht zwischen 430\,g und 520\,g. 
Mit den Schutzarten IP68 und IP69K ist der OS1 für den Betrieb 
unter verschiedenen Umweltbedingungen ausgelegt. Die spezifizierte 
Betriebstemperatur reicht von --40\,°C bis +60\,°C.  

Der OS1 liefert damit die Rohdaten in Form von dreidimensionalen 
Punktwolken, die im weiteren Verlauf dieser Arbeit verarbeitet und 
in semantisch nutzbare Objekte überführt werden.  

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{Bilder/Ouster.png}
    \caption{Ouster OS1.}
    \label{fig:ouster_os1}
\end{figure}

rviz2
ros2
bashfile

\subsection{Sensorpositionnierung am Opel Astra}

\chapter{Systemarchitektur}

In diesem Kapitel wird die technische Systemarchitektur beschrieben, die der Entwicklung der Umfelderkennungs­pipeline zugrunde liegt. 
Sie umfasst sowohl die Software- als auch die Hardwarekomponenten, die für die Erfassung, Verarbeitung und Auswertung der Sensordaten verwendet wurden. 
Zunächst werden die Grundlagen des Robot Operating System~2~(ROS~2) erläutert, das als Middleware zur Kommunikation zwischen den einzelnen Modulen dient. 
Anschließend wird die unter Linux~Ubuntu~22.04 eingerichtete Arbeitsumgebung vorgestellt, in der die Implementierung und Tests durchgeführt wurden. 
Abschließend erfolgt ein Überblick über das Gesamtsystem mit den definierten Anforderungen, der Datenerfassung und der praktischen Umsetzung der entwickelten Messkette.


\section{Grundlagen von ROS2}
\label{sec:ros2_basics}

Das \textit{Robot Operating System 2 (ROS2)} ist ein quelloffenes Framework, das speziell für die Entwicklung modularer und verteilter Softwaresysteme in der Robotik entwickelt wurde. 
Es ermöglicht eine flexible Kommunikation zwischen verschiedenen Softwarekomponenten und bietet eine Vielzahl von Tools und Bibliotheken, um Sensordaten zu verarbeiten, Aktoren anzusteuern und komplexe Algorithmen für autonome Systeme umzusetzen. 
ROS2 basiert auf einer Middleware-Architektur (DDS – \textit{Data Distribution Service}), die eine zuverlässige und echtzeitfähige Kommunikation zwischen den Komponenten ermöglicht. 
Dank seiner Skalierbarkeit und Plattformunabhängigkeit wird ROS2 sowohl in der Forschung als auch in der industriellen Anwendung eingesetzt, beispielsweise in autonomen Fahrzeugen, mobilen Robotern oder industriellen Inspektionssystemen.

In den folgenden Unterabschnitten werden die zentralen Konzepte von ROS2 näher erläutert:  
Zunächst wird das Prinzip der \textit{Nodes} beschrieben, die als eigenständige Prozesse die Grundbausteine eines ROS2-Systems bilden.  
Anschließend werden die \textit{Topics} vorgestellt, über welche die Kommunikation zwischen diesen Knoten erfolgt.  
Darauf folgt eine Beschreibung der Struktur der \textit{Nachrichten (Messages)}, die den Datenaustausch ermöglichen.  
Ferner wird das Konzept der \textit{Bags} erläutert, die zur Aufzeichnung und Wiedergabe von Sensordaten dienen.  
Abschließend wird das Visualisierungstool \textit{RViz2} vorgestellt, das zur Analyse und Überprüfung der gesammelten Daten in einer 3D-Umgebung eingesetzt wird.

\subsection{Nodes}
\label{sec:ros2_nodes}

In ROS2 stellen \textit{Nodes} die grundlegenden Ausführungseinheiten des Systems dar. 
Jeder Node repräsentiert einen eigenständigen Prozess, der eine bestimmte Funktion erfüllt – beispielsweise das Erfassen von Sensordaten, die Datenverarbeitung oder die Ansteuerung von Aktoren. 
Durch diese modulare Struktur können komplexe Systeme in überschaubare, wiederverwendbare Softwarekomponenten unterteilt werden, die über klar definierte Schnittstellen miteinander interagieren(~\cite{ros2_docs}).

\subsection{Topics}
\label{sec:ros2_topics}

Die Kommunikation zwischen Nodes erfolgt in ROS2 hauptsächlich über sogenannte \textit{Topics}. 
Ein Topic ist ein benannter Kommunikationskanal, über den Nachrichten nach dem \textit{Publish/Subscribe}-Prinzip ausgetauscht werden. 
Ein Node kann Daten auf einem Topic veröffentlichen (\textit{publish}) oder von diesem empfangen (\textit{subscribe}). 
Diese Entkopplung der Kommunikation ermöglicht eine hohe Flexibilität und Skalierbarkeit in verteilten Systemen(~\cite{ros2_docs}).

\subsection{Nachrichten}
\label{sec:ros2_messages}

Die über Topics ausgetauschten Informationen werden in Form von \textit{Nachrichten (Messages)} übertragen. 
Eine Nachricht besteht aus einer definierten Datenstruktur, die verschiedene Datentypen wie Ganzzahlen, Gleitkommazahlen, Arrays oder benutzerdefinierte Typen enthalten kann. 
Diese klar definierte Struktur ermöglicht einen standardisierten und sicheren Datenaustausch zwischen Nodes, unabhängig von der zugrunde liegenden Programmiersprache(~\cite{ros2_docs}).

\subsection{Bags}
\label{sec:ros2_bags}

\textit{ROS2 Bags} dienen zur Aufzeichnung, Speicherung und Wiederverwendung von Nachrichten, die über Topics ausgetauscht werden. 
Mit diesem Werkzeug können Sensordaten, Steuerinformationen oder Simulationsergebnisse gespeichert und zu einem späteren Zeitpunkt erneut abgespielt werden. 
Dies ist insbesondere für die Analyse, das Debugging und die Entwicklung von Algorithmen nützlich, wenn reproduzierbare Datensätze benötigt werden(~\cite{ros2_docs}).

\subsection{RViz2}
\label{sec:ros2_rviz2}

\textit{RViz2} ist ein Visualisierungstool, das zur Darstellung und Analyse der in ROS2 verarbeiteten Daten verwendet wird. 
Es ermöglicht die dreidimensionale Visualisierung von Punktwolken, Robotermodellen, Trajektorien und Sensordatenströmen in Echtzeit. 
Durch seine interaktive Benutzeroberfläche unterstützt RViz2 die Überprüfung von Systemzuständen, die Fehlersuche und die Entwicklung komplexer Algorithmen in der Robotik(~\cite{ros2_docs}).

\subsection{QoS}


\section{Point Cloud Library (PCL)}
\label{chap:pcl}

Die \textit{Point Cloud Library (PCL)} ist eine quelloffene C++-Bibliothek zur effizienten Verarbeitung und Analyse von dreidimensionalen Punktwolken. 
In dieser Arbeit bildet sie die Grundlage der Umfelderkennung, da sie eine Vielzahl von Funktionen zur Filterung, Segmentierung, Clusterbildung und geometrischen Modellierung bereitstellt (\cite{pcl_docs_2025}). 
PCL wird in der ROS~2-Umgebung über Schnittstellenpakete wie \texttt{pcl\_ros} und \texttt{pcl\_conversions} eingebunden, wodurch eine direkte Umwandlung zwischen den ROS-Datenstrukturen (\texttt{sensor\_msgs/PointCloud2}) und den internen PCL-Formaten (\texttt{pcl::PointCloud<T>}) ermöglicht wird. 

\section{Arbeitsumgebung: Linux Ubuntu 22.04}

Für die Entwicklung und Ausführung der im Rahmen dieser Arbeit implementierten ROS~2-Pipeline wurde das Betriebssystem \textbf{Ubuntu~22.04~LTS (Jammy~Jellyfish)} eingesetzt. 
Es handelt sich dabei um eine auf \textit{Linux} basierende Open-Source-Distribution, die sich durch hohe Stabilität, Sicherheit und breite Unterstützung in der wissenschaftlichen sowie industriellen Softwareentwicklung auszeichnet. 
Da Ubuntu die offizielle Referenzplattform für das \textit{Robot Operating System~2 (ROS~2)} bildet, ermöglicht es eine nahtlose Integration der benötigten Bibliotheken, Entwicklungswerkzeuge und Treiber \cite{ubuntu_docs_2025}.

Die Entscheidung für Ubuntu~22.04~LTS wurde insbesondere aufgrund seiner engen Verzahnung mit der ROS-Distribution \textit{Humble~Hawksbill} getroffen, die in dieser Arbeit verwendet wurde. 
Das System bietet eine zuverlässige Umgebung für die Entwicklung in C++ und Python – Sprachen, die für die Verarbeitung von LiDAR-Daten und die Implementierung der verwendeten Algorithmen zentral sind. 
Darüber hinaus sorgt die aktive Entwicklergemeinschaft durch regelmäßige Updates, ausführliche Dokumentationen und eine große Auswahl an Open-Source-Paketen für einen reibungslosen Entwicklungsprozess. 
Dank der modularen Struktur von Linux lässt sich die Arbeitsumgebung zudem flexibel an die spezifischen Anforderungen der Sensorintegration und der ROS-Module anpassen.

Im Anschluss an die Beschreibung der Software- und Hardwarearchitektur werden zwei zentrale Komponenten der Entwicklungsumgebung vorgestellt, die den Implementierungs- und Testprozess der Messkette wesentlich unterstützt haben.  
Dazu gehören insbesondere die unter Linux eingesetzten \textit{Bash-Skripte}, die zur Automatisierung wiederkehrender Aufgaben wie dem Starten von Launch-Dateien, dem Kompilieren des Workspaces oder der Durchführung von Datenaufzeichnungen verwendet wurden, sowie der \textit{Terminator-Terminalemulator}, der eine parallele und übersichtliche Verwaltung mehrerer ROS~2-Prozesse ermöglicht.  

\subsection{Terminator-Terminalemulator}
Für die parallele Ausführung und Überwachung mehrerer ROS~2-Knoten wurde der Terminalemulator \textit{Terminator} verwendet. 
Dieses Werkzeug ermöglicht die Aufteilung des Terminalfensters in mehrere, individuell steuerbare Bereiche, wodurch verschiedene Prozesse simultan beobachtet und kontrolliert werden können. 
Insbesondere während der Entwicklung und Testphase der Pipeline war es notwendig, mehrere ROS-Knoten (z.~B. Filtermodule und Visualisierungstools) gleichzeitig zu starten und deren Konsolenausgaben im Blick zu behalten. 

\textit{Terminator} erwies sich hierbei als besonders vorteilhaft, da er eine klare Strukturierung der Arbeitsumgebung erlaubt und so den Überblick über komplexe Systemprozesse erleichtert. 
Durch die Möglichkeit, wiederkehrende Layouts zu speichern, konnte zudem eine standardisierte Entwicklungsumgebung geschaffen werden, die den Arbeitsfluss weiter optimierte.

\subsection{Bash-Skripte}

Bash-Skripte sind Textdateien, die eine Reihe von Befehlen enthalten, welche sequenziell von der Unix-Shell \texttt{bash} (Bourne Again Shell) ausgeführt werden. 
Sie dienen der Automatisierung wiederkehrender Aufgaben und sind ein zentrales Werkzeug in Linux-basierten Entwicklungsumgebungen. 
Besonders im Bereich der Robotik und Sensordatenverarbeitung ermöglichen Bash-Skripte eine effiziente Steuerung komplexer Systeme, da sie mehrere Prozesse, Programme und ROS~2-Knoten automatisch starten oder beenden können.

Im Rahmen dieser Arbeit wurden Bash-Skripte eingesetzt, um die Ausführung der gesamten ROS~2-Pipeline zu vereinfachen und deren Integration in die bestehende grafische Benutzeroberfläche (GUI) zu ermöglichen. 
Das entwickelte Skript fungiert als Bindeglied zwischen der GUI und dem Python-\texttt{launch}-File der Pipeline. 
Beim Start über die GUI ruft das Skript automatisch die ROS~2-Umgebung auf, lädt die entsprechenden Workspaces und startet das zentrale Launch-File, das alle erforderlichen Knoten nacheinander aktiviert. 

\section{Überblick über das Gesamtsystem}

\subsection{Anforderungen an den Algorithmus}
Die vom Ouster OS1-LiDAR erfassten Sensordaten werden über eine ROS-2-Node in das Netzwerk eingespeist und bilden die Grundlage für die Entwicklung einer Messkette, die relevante Objekte des Umfelds erkennt und in strukturierter Form ausgibt. Ziel ist es, aus der Punktwolke eine Objektliste mit Position, Orientierung und Abmessungen der erkannten Objekte zu erzeugen.

Für eine zuverlässige Objekterkennung ist eine geeignete Aufbereitung der Rohdaten erforderlich. Dabei sollen Bodenpunkte und Störsignale entfernt sowie die Punktwolkendichte reduziert werden, um die Rechenlast zu verringern, ohne die Form der Objekte wesentlich zu verfälschen. Die Parameter dieser Schritte müssen anpassbar bleiben, um unterschiedliche Szenarien abbilden zu können.

Die anschließende Verarbeitung der Daten hat unter Echtzeitbedingungen zu erfolgen. Alle notwendigen Prozesse sollen innerhalb einer festen Zeitspanne abgeschlossen sein, damit die Ergebnisse kontinuierlich bereitgestellt werden können. Die angestrebte Gesamtlatenz liegt unter 100 ms bei einer Sensorrate von 10 Hz. Gleichzeitig ist ein ressourcenschonender Betrieb sicherzustellen, der auch auf Standardrechnern ohne spezielle Hardware möglich ist.

Das System soll die erkannten Objekte in Form einer strukturierten Objektliste ausgeben. Diese Ausgabe erfolgt sowohl maschinenlesbar als auch im Terminal, um eine einfache Überprüfung der Ergebnisse zu ermöglichen. Zudem ist die Integration der Messkette in die bestehende grafische Benutzeroberfläche (GUI) der Hochschule vorgesehen, um eine zentrale Steuerung, Parametrierung und visuelle Darstellung aller Verarbeitungsschritte zu gewährleisten.

Die festgelegten Anforderungen bilden die Grundlage für die Konzeption, Umsetzung und spätere Bewertung der Messkette im Hinblick auf Genauigkeit, Stabilität und Echtzeitfähigkeit.

Die wichtigsten Anforderungen an die entwickelte Messkette sind in Tabelle~\ref{tab:anforderungen_messkette} zusammengefasst.

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|p{12cm}|}
    \hline
    \textbf{Nr.} & \textbf{Anforderung} \\ \hline
    1 & Eingabe: Sensordaten des Ouster~OS1-LiDAR \\ \hline
    2 & Ausgabe: Objektliste mit Position, Orientierung und Abmessungen \\ \hline
    3 & Entfernung von Bodenpunkten und Störsignalen \\ \hline
    4 & Reduzierung der Punktwolken­dichte zur Rechenoptimierung \\ \hline
    5 & Anpassbare Parameter für unterschiedliche Umgebungen \\ \hline
    6 & Echtzeitverarbeitung: Latenz \textless{}100\,ms bei 10\,Hz Sensorrate \\ \hline
    7 & Ressourcenschonender Betrieb auf Standardhardware \\ \hline
    8 & Maschinenlesbare und terminalbasierte Ergebnis­ausgabe \\ \hline
    9 & Integration in die bestehende grafische Benutzeroberfläche (GUI) \\ \hline
  \end{tabular}
  \caption{Anforderungen an die Messkette zur Umfelderkennung}
  \label{tab:anforderungen_messkette}
\end{table}

\subsection{Datenerfassung}
\label{chap:datenerfassung}

Die Datenerfassung bildet den Ausgangspunkt der gesamten Messkette und umfasst die Aufnahme sowie Übertragung der vom Ouster~OS1-LiDAR generierten Punktwolkendaten.  
Der grundlegende Aufbau der Datenaufnahme ist in Abbildung~\ref{fig:messkette_sensordaten} dargestellt und orientiert sich an dem im Rahmen des Projekts ``Analyse von Lidar-Sensordaten mittels ROS \& Matlab'' entwickelten Konzept nach~\cite{wendel2025}.

Die Daten werden über eine Ethernet-Verbindung an den \textit{dSPACE MAB~III Embedded-PC} übertragen, der als zentrale Erfassungseinheit fungiert. 
Zur Integration in das bestehende Netzwerk wird ein USB-A-auf-Ethernet-Adapter des Typs \textit{Renkforce RF-4708614} mit einer Übertragungsrate von bis zu \(1\,\text{Gbit/s}\) verwendet (\cite{conrad2025}). 
Die Kommunikation zwischen Sensor und Embedded-System erfolgt über das \textit{Robot Operating System 2 (ROS 2)}, welches den Austausch der Sensordaten in Echtzeit ermöglicht.

Der Ouster~OS1 publiziert seine Messdaten in Form von \texttt{sensor\_msgs/PointCloud2}-Nachrichten auf dem Topic \texttt{/ouster/points}. 
Diese Punktwolken bilden die Eingangsgröße der gesamten ROS~2-Pipeline, in der die Daten anschließend gefiltert, segmentiert und zu erkannten Objekten verarbeitet werden. 
Die LiDAR-Daten enthalten für jeden Punkt die kartesischen Koordinaten \((x, y, z)\), die Intensität des reflektierten Signals sowie den Ring-Index des jeweiligen Lasers. 
Zur Synchronisierung mit anderen Verarbeitungsschritten wird der Zeitstempel direkt aus der LiDAR-Firmware übernommen.

Die erfassten Punktwolken werden über das lokale ROS~2-Netzwerk an den Laptop übertragen, auf dem die Daten wahlweise in \textit{RViz2} visualisiert oder über entwickelte GUI gestartet werden können.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{Bilder/messkette_sensordaten.png}
  \caption{Aufbau der Datenerfassung mit dem Ouster~OS1-LiDAR 
  (überarbeitet nach ~\cite{wendel2025}).}
  \label{fig:messkette_sensordaten}
\end{figure}

\subsection{Umsetzung}

Die nachfolgende Abbildung~\ref{fig:ros2-pipeline-alltopics} zeigt die in dieser Arbeit entwickelte ROS~2-Pipeline zur Umfelderkennung auf Basis von LiDAR-Sensordaten. 
Sie veranschaulicht den Aufbau der Messkette, beginnend mit dem Einlesen der Rohdaten bis hin zur Visualisierung der erkannten Objekte in \textit{RViz}. 

Jede Node übernimmt dabei eine klar definierte Aufgabe – von der Datenvorverarbeitung über die Segmentierung und Clusterbildung bis hin zur Objektverfolgung – und kommuniziert über spezifische ROS-Topics mit den nachfolgenden Modulen. 

\begin{figure}
\centering
\begin{tikzpicture}[
    node distance=2.2cm,
    process/.style={rectangle, rounded corners, draw=black, fill=blue!5,
                    text centered, minimum width=4.2cm, minimum height=1cm},
    startstop/.style={ellipse, draw=black, fill=gray!15,
                      text centered, minimum width=3cm, minimum height=1cm},
    rviz/.style={rectangle, draw=black, fill=green!10, rounded corners,
                 text centered, minimum width=3.5cm, minimum height=0.9cm,
                 font=\scriptsize},
    arrow/.style={thick,->,>=stealth},
    every node/.style={font=\small}
  ]

  % Hauptlinie
  \node (start)   [startstop] {Start};
  \node (input)   [process, below of=start] {Messdaten einlesen};
  \node (crop)    [process, below of=input] {CropBox-Filter};
  \node (voxel)   [process, below of=crop] {VoxelGrid-Filter};
  \node (ground)  [process, below of=voxel] {Bodensegmentierung};
  \node (cluster) [process, below of=ground] {Cluster-Extraktion \& Bounding Boxes};
  \node (track)   [process, below of=cluster] {Objektverfolgung};
  \node (end)     [startstop, below of=track] {Ende};

  % RViz-Knoten (rechts)
  \node (rviz_input)  [rviz, right=4.1cm of input]  {RViz};
  \node (rviz_crop)   [rviz, right=4.1cm of crop]   {RViz};
  \node (rviz_voxel)  [rviz, right=4.1cm of voxel]  {RViz};
  \node (rviz_ground) [rviz, right=4.1cm of ground] {RViz};
  \node (rviz_cluster)[rviz, right=2.8cm of cluster]{RViz};
  \node (rviz_track)  [rviz, right=4.1cm of track]  {RViz};

  % Vertikale Pfeile (Pipeline)
  \draw[arrow] (start) -- (input);
  \draw[arrow] (input) -- node[right]{\scriptsize \texttt{/ouster/points}} (crop);
  \draw[arrow] (crop)  -- node[right]{\scriptsize \texttt{/points\_cropped}} (voxel);
  \draw[arrow] (voxel) -- node[right]{\scriptsize \texttt{/points\_voxel}} (ground);
  \draw[arrow] (ground)-- node[right]{\scriptsize \texttt{/obstacle\_points}} (cluster);
  \draw[arrow] (cluster)-- node[right]{\scriptsize \texttt{/detections\_raw}} (track);
  \draw[arrow] (track) -- node[right]{\scriptsize \texttt{/tracks\_raw}} (end);

  % Pfeile zu RViz (alle Schritte)
  \draw[arrow] (input.east)  -- node[above,sloped]{\scriptsize \texttt{/ouster/points}} (rviz_input.west);
  \draw[arrow] (crop.east)   -- node[above,sloped]{\scriptsize \texttt{/points\_cropped}} (rviz_crop.west);
  \draw[arrow] (voxel.east)  -- node[above,sloped]{\scriptsize \texttt{/points\_voxel}} (rviz_voxel.west);
  \draw[arrow] (ground.east) -- node[above,sloped]{\scriptsize \texttt{/obstacle\_points}} (rviz_ground.west);
  \draw[arrow] (cluster.east)-- node[above,sloped]{\scriptsize \texttt{/detections\_markers}} (rviz_cluster.west);
  \draw[arrow] (track.east)  -- node[above,sloped]{\scriptsize \texttt{/tracks\_markers}} (rviz_track.west);

  % Caption & Label
  \end{tikzpicture}
  \caption{ROS~2-Algorithmus mit allen Nodes, ihren Topic-Verbindungen und den Ausgaben zur Visualisierung in RViz.}
  \label{fig:ros2-pipeline-alltopics}
\end{figure}


\chapter{Vorverarbeitung}
\section{CropBox-Filter}

Zur Begrenzung der Punktwolke auf einen relevanten Bereich wird der \textit{CropBox-Filter} der 
Point Cloud Library (PCL) eingesetzt. 
Dieser Filter ermöglicht es, eine achsenparallel ausgerichtete Begrenzungsbox (AABB) zu definieren, 
innerhalb derer alle Punkte einer Punktwolke beibehalten werden, während Punkte außerhalb dieses 
Volumens verworfen werden. 
Gemäß der Dokumentation \cite{pcl_cropbox_2025} basiert die Implementierung der Klasse 
\texttt{pcl::CropBox<pcl::PCLPointCloud2>} auf einer effizienten räumlichen Filterung, 
die eine Punktwolke entlang der Achsen $x$, $y$ und $z$ beschneidet.  
Dadurch wird das Datenvolumen reduziert, was zu einer deutlichen Verringerung des 
Rechenaufwands für nachfolgende Verarbeitungsschritte führt.

Im Rahmen dieser Arbeit wurde der \texttt{crop\_box\_node} in C++ unter ROS~2 
entwickelt, um diesen Filter dynamisch innerhalb der gesamten Verarbeitungskette anzuwenden. 
Der Node empfängt eine eingehende Punktwolke vom Ouster-LiDAR-Sensor auf dem Topic 
\texttt{/ouster/points}, filtert die Daten gemäß den eingestellten Grenzen und publiziert die 
gefilterte Punktwolke auf \texttt{/points\_cropped}. 
Die Parameter \texttt{min\_bound} und \texttt{max\_bound} definieren die minimale und maximale Ausdehnung 
des Arbeitsvolumens in Metern, während die Parameter \texttt{input\_topic} und 
\texttt{output\_topic} den Ein- und Ausgang der Datenströme steuern. 
Das Konzept ist in Tabelle~\ref{tab:cropbox_params} zusammengefasst.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|c|}
    \hline
    \textbf{Parametername} & \textbf{Startwert} \\ \hline
    \texttt{input\_topic}  & \texttt{/ouster/points} \\ \hline
    \texttt{output\_topic} & \texttt{/points\_cropped} \\ \hline
    \texttt{min\_bound}    & \([-20.0, -15.0, -3.0]\) \\ \hline
    \texttt{max\_bound}    & \([20.0, 15.0, 5.0]\) \\ \hline
  \end{tabular}
  \caption{Startparameter des CropBox-Filters}
  \label{tab:cropbox_params}
\end{table}

Der \texttt{crop\_box\_node} wurde so implementiert, dass er sowohl Pflichtfelder 
(\texttt{x}, \texttt{y}, \texttt{z}) als auch optionale Felder wie 
\texttt{intensity} (FLOAT32/UINT16) und \texttt{ring} erkennt.  
Für jeden Punkt wird geprüft, ob er innerhalb des durch die Parameter definierten 
Volumens liegt. Ist dies der Fall, wird er beibehalten und in die 
Ausgabepunktwolke übernommen. 
Die Implementierung erlaubt zudem eine Laufzeitänderung der Grenzen 
durch dynamische Re-Konfiguration via \texttt{set\_parameters}, ohne dass 
die ROS~2-Verbindungen neu aufgebaut werden müssen.

Abbildung~\ref{fig:cropbox_compare} zeigt die Wirkung des Filters im Vergleich: 
Links ist die ursprüngliche Punktwolke mit vollständiger Umgebung dargestellt, 
während rechts nur der relevante Bereich nach Anwendung des CropBox-Filters zu sehen ist.  
Diese Reduktion der Datenmenge ist insbesondere für Echtzeit-Anwendungen im 
Bereich der Umfelderkennung von Vorteil.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{Bilder/cropbox_compare.png}
  \caption{Vergleich der Punktwolke vor (links) und nach (rechts) Anwendung des 
  \textit{CropBox-Filters}.}
  \label{fig:cropbox_compare}
\end{figure}


\section{VoxelGrid-Filter}

Der \textit{VoxelGrid-Filter} reduziert die Punktanzahl, indem der Raum in kubische Voxel der
Kantenlänge $l=\texttt{voxel\_size}$ diskretisiert wird und pro Voxel nur ein repräsentativer Punkt
(z.\,B. der erste oder ein Schwerpunkt) beibehalten wird. Dadurch sinken Datenrate und
Rechenaufwand nachgelagerter Schritte (Bodenebensegmentierung, Clustering, Tracking), bei
gleichzeitiger Kontrolle des Informationsverlustes über die Wahl von $l$.
Das Funktionsprinzip entspricht der gängigen Downsampling-Strategie in der Literatur
(vgl. Abbildung~\ref{fig:voxel_principle}), siehe z.\,B. die Darstellung in \emph{Applied Sciences}
(2024)\footnote{Abbildung nach Quelle: \url{https://www.mdpi.com/2076-3417/14/8/3160}.}.

% préambule : \usepackage{graphicx}
\begin{figure}[H]
  \centering
  \includegraphics[width=.82\textwidth]{Bilder/voxelgrid_prinzip.png}%
  \caption{Funktionsprinzip des VoxelGrid-Filters: Aufteilung des Raums in Voxel und
  Beibehaltung eines repräsentativen Punktes pro Voxel (nach Lyu~u.\,a., 2024).}
  \label{fig:voxel_principle}
\end{figure}

Der \textit{VoxelFilterNode} liest die
zuvor per \textit{CropBox} zugeschnittene Punktwolke auf \texttt{points\_cropped}, führt die
Voxelisierung mit der Parametergröße \texttt{voxel\_size} durch und publiziert die
downsamplete Punktwolke auf \texttt{points\_voxel}. Die wichtigsten Startparameter sind in
Tabelle~\ref{tab:voxel_params} zusammengefasst.

% préambule : \usepackage{booktabs} (optionnel, pour un rendu plus pro)
\begin{table}[H]
  \centering
  \begin{tabular}{|l|c|}
    \hline
    \textbf{Parametername} & \textbf{Startwert} \\ \hline
    \texttt{input\_topic}  & \texttt{points\_cropped} \\ \hline
    \texttt{output\_topic} & \texttt{points\_voxel} \\ \hline
    \texttt{voxel\_size}   & \(\,0{,}20\,\mathrm{m}\,\) \\ \hline
  \end{tabular}
  \caption{Startparameter des VoxelGrid-Filters}
  \label{tab:voxel_params}
\end{table}

Wie in Abbildung~\ref{fig:voxel_compare} dargestellt, führt die Anwendung des
VoxelGrid-Filters zu einer gleichmäßigeren und deutlich reduzierten Punktdichte,
ohne dass dabei wichtige Strukturen der Szene verloren gehen.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{Bilder/voxel_compare.png}
  \caption{Vergleich nach \textit{CropBox} (links) und nach \textit{VoxelGrid-Filter} (rechts).
  Das Downsampling reduziert die Punktdichte deutlich, bewahrt jedoch die wesentliche
  Geometrie für die nachfolgenden Schritte.}
  \label{fig:voxel_compare}
\end{figure}


\section{Bodensegmentierung}
\label{chap:bodensegmentierung}

Die Bodensegmentierung ist ein essenzieller Vorverarbeitungsschritt innerhalb der Pipeline zur Objekterkennung. 
Ihr Ziel besteht darin, Bodenpunkte zuverlässig von Hindernissen und anderen Objekten zu trennen, um die nachfolgenden Schritte – insbesondere die Cluster- und Objekterkennung – zu erleichtern. 
Im Folgenden werden zunächst die Anforderungen an die Bodensegmentierung beschrieben, anschließend verschiedene Methoden anhand definierter Bewertungskriterien verglichen und abschließend die in dieser Arbeit implementierte Methode vorgestellt.


\subsection{Anforderungen an der Bodensegmentierung}
\label{sec:anforderungen_bodenseg}

Die wichtigsten Anforderungen an ein Bodensegmentierungsverfahren lassen sich nach~\cite{gomes2023survey} in mehreren Kriterien zusammenfassen, 
die in Tabelle~\ref{tab:anforderungen_bodensegmentierung} dargestellt sind.

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.3} % augmente l'espacement vertical
\setlength{\tabcolsep}{8pt}       % ajuste l'espacement horizontal
\begin{tabular}{|p{4cm}|p{9.5cm}|}
\hline
\textbf{Kriterium} & \textbf{Beschreibung} \\ \hline

\textbf{Echtzeitfähigkeit} & 
Das Verfahren muss Punktwolken in Echtzeit verarbeiten können (z.\,B. <100\,ms pro Frame), 
um eine kontinuierliche Fahrzeugsteuerung zu ermöglichen. \\ \hline

\textbf{Rechenaufwand} & 
Geringe Rechen- und Speicheranforderungen sind notwendig, 
da die Algorithmen häufig auf eingebetteten Systemen mit begrenzten Ressourcen laufen. \\ \hline

\textbf{Segmentierungs\-robustheit} & 
Das Verfahren sollte robust gegenüber Über- und Untersegmentierung sein 
und Bodenpunkte korrekt von Hindernissen trennen. \\ \hline

\textbf{Leistung bei steigenden Hindernissen} & 
Die Methode sollte sanft ansteigende Strukturen (z.\,B. Rampen oder Bordsteine) 
korrekt als Teil des Bodens erkennen. \\ \hline

\textbf{Leistung bei unebenem Gelände} & 
Auch bei Neigungen oder unregelmäßigen Bodenoberflächen 
muss die Bodenschätzung stabil bleiben. \\ \hline

\textbf{Leistung bei spärlichen Daten} & 
Das Verfahren sollte bei geringer Punktdichte (z.\,B. großer Abstand zum Sensor) 
zuverlässige Ergebnisse liefern. \\ \hline
\end{tabular}

\vspace{5pt}
\caption{Hauptanforderungen an die Bodensegmentierung (nach~\cite{gomes2023survey})}
\label{tab:anforderungen_bodensegmentierung}
\end{table}

Diese Arbeit fokusiert sich auf vier Hauptkriterien:

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|c|p{4cm}|p{9cm}|}
\hline
\textbf{Nr.} & \textbf{Kriterium} & \textbf{Beschreibung} \\ \hline
1 & Echtzeitfähigkeit & Bewertet, ob die Methode unter den gegebenen Rechenbedingungen (10\,Hz Sensorrate) eine kontinuierliche Verarbeitung der Punktwolke in Echtzeit ermöglicht. \\ \hline
2 & Robustheit & Misst die Widerstandsfähigkeit gegenüber Rauschen, Ausreißern und unterschiedlichen Geländetypen (z.\,B. unebenes Terrain oder variable Punktdichten). \\ \hline
3 & Genauigkeit & Beschreibt die Fähigkeit der Methode, die Bodenpunkte präzise von Nicht-Bodenpunkten zu trennen und somit eine zuverlässige Segmentierung zu gewährleisten. \\ \hline
4 & Rechenaufwand & Bewertet den benötigten Ressourcenverbrauch (CPU/GPU-Zeit und Speicherbedarf) für die Durchführung der Bodensegmentierung. \\ \hline
\end{tabular}
\caption{Bewertungskriterien zur Analyse der Bodensegmentierungsmethoden.}
\label{tab:bewertungskriterien}
\end{table}

Die Auswahl dieser vier Bewertungskriterien basiert auf den Anforderungen, 
die bei der Verarbeitung von LiDAR-Daten in urbanen Umgebungen besonders relevant sind.  
In städtischen Szenarien treten komplexe Strukturen, wechselnde Oberflächenmaterialien 
und zahlreiche bewegte Objekte auf, was eine hohe Robustheit und Genauigkeit bei der 
Bodensegmentierung erfordert.  
Gleichzeitig muss die Verarbeitung der Sensordaten in Echtzeit erfolgen, 
um eine kontinuierliche Umfeldwahrnehmung und gegebenenfalls eine sichere Fahrzeugführung 
zu gewährleisten.  
Da in praktischen Anwendungen häufig nur begrenzte Rechenressourcen zur Verfügung stehen, 
ist auch der Rechenaufwand ein entscheidender Faktor für die Auswahl geeigneter Verfahren.  
Diese vier Kriterien bilden somit die Grundlage für eine objektive Bewertung 
und den methodischen Vergleich der in dieser Arbeit betrachteten Ansätze.

\subsection{Methoden der Bodensegmentierung}
\label{chap:bodenmethoden}

Die Bodensegmentierung ist ein wesentlicher Verarbeitungsschritt in der Umfeldwahrnehmung autonomer Fahrzeuge. 
Ihr Ziel ist es, die Bodenpunkte in einer LiDAR-Punktwolke zuverlässig von Objekten und Hindernissen zu trennen, 
um Navigations- und Objekterkennungsalgorithmen zu entlasten. 
Nach~\cite{gomes2023survey} lassen sich die gängigen Verfahren zur Bodensegmentierung 
in fünf Hauptkategorien einteilen (siehe Abbildung~\ref{fig:boden_taxonomie}): 

\begin{itemize}
    \item 2.5D-Gitterbasierte Verfahren
    \item Bodenmodellierung (Ground Modelling)
    \item Methoden auf Basis benachbarter Punkte und lokaler Merkmale
    \item Verfahren höherer Ordnung (Higher-Order Inference)
    \item Lernbasierte Verfahren (Deep Learning)
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{Bilder/boden_taxonomie.png}
  \caption{Klassifizierung und Systematisierung bestehender Bodensegmentierungsmethoden}
  \label{fig:boden_taxonomie}
\end{figure}


Diese Klassifikation deckt sowohl klassische geometrische Ansätze als auch moderne, neuronale Verfahren ab. 
Im Folgenden werden die wichtigsten Prinzipien und repräsentativen Algorithmen jeder Kategorie erläutert.

\subsubsection{2.5D-Gitterbasierte Verfahren}
% ============================

Ein verbreiteter Ansatz zur Bodensegmentierung besteht darin, die dreidimensionale Punktwolke in eine zweidimensionale Rasterdarstellung zu überführen.  
Dabei werden die Punkte nach ihren Koordinaten in diskrete Zellen eingeteilt, sodass jede Zelle statistische Höheninformationen über die in ihr enthaltenen Punkte speichert.  
Dieser sogenannte 2.5D-Ansatz reduziert die Komplexität der Verarbeitung erheblich, da die Analyse nicht mehr im vollen 3D-Raum, sondern auf einer strukturierten Gitterebene erfolgt.

Douillard et al.~\cite{douillard2011segmentation} präsentieren ein solches Verfahren auf Basis von \textit{Elevation Maps}, bei dem jede Gitterzelle durch den Mittelwert der Höhenwerte ihrer Punkte beschrieben wird.  
Zellen mit geringer Höhenvarianz werden als Boden klassifiziert, während größere Abweichungen auf Objekte oder Hindernisse hinweisen.  
Durch diese Reduktion auf lokale Höhenstatistiken kann die Methode Bodenflächen effizient und robust in urbanen Umgebungen identifizieren, ohne den gesamten Punktwolkenraum verarbeiten zu müssen.


% ============================
\subsubsection{Bodenmodellierung (Ground Modelling)}
% ============================

Diese Methoden approximieren die Bodenfläche durch mathematische Modelle, typischerweise in Form von Ebenen oder Linien, um die Trennung zwischen Boden- und Nichtbodenpunkten zu ermöglichen.

\textbf{Plane Fitting:}  
In \cite{hu2013robust} wird die Identifikation von Bodenpunkten mithilfe der \textit{Random Sample Consensus (RANSAC)}-Methode beschrieben, 
bei der eine Ebene an die niedrigsten Punkte angepasst und Punkte mit geringem orthogonalen Abstand als \textit{Inlier} klassifiziert werden.  
Ein ähnlicher Ansatz teilt die Punktwolke in konzentrische Zonen auf und passt für jede Zone lokal angepasste Ebenen an, 
wobei die \textit{Principal Component Analysis (PCA)} eingesetzt wird, um die Hauptrichtung der Punktverteilung zu bestimmen und daraus die bestmögliche Ebenenorientierung zu berechnen \cite{lim2020fast}.  
Dieser Ansatz erreicht eine hohe Präzision (F1-Score $\approx 0.93$) bei gleichzeitigem Echtzeitverhalten.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{Bilder/plane_fitting.png}
  \caption{Visuelle Darstellung einer orthogonalen Distanzklassifizierung (~\cite{gomes2023survey})}
  \label{fig:plane_fitting}
\end{figure}

\textbf{Linienextraktion:}  
Ein auf lokalen Linienanpassungen basierendes Verfahren modelliert die entlang eines Laserstrahls erfassten Punkte als nahezu linearen Verlauf \cite{himmelsbach2010fast}.  
Dieses Verfahren ist recheneffizient und eignet sich für Echtzeitverarbeitung, weist jedoch Schwächen in stark unebenem oder komplexem Gelände auf.

\textbf{Gaussian Process Regression (GPR):}  
Die \textit{Gaussian Process Regression (GPR)} wurde zur Modellierung der Bodenhöhe eingeführt, 
wobei der Höhenverlauf als kontinuierliche Funktion mit zugehöriger Unsicherheitsabschätzung beschrieben wird \cite{douillard2011segmentation}.  
Durch die Verwendung nichtstationärer Kovarianzfunktionen kann sich das Modell lokal an unterschiedliche Geländestrukturen anpassen und dadurch eine hohe Genauigkeit auch in unregelmäßigem Terrain erreichen \cite{chen2014real}.

% ============================
\subsubsection{Benachbarte Punkte und lokale Merkmale}
% ============================

Diese Algorithmen analysieren geometrische Beziehungen zwischen benachbarten Punkten in der Punktwolke.  
Dabei wird häufig die vertikale Kanalstruktur moderner LiDAR-Sensoren (z.\,B. Velodyne VLP-16 oder Ouster OS1-64) ausgenutzt, um lokale Abhängigkeiten entlang der Scanlinien zu erkennen.

\textbf{Kanalbasierte Verfahren:}  
In \cite{chu2019ground} wird die Bodenextraktion entlang vertikaler Scans beschrieben, 
bei der lokale Höhenunterschiede und Gradienten ausgewertet werden.  
Punkte zwischen einem Startbodenpunkt und einem definierten Schwellenwert werden als Boden klassifiziert.  
Das Verfahren ist recheneffizient, reagiert jedoch empfindlich auf eine ungenaue Parametrierung, etwa bei der Wahl des Höhen- oder Gradientschwellenwerts.

\textbf{Region-Growing und Clustering:}  
In \cite{moosmann2009segmentation} wird ein graphenbasiertes Region-Growing-Verfahren vorgestellt, 
bei dem benachbarte Punkte iterativ zu Regionen zusammengefügt werden, 
sofern lokale geometrische Kriterien (z.\,B. Konvexität) erfüllt sind.  
Ein alternativer Ansatz kombiniert voxelbasiertes Clustering mit statistischer Analyse, 
um Bodencluster zuverlässig zu isolieren \cite{douillard2011segmentation}.  
Solche Methoden liefern stabile Ergebnisse auch in komplexen Szenen, 
sind jedoch rechenintensiver und daher weniger für Echtzeitanwendungen geeignet.

\textbf{Range-Image-Methoden:}  
In \cite{bogoslavskyi2016fast} wird die Punktwolke in ein zweidimensionales Entfernungsbild (\textit{Range Image}) projiziert, 
bei dem jeder Pixel den Abstand eines Messpunkts zum Sensor repräsentiert.  
Diese Repräsentation ermöglicht eine effiziente Definition von Nachbarschaften und vereinfacht die anschließende Segmentierung erheblich.  
Mit diesem Ansatz kann die Bodenextraktion in wenigen Millisekunden pro Frame durchgeführt werden, was eine Echtzeitverarbeitung erlaubt.

% ============================
\subsubsection{Verfahren höherer Ordnung}
% ============================

Ansätze dieser Kategorie verwenden probabilistische Graphmodelle wie 
\textit{Markov Random Fields (MRF)} oder \textit{Conditional Random Fields (CRF)}, 
um Abhängigkeiten zwischen benachbarten Punkten explizit zu modellieren.  
Durch die Berücksichtigung solcher räumlicher Korrelationen können Fehlklassifikationen, 
insbesondere in spärlichen oder verrauschten Punktwolken, deutlich reduziert werden.

In \cite{guo2011ground} wird ein MRF-Modell mit dem \textit{Belief Propagation (BP)}-Verfahren kombiniert, 
das die Wahrscheinlichkeiten einzelner Punktzuordnungen iterativ aktualisiert, 
um eine konsistente Bodenfläche auch in unebenem Gelände zu rekonstruieren.  
Ein weiterentwickelter Ansatz integriert zeitliche Abhängigkeiten in ein CRF-Modell, 
wodurch die Konsistenz zwischen aufeinanderfolgenden Frames verbessert 
und die Stabilität der Segmentierung bei Bewegungen erhöht wird \cite{rummelhard2015temporal}.

% ============================
\subsubsection{Lernbasierte Verfahren}
% ============================

In den letzten Jahren haben sich tief neuronale Netze als besonders leistungsfähige Ansätze für die Punktwolkensegmentierung etabliert.  
Je nach Architektur werden die Sensordaten entweder direkt in Punktform verarbeitet oder zuvor in strukturierte Darstellungen überführt, um eine effiziente Merkmalsextraktion zu ermöglichen.

\textbf{PointNet- und Voxel-basierte Modelle:}  
Das in \cite{qi2017pointnet} vorgestellte PointNet-Framework ermöglicht die direkte Verarbeitung unstrukturierter Punktwolken, indem es für jeden Punkt Merkmale extrahiert und diese global aggregiert.  
Zur Erfassung lokaler geometrischer Abhängigkeiten wurde das Konzept in PointNet++ erweitert.  
Alternativ unterteilen voxelbasierte Verfahren wie VoxelNet \cite{zhou2018voxelnet} oder PointPillars \cite{lang2019pointpillars} die Punktwolke in diskrete 3D-Zellen (Voxel) bzw. Säulen und verwenden dreidimensionale Faltungsnetzwerke (3D-CNNs) zur Merkmalsanalyse.  

\textbf{Bildbasierte Ansätze:}  
In \cite{wu2018squeezeseg} und \cite{milioto2019rangenet++} werden Punktwolken in zweidimensionale Entfernungsbilder (\textit{Range Images}) projiziert, sodass konventionelle 2D-Faltungsnetzwerke auf LiDAR-Daten angewendet werden können.  
Diese Repräsentation ermöglicht eine Echtzeitverarbeitung auf GPUs bei gleichzeitig hoher Segmentierungsgenauigkeit.  

\textbf{Spezialisierte Netze für Bodensegmentierung:}  
Ein speziell für die Bodenerkennung entwickeltes neuronales Modell ist GndNet \cite{paigwar2020gndnet}.  
Das Netz basiert auf einem zweidimensionalen Gittermodell, in dem für jede Zelle die Bodenhöhe vorhergesagt wird.  
Dieses Verfahren erreicht eine mittlere Intersection-over-Union (IoU) von 83,6\,\% bei einer Laufzeit von nur 17,9\,ms pro Frame und ist somit für Echtzeitanwendungen geeignet.

% ============================
\subsubsection{Zusammenfassung}
% ============================

Tabelle~\ref{tab:vergleich_bodenmethoden} fasst die wesentlichen Eigenschaften der vorgestellten Bodensegmentierungsmethoden zusammen.  
Sie verdeutlicht die jeweiligen Stärken und Grenzen der Ansätze sowie ihre typischen Einsatzgebiete in der mobilen Robotik und Umfelderkennung.

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|p{3.5cm}|p{4cm}|p{4cm}|p{3cm}|}
\hline
\textbf{Methodenkategorie} & \textbf{Vorteile} & \textbf{Nachteile} & \textbf{Typisches Einsatzgebiet} \\ \hline
2.5D-Gitterbasierte Verfahren & Geringe Rechenlast, robust auf ebenem Gelände & Begrenzte Genauigkeit bei Überhängen oder Brücken & Stadt- und Straßenszenarien \\ \hline
Bodenmodellierung & Hohe Präzision, einfache mathematische Umsetzung & Geringe Robustheit bei komplexen Oberflächenformen & Flaches bis leicht geneigtes Terrain \\ \hline
Lokale Merkmalsanalyse & Unempfindlich gegenüber Dichteänderungen & Starke Abhängigkeit von Parameterwahl und Sensorgeometrie & Dynamische oder unstrukturierte Umgebungen \\ \hline
Probabilistische Graphmodelle (MRF/CRF) & Hohe Konsistenz durch Nachbarschaftsbeziehungen & Hohe Rechenkomplexität, geringe Echtzeitfähigkeit & Forschungs- und Entwicklungsumgebungen \\ \hline
Lernbasierte Verfahren & Sehr hohe Genauigkeit, anpassungsfähig durch Training & Großer Trainings- und Hardwareaufwand & Autonomes Fahren, Echtzeit-Perzeption \\ \hline
\end{tabular}
\caption{Vergleich der Bodensegmentierungsmethoden in Bezug auf Rechenaufwand, Robustheit und Einsatzgebiet (nach~\cite{gomes2023survey}).}
\label{tab:vergleich_bodenmethoden}
\end{table}

\begin{table}[h!]
  \centering
  \renewcommand{\arraystretch}{1.2}
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|c|c|c|c|c|}
      \hline
      \textbf{Methode} &
      \textbf{Genauigkeit} &
      \textbf{Rechenaufwand} &
      \textbf{Robustheit} &
      \textbf{Echtzeitfähig} &
      \textbf{Geländeanpassung} \\
      \hline
      Höhenbasierte Verfahren & Niedrig & Sehr gering & Niedrig & Ja & Nein \\ \hline
      Rasterbasierte Verfahren & Mittel & Gering & Mittel & Ja & Teilweise \\ \hline
      \textbf{RANSAC (Plane Fitting)} & \textbf{Hoch} & \textbf{Mittel bis hoch} & \textbf{Hoch} & \textbf{Eingeschränkt} & \textbf{Nein} \\ \hline
      Morphologische Filter & Mittel & Gering & Mittel & Ja & Teilweise \\ \hline
      ML-/DL-basierte Verfahren & Sehr hoch & Hoch & Hoch & Nein & Ja \\ \hline
    \end{tabular}%
  }
  \caption{Vergleich der Methoden zur Bodensegmentierung nach Bewertungskriterien (nach~\cite{gomes2023survey}).}
  \label{tab:boden-methodenvergleich-ransac}
\end{table}


Das RANSAC-Verfahren zeichnet sich durch eine hohe Robustheit gegenüber Ausreißern aus und liefert eine präzise Anpassung planarer Bodenflächen.  
Im Vergleich zu rasterbasierten oder morphologischen Methoden ermöglicht es eine genauere Modellierung, erfordert jedoch einen höheren Rechenaufwand und eine sorgfältige Parametrierung, um eine stabile Echtzeitverarbeitung zu gewährleisten.  
Aufgrund seiner Balance zwischen Präzision und algorithmischer Einfachheit wurde RANSAC als Grundlage für die in dieser Arbeit implementierte Bodensegmentierung ausgewählt.

\section{Implementierung}

\label{sec:implementierung_ransac}

Die Bodensegmentierung wurde als eigenständiger ROS~2-Knoten in \texttt{C++} mit der \textit{Point Cloud Library (PCL)} implementiert. 
Der Knoten \texttt{ransac\_ground\_node} abonniert gefilterte LiDAR-Punktwolken, schätzt eine Bodenebene mittels \textit{SAC-RANSAC} und veröffentlicht eine Hindernis-Punktwolke, aus der die Bodenpunkte entfernt wurden. 
Die Architektur ist strikt streaming-orientiert (Callback-basiert) und verzichtet auf Blockierungen, wodurch eine niedrige Latenz erzielt wird.

Der Knoten deklariert die Ein- und Ausgabetopics als Parameter und nutzt \texttt{SensorDataQoS}:
\begin{itemize}
  \item \textbf{Eingabe (\texttt{input\_topic})}: \texttt{/points\_voxel} \ \emph{(Typ: \texttt{sensor\_msgs/PointCloud2})}, enthält bereits per VoxelGrid ausgedünnte Punkte (\texttt{pcl::PointXYZI}).
  \item \textbf{Ausgabe (\texttt{output\_topic})}: \texttt{/obstacle\_points} \ \emph{(Typ: \texttt{sensor\_msgs/PointCloud2})}, enthält ausschließlich Nicht-Boden-Punkte.
\end{itemize}
Leere Eingaben werden verworfen, um unnötige Rechenarbeit zu vermeiden.

Die wesentlichen Laufzeitparameter werden als ROS-Parameter deklariert und können über Launch-Dateien oder \texttt{ros2 param} angepasst werden (Default-Werte aus dem Code):
\begin{table}[h!]
\centering
\begin{tabular}{|l|c|l|}
\hline
\textbf{Parameter} & \textbf{Default} & \textbf{Bedeutung} \\ \hline
\texttt{input\_topic} & \texttt{/points\_voxel} & Eingangs-Punktwolke (vorsegmentiert per VoxelGrid) \\ \hline
\texttt{output\_topic} & \texttt{/obstacle\_points} & Ausgabe ohne Bodenpunkte \\ \hline
\texttt{distance\_threshold} & $0{,}15\,\mathrm{m}$ & maximaler Punkt-zu-Ebene-Abstand für Inlier \\ \hline
\texttt{max\_iterations} & $500$ & maximale RANSAC-Iterationen \\ \hline
\end{tabular}
\caption{RANSAC-Parametrisierung des \texttt{ransac\_ground\_node}.}
\label{tab:ransac_params}
\end{table}

Zur Ebenenschätzung wird \texttt{pcl::SACSegmentation} mit \texttt{SACMODEL\_PLANE} und \texttt{SAC\_RANSAC} verwendet, inkl.\ Koeffizientenoptimierung:
\begin{enumerate}
  \item \textbf{Konvertierung}: \texttt{sensor\_msgs/PointCloud2} $\rightarrow$ \texttt{pcl::PointCloud<pcl::PointXYZI>}.
  \item \textbf{RANSAC-Konfiguration}: \texttt{setOptimizeCoefficients(true)}, \texttt{setModelType(PLANE)}, \texttt{setMethodType(RANSAC)}, \texttt{setDistanceThreshold}, \texttt{setMaxIterations}.
  \item \textbf{Segmentierung}: \texttt{seg.segment(inliers, coefficients)} liefert Inlier-Indizes der Bodenpunkte und Ebenenparameter $\mathbf{n}=(a,b,c)$, $d$ der Ebene
  \[
    a x + b y + c z + d = 0.
  \]
  \item \textbf{Extraktion der Hindernisse}: \texttt{pcl::ExtractIndices} mit \texttt{setNegative(true)} filtert alle Punkte \emph{außerhalb} der Bodenenebene (\,$>$ \texttt{distance\_threshold}\,).
  \item \textbf{Publikation}: Rückkonvertierung nach \texttt{PointCloud2} und Veröffentlichung auf \texttt{/obstacle\_points} (mit ursprünglichem Header/Frame).
\end{enumerate}
Ein Punkt $\mathbf{p}=(x,y,z)^\top$ zählt als Inlier, wenn sein orthogonaler Abstand zur Ebene
\[
  \mathrm{dist}(\mathbf{p}, \Pi) \;=\; \frac{|a x + b y + c z + d|}{\sqrt{a^2+b^2+c^2}}
\]
kleiner gleich \texttt{distance\_threshold} ist.

Als Ergebnis ergibt sich eine Punktwolke, die frei von Bodenpunkten ist (siehe Abbildung~\ref{fig:ransac_compare}). 
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{Bilder/ransac_compare.png}
  \caption{Vergleich der ursprünglichen Punktwolke (links) und der nach dem RANSAC-Algorithmus gefilterten Punktwolke (rechts)}
  \label{fig:ransac_compare}
\end{figure}

\chapter{Cluster-Extraktion und Ermittlung von Bounding-Boxes}
\label{chap:cluster_extraction}

Nach der Entfernung der Bodenpunkte mittels RANSAC (siehe Kapitel~\ref{chap:bodensegmentierung}) erfolgt die Gruppierung der verbleibenden Punktwolke in zusammenhängende Punktmengen. 

\section{Prinzip}

Dieser Schritt dient der Identifikation einzelner Objekte und bildet die Grundlage für deren spätere Verfolgung.
Das euklidische Clustering nutzt die Distanzbeziehungen zwischen Punkten in der Punktwolke, um zusammenhängende Punktmengen zu identifizieren. 
Zunächst wird ein k-d-Baum (\texttt{pcl::search::KdTree}) erstellt, der eine effiziente Nachbarschaftssuche ermöglicht. 
Für zwei Punkte \(p_i\) und \(p_j\) gilt:
\[
d(p_i, p_j) = \sqrt{(x_i - x_j)^2 + (y_i - y_j)^2 + (z_i - z_j)^2} < \varepsilon,
\]
wobei \(\varepsilon\) die Clustertoleranz bezeichnet.

\section{Umsetzung}

Zur Umsetzung wird die in der \textit{Point Cloud Library (PCL)} enthaltene Klasse \texttt{pcl::EuclideanClusterExtraction} verwendet \cite{pcl2025euclideancluster, rusu2011pcl}. 
Die erkannten Cluster werden anschließend mit achsenparallelen Begrenzungsboxen (AABB) beschrieben, deren Parameter mit \texttt{pcl::MomentOfInertiaEstimation} bestimmt werden \cite{pcl2025momentofinertia}.
 
Punkte, die innerhalb dieser Distanz liegen, werden demselben Cluster zugeordnet. 
Der Algorithmus wiederholt diesen Vorgang rekursiv, bis alle Punkte verarbeitet sind.  
Diese Methode ist eine bewährte Standardtechnik für LiDAR-basierte Objekterkennung in Echtzeitanwendungen \cite{himmelsbach2010fast, geiger2012kitti}.

Die Implementierung erfolgt als ROS~2-Knoten \texttt{cluster\_extraction\_node}, der Punktwolken vom Topic \texttt{/obstacle\_points} empfängt. 
Für die Clusterbildung wird die Klasse \texttt{pcl::EuclideanClusterExtraction} verwendet, und zur Bestimmung der Objektabmessungen wird \texttt{pcl::MomentOfInertiaEstimation} eingesetzt. 
Die Methode wurde aufgrund ihrer Robustheit und einfachen Parametrierbarkeit gewählt und eignet sich insbesondere für dichte 3D-LiDAR-Daten.

Die wichtigsten Parameter sind in Tabelle~\ref{tab:cluster_params} aufgeführt. 
Sie steuern die Sensitivität und Granularität der Clusterbildung und können über ROS-Parameter angepasst werden.

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|l|c|p{8cm}|}
\hline
\textbf{Parameter} & \textbf{Standardwert} & \textbf{Bedeutung} \\ \hline
\texttt{cluster\_tolerance} & $0{,}5\,\mathrm{m}$ & Maximaler Abstand zwischen Punkten innerhalb eines Clusters \\ \hline
\texttt{min\_cluster\_size} & 30 & Minimale Punktzahl pro Cluster \\ \hline
\texttt{max\_cluster\_size} & 8000 & Maximale Punktzahl pro Cluster \\ \hline
\texttt{max\_clusters} & 200 & Maximale Anzahl an Clustern pro Frame \\ \hline
\end{tabular}
\caption{Parameter der euklidischen Cluster-Extraktion.}
\label{tab:cluster_params}
\end{table}


\begin{enumerate}
  \item \textbf{Eingangsdaten:} Punktwolken werden von \texttt{/obstacle\_points} empfangen und in \texttt{pcl::PointCloud<pcl::PointXYZ>} konvertiert.
  \item \textbf{Nachbarschaftssuche:} Aufbau eines k-d-Baums zur effizienten Punktabfrage.
  \item \textbf{Clusterbildung:} Anwendung von \texttt{setClusterTolerance}, \texttt{setMinClusterSize}, \texttt{setMaxClusterSize} und \texttt{extract()}.
  \item \textbf{Berechnung der Bounding-Boxes:} 
        Die Achsen-parallelen Begrenzungsboxen (AABB) werden über die Minimal- und Maximalpunkte der Punktwolke berechnet:
        \[
        L = x_{\max} - x_{\min}, \quad 
        W = y_{\max} - y_{\min}, \quad 
        H = z_{\max} - z_{\min}.
        \]
        Ergänzend liefert \texttt{pcl::MomentOfInertiaEstimation} die Trägheitsmomente und Schwerpunkte, wodurch bei Bedarf auch orientierte Boxen (OBB) berechnet werden können \cite{pcl2025momentofinertia}.
  \item \textbf{Größenfilterung:} Cluster, deren Dimensionen außerhalb plausibler Grenzen liegen (\(L, W, H\)), werden verworfen, um Rauschen und Ausreißer zu eliminieren.
  \item \textbf{Publikation:} Die resultierenden Cluster werden als \texttt{visualization\_msgs/MarkerArray} für RViz und als \texttt{vision\_msgs/Detection3DArray} für das Tracking-Modul veröffentlicht.
\end{enumerate}

Die Methode zeigt eine robuste Clustertrennung bei moderater Punktdichte und kann in Echtzeit auf CPU-basierter Hardware ausgeführt werden. 
Durch die Kombination mit dem RANSAC-Filter werden Bodenpunkte zuverlässig entfernt, was die Qualität der Segmentierung deutlich verbessert. 
Grenzen bestehen bei überlappenden Objekten oder stark variierender Punktdichte, was zu Teilsegmentierungen führen kann. 
Insgesamt bietet die PCL-basierte Cluster-Extraktion eine ausgewogene Balance zwischen Rechenaufwand und Genauigkeit und bildet die Grundlage für die nachfolgende Objektverfolgung.

Abbildung~\ref{fig:clustering_result} zeigt das Ergebnis der Cluster-Extraktion.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{Bilder/clustering_result.png}
  \caption{Detektierter Fahrzeug-Cluster mit AABB-Dimensionen (4.06 × 2.04 × 1.33 m)}
  \label{fig:clustering_result}
\end{figure}

\chapter{Objektverfolgung}
\label{chap:tracking}

Nach der Cluster-Extraktion und Bestimmung der Bounding-Boxes (siehe Kapitel~\ref{chap:cluster_extraction}) erfolgt in dieser Arbeit die zeitliche Verfolgung der erkannten Objekte. 
Dieser Schritt stellt sicher, dass identifizierte Objekte über mehrere aufeinanderfolgende Frames hinweg konsistent verfolgt werden können, selbst bei Messrauschen oder kurzzeitigen Ausfällen. 
Zur Umsetzung wurde ein leichtgewichtiger Multi-Objekt-Tracker entwickelt, der auf einem linearen Kalman-Filter und dem ungarischen Zuordnungsverfahren basiert \cite{welch1995kalman, kuhn1955hungarian, bewley2016sort}.

Die Objektverfolgung erweitert die reine Objekterkennung um die zeitliche Komponente und ist somit ein zentraler Bestandteil der vollständigen Umfelderkennung. 
Während die Objekterkennung festlegt, \textit{welche} Objekte in einem bestimmten Moment existieren, bestimmt die Verfolgung, \textit{wie} sich diese Objekte im Laufe der Zeit bewegen. 
Dadurch können Trajektorien, Geschwindigkeiten und Bewegungstrends ermittelt werden, was insbesondere für mobile Robotik- und Fahrerassistenzsysteme entscheidend ist \cite{barth2020tracking, radarvision2021fusion}.


\section{Kalman-Filter}

Zur kontinuierlichen Schätzung der Objektzustände wird ein zweidimensionaler Kalman-Filter eingesetzt. 
Der Zustandsvektor beschreibt die Position und Geschwindigkeit eines Objekts in der XY-Ebene:
\[
\mathbf{x} = [x, y, v_x, v_y]^{\mathrm{T}}.
\]
Das zugrunde liegende Bewegungsmodell geht von konstanter Geschwindigkeit aus, wobei die Systemdynamik durch die Zustandsübergangsmatrix \(\mathbf{F}\) definiert ist:
\[
\mathbf{F} =
\begin{bmatrix}
1 & 0 & \Delta t & 0 \\
0 & 1 & 0 & \Delta t \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}.
\]
Der Filter besteht aus einer Prädiktions- und einer Korrekturphase, die rekursiv ausgeführt werden \cite{welch1995kalman}. 
Rauschen und Unsicherheiten werden über die Kovarianzmatrizen \(\mathbf{Q}\) (Prozessrauschen) und \(\mathbf{R}\) (Messrauschen) modelliert. 
Die Verwendung des Kalman-Filters sorgt für eine robuste Schätzung auch bei unvollständigen oder verrauschten Detektionsdaten.


\section{Ungarisches Zuordnungsverfahren}

Da mehrere Objekte gleichzeitig erkannt werden können, muss jedes erkannte Objekt der entsprechenden bestehenden Spur (Track) zugeordnet werden. 
Hierfür wird das sogenannte \textit{Data Association Problem} gelöst. 
Die Kostenmatrix \(C_{ij}\) beschreibt den euklidischen Abstand zwischen einer vorhergesagten Spur \(i\) und einer neuen Messung \(j\):
\[
C_{ij} = \sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}.
\]
Die Minimierung der Gesamtkosten erfolgt mittels des ungarischen Algorithmus \cite{kuhn1955hungarian}, der eine optimale 1-zu-1-Zuordnung berechnet. 
Nur Paare mit einer Distanz kleiner als dem Gate-Wert (\(d_{\mathrm{max}} = 2\,\mathrm{m}\)) werden akzeptiert.

Jede Spur wird als \texttt{Track}-Objekt gespeichert und enthält die geschätzten Zustände, Bounding-Box-Parameter und Klassenzugehörigkeit (z.\,B. \textit{Mensch}, \textit{Fahrradfahrer}, \textit{Pkw}, \textit{Lkw}).  
Tracks, die mehrere Frames ohne neue Zuordnung überdauern, werden nach einer bestimmten Anzahl verpasster Frames (\texttt{max\_missed = 5}) gelöscht, während neu erkannte Objekte erst nach mehreren Bestätigungen (\texttt{min\_hits = 3}) als gültig betrachtet werden.  
Dieses Prinzip lehnt sich an den bekannten SORT-Tracker (\textit{Simple Online and Real-Time Tracking}) an \cite{bewley2016sort}.


\section{Umsetzung und Ergebnisse}

Die Implementierung erfolgt als ROS~2-Knoten \texttt{sort\_tracker\_node}, der die von der Cluster-Extraktion bereitgestellten Detektionen (\texttt{/detections\_raw}) abonniert und verfolgte Objekte als \texttt{/tracks\_raw} und \texttt{/tracks\_markers} publiziert.  

\begin{enumerate}
  \item \textbf{Datenaufnahme:} Empfangen der \texttt{vision\_msgs/Detection3DArray}-Nachrichten.
  \item \textbf{Vorhersage:} Alle bestehenden Kalman-Filter führen eine Prädiktion basierend auf der letzten Zeitschrittgröße \(\Delta t\) durch.
  \item \textbf{Zuordnung:} Berechnung der Kostenmatrix und Lösung des Matching-Problems mittels ungarischem Algorithmus.
  \item \textbf{Korrektur:} Aktualisierung der Kalman-Filter-Zustände für erfolgreich zugeordnete Tracks.
  \item \textbf{Track-Management:} Hinzufügen neuer Spuren, Löschen alter oder unzuverlässiger Tracks.
  \item \textbf{Klassifikation:} Heuristische Klassifikation basierend auf Bounding-Box-Abmessungen und Bewegung (z.\,B. \textit{Person}, \textit{Fahrradfahrer}, \textit{Pkw}, \textit{Lkw}).
  \item \textbf{Visualisierung:} Ausgabe von RViz-Markern zur Darstellung der verfolgten Objekte und deren IDs.
\end{enumerate}

Nach der Entfernung der Bodenpunkte mittels RANSAC wird die Objekterkennung ausschließlich auf die verbleibenden Punkte oberhalb der Bodenebene angewendet.
Da die Bodenhöhe somit bereits herausgefiltert ist, müssen die in der Klassifikation verwendeten Abmessungen der Objekte entsprechend angepasst werden.
Die in Tabelle \ref{tab:class_dims} dargestellten Werte berücksichtigen diese Anpassung und dienen der heuristischen Einteilung der detektierten Cluster in die vier Zielklassen: Mensch, Fahrradfahrer, Pkw und Lkw.

\begin{table}[h!]
  \centering
  \begin{tabular}{|l|c|c|c|}
    \hline
    \textbf{Objektklasse} & \textbf{Länge [m]} & \textbf{Breite [m]} & \textbf{Höhe [m]} \\ \hline
    Mensch         & 0.3 – 1.6  & 0.4 – 1.0  & 1.0 – 2.0 \\ \hline
    Fahrradfahrer  & 0.6 – 2.0  & 0.6 – 1.5  & 0.4 – 1.6 \\ \hline
    Pkw            & 2.0 – 4.5  & 1.4 – 3.0  & 0.6 – 2.1 \\ \hline
    Lkw            & 4.5 – 8.0  & 2.0 – 5.0  & 2.0 – 4.2 \\ \hline
  \end{tabular}
  \caption{Angepasste Abmessungsbereiche der Objektklassen nach der Bodenentfernung}
  \label{tab:class_dims}
\end{table}

Das entwickelte Tracking-System erlaubt eine stabile und konsistente Verfolgung mehrerer Objekte über aufeinanderfolgende Frames. 
Durch die Kombination von Kalman-Filter und ungarischem Algorithmus wird eine robuste Zuordnung auch bei zeitweise fehlenden Messungen gewährleistet. 

\chapter{Test und Bewertung}
\label{chap:test_und_bewertung}

In diesem Kapitel werden die Testergebnisse der entwickelten Umfelderkennungs-Pipeline vorgestellt und bewertet. 
Die Pipeline wurde vollständig in ROS~2 umgesetzt und besteht aus den in Abbildung~\ref{fig:ros2-pipeline-alltopics} gezeigten Modulen: 
\textit{Crop-Box-Filter}, \textit{VoxelGrid-Filter}, \textit{RANSAC-Bodensegmentierung}, \textit{Cluster-Extraktion} sowie \textit{Objektverfolgung (Tracking)}. 
Die Konfiguration erfolgte über das Launch-File \textit{ouster\_pipeline.launch.py}, welches alle Module innerhalb des gemeinsamen Namensraums \textit{/pipeline} startet.

Das System lief auf einem Ubuntu~22.04-Rechner mit ROS~2~Humble und einem AMD Ryzen 5 Prozessor (8~Kerne, 16~Threads). 
Zur Visualisierung und qualitativen Analyse wurde \textit{RViz} eingesetzt.  

\section{Testumgebung und Versuchsaufbau}
\label{sec:testumgebung}

Für die Evaluierung der entwickelten Messkette wurden eigens aufgezeichnete LiDAR-Datensätze des Ouster~OS1-Sensors verwendet.  
Die Aufnahmen erfolgten in verschiedenen urbanen Szenarien auf dem Campusgelände der Technischen Hochschule Nürnberg, 
darunter Parkflächen, Zufahrtsstraßen und Bereiche mit Fußgänger- und Fahrzeugverkehr.  Das Wetter am Tag war weder sonnig noch wolkenverhangen.
Ziel dieser Datensätze war es, typische Objekte und Strukturen einer realen Verkehrsumgebung – 
wie Personen, Fahrradfahrer, Pkw und Lkw – unter realistischen Bedingungen zu erfassen.  
Die Sensordaten wurden mit einer festen Sensorrate von 10\,Hz aufgenommen und als \textit{ROS\,2\,Bag}-Dateien gespeichert, 
um sie für wiederholte Tests und algorithmische Vergleichsstudien reproduzierbar einsetzen zu können.  

Für die Durchführung der Tests wurde der Terminal-Emulator \textbf{Terminator} verwendet, 
da er eine parallele Steuerung mehrerer ROS~2-Prozesse in getrennten Fenstern ermöglicht.  
In der Testkonfiguration wurden drei Terminals eingesetzt, die jeweils eine spezifische Aufgabe übernahmen:

\begin{itemize}
  \item \textbf{Terminal~1 – Datenwiedergabe:}  
        Wiedergabe der aufgezeichneten Sensordaten mit dem Befehl  
        \texttt{ros2 bag play <Name der ROSBag Datei> --loop}, um die Punktwolken kontinuierlich in das ROS-Netzwerk einzuspeisen.  
  \item \textbf{Terminal~2 – Arbeitsumgebung:}  
        Ausführung der Befehle \texttt{colcon build} und \texttt{source install/setup.bash}  
        zur Kompilierung und Aktivierung der ROS-Umgebung,  
        gefolgt vom Start der Messkette mittels \texttt{ros2 launch ouster\_pipeline.launch.py}.  
        Dadurch wurden die implementierten Nodes (CropBox-, Voxel-, RANSAC-, Cluster- und Tracking-Node) automatisch gestartet.  
  \item \textbf{Terminal~3 – Visualisierung:}  
        Start von \texttt{RViz2} zur grafischen Darstellung der verarbeiteten Punktwolken,  
        Bounding-Boxes und Objekttrajektorien.
\end{itemize}

Diese dreigeteilte Teststruktur ermöglichte eine klare funktionale Trennung zwischen 
Datenzufuhr, Verarbeitung und Visualisierung.  
Dadurch konnte das Systemverhalten in Echtzeit beobachtet und mögliche Abweichungen 
oder Laufzeitprobleme gezielt analysiert werden.

\section{Parametrierung und Einflussanalyse}
\label{sec:parametrierung}

Die Parametrierung der Messkette hat einen entscheidenden Einfluss auf die Genauigkeit, Robustheit und Echtzeitfähigkeit der Ergebnisse.  
Um geeignete Werte für die in Tabelle~\ref{tab:launch_params} aufgeführten Parameter zu bestimmen, 
wurde eine empirische Untersuchung durchgeführt, in der zentrale Parameter systematisch variiert und ihre Auswirkungen auf die Segmentierungs- und Tracking-Ergebnisse bewertet wurden.  

\subsection{Untersuchungsmethodik}
Für die Analyse wurde jeweils ein Parameter in einem definierten Wertebereich verändert, 
während die übrigen Parameter konstant blieben (sogenannte \textit{Einflussanalyse}).  
Die Bewertung erfolgte auf Basis visueller Vergleichsdaten in \textit{RViz} sowie durch Messung der Verarbeitungszeit pro Frame.  
Beispielsweise wurde der RANSAC-Abstandsschwellenwert \texttt{distance\_threshold} 
in Schritten von 0{,}1\,m im Bereich von 0\,m bis 1{,}0\,m variiert, 
um den optimalen Kompromiss zwischen Bodenentfernung und Punktverlust zu bestimmen.

Representative Aufzeichnung

\subsection{Parameterstudie}

\subsubsection{min\_bound und max\_bound}

Zum Ziel der Analyse der Einfluss der Größe der CropBox auf der effizienten Klassifizierung der Objekte werden die y-Parameter von textit{min\_bound} und textit{max\_bound} geändert. 



\subsection{Ergebnisse der Parameterstudie}

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabularx}{\textwidth}{|l|c|X|c|}
\hline
\textbf{Parameter} & \textbf{Wertebereich} & \textbf{Kriterium} & \textbf{Optimales Ergebnis} \\ \hline
\texttt{distance\_threshold} & 0–1{,}0\,m & Saubere Bodenentfernung, keine Objektverluste & 0{,}25\,m \\ \hline
\texttt{voxel\_size} & 0{,}05–0{,}5\,m & Echtzeitfähigkeit, Detailerhalt & 0{,}2\,m \\ \hline
\texttt{cluster\_tolerance} & 0{,}2–1{,}0\,m & Objekttrennung, keine Übersegmentierung & 0{,}5\,m \\ \hline
\texttt{gate\_dist\_max} & 2{,}0–6{,}0\,m & Zuverlässige Objektverfolgung & 4{,}0\,m \\ \hline
\end{tabularx}
\caption{Untersuchungsbereiche und ermittelte Optimalwerte der Hauptparameter.}
\label{tab:param_study}
\end{table}

Die Untersuchungen zeigen, dass insbesondere die Parameter \texttt{distance\_threshold} und \texttt{voxel\_size} 
einen maßgeblichen Einfluss auf die Gesamtergebnisse haben.  
Ein zu kleiner Schwellenwert bei der Bodenentfernung (\textless{}0{,}15\,m) führt dazu, dass 
Teile des Bodens als Objekte erkannt werden, während ein zu großer Wert (\textgreater{}0{,}4\,m) 
relevante Objektpunkte entfernt.  
Die Wahl von 0{,}25\,m erwies sich als stabiler Kompromiss.  
Ähnlich zeigte sich bei der Voxelgröße, dass Werte zwischen 0{,}15\,m und 0{,}25\,m 
eine gute Balance zwischen Rechenzeit und Detailgenauigkeit bieten.  
Werte über 0{,}3\,m führten zu einem erkennbaren Informationsverlust bei kleinen Objekten (z.\,B. Fußgänger).

Die gewählten Parameterwerte entsprechen somit einem ausgewogenen Kompromiss zwischen 
Echtzeitfähigkeit und Segmentierungsqualität.  
Eine dynamische Parametrierung, die sich an die lokale Punktdichte oder Geländebedingungen anpasst, 
könnte in zukünftigen Arbeiten zu einer weiteren Verbesserung führen.

\section{Bewertung}

\subsection{Testfälle}

\subsubsection{Ebenes Straßenstück mit wenigen Objekten}
\subsubsection{Leichte Steigung}
\subsubsection{Bereich mit Menschenmenge}
\subsubsection{Gemischter Verkehr}
\subsubsection{Teilverdeckten Objekten}
\subsubsection{Dynamische Ein‑ und Austrittsbereiche}

\subsection{Integrierbarkeit in der GUI}


Die entwickelte Pipeline konnte alle vier Zielobjekte — \textbf{Personen}, \textbf{Fahrradfahrer}, \textbf{Pkw} und \textbf{Lkw} — erfolgreich erkennen. 
Die Kombination aus RANSAC-Filterung und euklidischem Clustering ermöglichte eine stabile Trennung der Objekte in den meisten Szenarien. 
Die Bounding-Boxen wurden zuverlässig um die Objekte gelegt, und die nachgeschaltete Verfolgung sorgte für eine konsistente Bewegungsschätzung über mehrere Frames hinweg.

Allerdings traten in einigen Fällen \textbf{falsche Klassifikationen (False Positives)} auf. 
Diese betrafen insbesondere stationäre oder kleinformatige Objekte (z.\,B. Verkehrszeichen oder Bordsteinkanten), die aufgrund ihrer geometrischen Ähnlichkeit fälschlicherweise als \textit{Person} oder \textit{Fahrradfahrer} klassifiziert wurden. 
Diese Fehlzuordnungen resultieren hauptsächlich aus der rein heuristischen Klassifikationslogik, die auf den Bounding-Box-Dimensionen basiert. 
Eine Kombination aus Form- und Bewegungsmerkmalen könnte die Genauigkeit hier weiter verbessern \cite{yang2021lidartracking}.

Das Tracking auf Basis des linearen Kalman-Filters und der ungarischen Zuordnung zeigte in den Tests eine hohe Stabilität. 
Bewegte Objekte konnten auch über kurzzeitige Abschattungen oder Sensoraussetzer hinweg korrekt verfolgt werden. 
Die Echtzeitfähigkeit wurde durch die geringe Rechenkomplexität der einzelnen Module gewährleistet: auf der Testplattform lag die durchschnittliche Verarbeitungsgeschwindigkeit bei etwa \textbf{15–20~Hz}.

Schwierigkeiten traten vereinzelt bei stark überlappenden Objekten auf, insbesondere wenn zwei Objekte (z.\,B. Personen nebeneinander) in der Punktwolke zu einem gemeinsamen Cluster verschmolzen. 
Hierdurch kam es zu Track-Sprüngen oder Objektfusionen. 
Ein zukünftig denkbarer Lösungsansatz wäre der Einsatz eines \textit{Multi-Hypothesen-Trackers} oder eines \textit{Joint Probabilistic Data Association Filters (JPDAF)} \cite{bar2004tracking, blackman1999design}.

Die Pipeline erreicht eine \textbf{hohe Robustheit bei dynamischen Szenen} und liefert ein konsistentes Umgebungsmodell in Echtzeit. 
Die modular aufgebaute Architektur erleichtert die Anpassung an unterschiedliche Sensorkonfigurationen und Szenarien. 
Abbildung~\ref{fig:pipeline_rviz} zeigt eine exemplarische Ausgabe in \texttt{RViz2}, in der die erkannten Objekte mit Bounding-Boxes und Tracking-IDs dargestellt sind.

Trotz der insgesamt guten Erkennungsleistung bleibt die \textbf{Klassifikationsgenauigkeit} ein zentrales Verbesserungspotential. 
Eine zukünftige Erweiterung könnte durch den Einsatz lernbasierter Verfahren (z.\,B. \textit{PointNet++} oder \textit{VoxelNet}) erfolgen, die semantische Informationen aus den Punktwolken extrahieren \cite{qi2017pointnetplusplus, zhou2018voxelnet}. 
Dies würde die Fähigkeit des Systems zur Differenzierung ähnlicher Objektformen deutlich erhöhen.

Die Tests zeigen, dass die entwickelte Pipeline in der Lage ist, komplexe Umgebungen mit LiDAR-Daten effizient zu analysieren und mehrere bewegte Objekte gleichzeitig zu erkennen und zu verfolgen. 
Die Echtzeitfähigkeit, Modularität und Robustheit gegenüber Störungen bestätigen die Praxistauglichkeit des Ansatzes. 
Die beobachteten False Positives weisen jedoch darauf hin, dass für eine zuverlässige Klassifikation eine Kombination aus geometrischen, kinematischen und semantischen Merkmalen erforderlich ist.

% ====== 8 Zusammenfassung und Ausblick ======
\chapter{Zusammenfassung und Ausblick}
% Inhalt hier ergÃ¤nzen
\section{Zusammenfassung}
\section{Ausblick}

% ====== Literaturverzeichnis ======
\cleardoublepage
\printbibliography[title=Literaturverzeichnis]

% ====== Anhang ======
\appendix
\chapter{Anhang A}
% Anhang-Inhalte
\chapter{Anhang B}
% Anhang-Inhalte

\label{LastPage}
\end{document}

