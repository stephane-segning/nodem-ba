
% !TeX program = pdflatex
\documentclass[12pt,a4paper,oneside]{scrreprt}

% ---------- Packages ----------
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{lmodern}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{underscore}
\usepackage{fancyhdr}


\usepackage{csquotes}
\usepackage{tikz}
\usetikzlibrary{shapes.multipart}
\usepackage{tabularx}
\newcolumntype{Y}{>{\raggedright\arraybackslash}X}
\usepackage{array}
\usepackage{enumitem}
\usepackage{listings}
\usepackage[printonlyused]{acronym}
\usepackage{tocbasic} % better control of ToC lists
\usepackage{scrlayer-scrpage} % recommended for KOMA-Script classes
\usepackage{lastpage}
\usepackage{titling}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{float}
\usepackage{pgf-pie}
\usepackage{siunitx}
\usepackage{pgfplots}
\usepackage[a4paper, left=3cm, right=2cm, top=2cm, bottom=3cm]{geometry}
\pgfplotsset{compat=1.18}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}
\lstset{basicstyle=\ttfamily\small, numbers=left, numbersep=5pt, frame=single, breaklines=true}

% ---------- Page Setup ----------
\onehalfspacing
\setkomafont{sectioning}{\normalfont\bfseries}
\setkomafont{disposition}{\normalfont\bfseries}
\renewcommand{\arraystretch}{1.2}

% ---------- Header / Footer ----------
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyhead[L]{\nouppercase{\leftmark}} % aktueller Kapitelname
\fancyhead[R]{\includegraphics[height=1cm]{Bilder/TH_Nuernberg_Logo.png}} % passe den Pfad an!
\fancyfoot[C]{\thepage\ / \pageref{LastPage}}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

% --- Optional: Stil für Kapitelanfangsseiten (ohne Logo) ---
\fancypagestyle{plain}{
  \fancyhead{}
  \fancyfoot[C]{\thepage}
  \renewcommand{\headrulewidth}{0pt}
}

\begin{titlepage}
\begin{center}

% --- Logo OHM + Fakultät ganz oben ---
\includegraphics[width=0.75\textwidth]{Bilder/ohm.png}\\[1cm]

% --- Großes IFZN-Logo in der Mitte ---
\includegraphics[width=0.75\textwidth]{Bilder/IFZN.png}\\[1cm]

% --- Arbeitstyp ---
{\Large \textbf{Bachelorarbeit}}\\[1cm]

% --- Titel im Rahmen (ähnlich zur Radar-Arbeit) ---
\setlength{\fboxrule}{0.8pt}
\setlength{\fboxsep}{10pt}
\begin{center}
\fbox{%
  \parbox{0.9\textwidth}{\centering
    \textbf{\large
    Entwicklung und Evaluierung von Algorithmen zur Umfelderkennung auf Basis\\[0.2cm]
    hochauflösender 360°-LiDAR-Sensordaten}
  }
}
\end{center}

\vspace{1cm}

% --- Autor / Gutachter / Ort & Datum ---
\begin{tabular}{ll}
\textbf{Autor:}            & Ingrid Nodem Lambou \\[0.2cm]
\textbf{Erstgutachter:}    & Prof.\,Dr.\,Christina Singer \\[0.2cm]
\textbf{Zweitgutachter:}   & Prof.\,Dr.\,Christian Pfitzner \\[0.2cm]
\textbf{Ort, Abgabetermin:} & Nürnberg, 30.11.2025 \\
\end{tabular}

\end{center}
\end{titlepage}

%-----------Literatur--------------
\usepackage[backend=biber,style=authoryear,sorting=nyt,maxbibnames=99]{biblatex}
\addbibresource{literatur.bib} % nom exact du .bib

%--Document can have many numberedd subsection---
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3} % (optionnel : pour qu'elles apparaissent aussi dans la table des matières)

% --- TikZ für Flussdiagramme ---
\usetikzlibrary{shapes.geometric, arrows, positioning}

% --- Stile für Flowchart-Knoten und Pfeile ---
\tikzset{
  startstop/.style = {rectangle, rounded corners, minimum width=3.2cm,
    minimum height=1.0cm, text centered, draw=black, fill=red!30},
  process/.style   = {rectangle, minimum width=3.6cm, minimum height=1.0cm,
    text centered, draw=black, fill=orange!30},
  arrow/.style     = {thick,->,>=stealth}
}

% ---------- Document ----------
\DeclareUnicodeCharacter{2212}{\textminus}
\usepackage{textcomp}
\begin{document}
\pagenumbering{Roman}

% --- Offizielles Deckblatt ---
\cleardoublepage
\thispagestyle{plain}

\section*{Offizielles Deckblatt}

\vspace{0.8cm}

\begin{tabular}{@{}ll}
\textbf{Bearbeiter:}       & Ingrid Nodem Lambou \\[0.25cm]
\textbf{Matrikel-Nr.:}     & 3584699 \\[0.8cm]

\textbf{Studiengang:}      & Mechatronik \\[0.25cm]
\textbf{Studienschwerpunkt:} & Fahrzeugtechnik \\[0.8cm]

\textbf{Erstprüfer:}       & Prof.\,Dr.\,-Ing. Christina Singer \\[0.25cm]
\textbf{Zweitprüfer:}      & Prof.\,Dr.\,-Ing. Christian Pfitzner \\[0.8cm]

\textbf{Durchgeführt beim:} & Institut für Fahrzeugtechnik Nürnberg (IFZN) \\[0.8cm]

\textbf{Betreuer:} & \begin{tabular}[t]{@{}l@{}}
Dipl.\ Ing.\ Ralf Falgenhauer \\ 
0911\,/\,58801427 \\ 
ralf.falgenhauer@th-nuernberg.de
\end{tabular} \\[1.5cm]

\textbf{Ausgabedatum:}     & 01.06.2025 \\
\textbf{Abgabedatum:}      & 29.11.2025 \\
\end{tabular}

\vspace{1.1cm}

\textbf{Thema der Arbeit:}\\[0.25cm]
Entwicklung und Evaluierung von Algorithmen zur Umfelderkennung auf Basis\\
hochauflösender 360°-LiDAR-Sensordaten

\vspace{1.5cm}

Die Arbeit ist frei einsehbar: Ja: $\square$\quad Nein: $\boxtimes$ \\[0.5cm]

Die Arbeit darf nur mit Zustimmung von: Prof.\,Dr.\,Christina Singer, christina.singer@th-nuernberg.de eingesehen werden.

% ====== Kurzfassung / Zusammenfassung ======
\chapter*{Kurzfassung}
\addcontentsline{toc}{chapter}{Kurzfassung}
\noindent
Die vorliegende Arbeit entwickelt eine modulare ROS2-Verarbeitungskette für die \ac{LiDAR}-basierte Umfelderkennung des Projektautomobils \textit{CarCeptionX}. Aufbauend auf der Ouster-\ac{OS1}-Messkette werden die Stufen Zuschnitt (CropBox), Voxel-Filter, RANSAC-Bodensegmentierung, Cluster-/Bounding-Box-Bildung und ein kalmanfilterbasiertes Tracking zu einer durchgängigen Pipeline verknüpft. Ziel ist eine echtzeitfähige Erkennung von Verkehrsteilnehmern bei mindestens \SI{90}{\percent} Detektionsrate.

Eine Parameterstudie zu Voxelgröße, RANSAC-Distanzschwelle und Cluster-Toleranz zeigt, dass moderate Einstellungen die robustesten Ergebnisse liefern: \(\textit{voxel\_size} \approx 0{,}15{-}0{,}20\,\text{m}\), \(\textit{distance\_threshold} \approx 0{,}15\,\text{m}\) und \(\textit{cluster\_tolerance} \approx 0{,}50\,\text{m}\) vermeiden sowohl Übersegmentierung als auch das Verschmelzen benachbarter Objekte. Damit erreicht die Pipeline in unterschiedlichen Szenarien Detektionszahlen, die eng an den realen Objektzahlen liegen.

Fahrtests auf gerader Strecke, in Kurven und auf unebenen Stadtabschnitten bestätigen die Echtzeitfähigkeit: Die End-to-End-Latenz von zugeschnittenen Punktwolken bis zu den Tracks liegt je nach Szenario zwischen rund \SI{20}{\milli\second} und \SI{86}{\milli\second} bei CPU-Lasten unter \SI{35}{\percent} und einem Speicherbedarf um \SI{200}{\mega\byte}. Gleichzeitig bleibt die Ausgaberate der Tracks im Bereich von \SIrange{1.8}{3.0}{\hertz}; nur in statischen Szenen treten kurzlebige Track-IDs auf, die auf weiteres Feintuning des Track-Managements hindeuten, die Kernanforderungen aber nicht beeinträchtigen.

In realen Messszenarien auf urbanen Teststrecken zeigt die Verarbeitungskette stabile Resultate: Die Bodensegmentierung trennt befahrbare Fläche und Hindernisse verlässlich, die Clusterbildung erzeugt weitgehend trennscharfe Objektboxen, und das Tracking liefert kohärente Trajektorien auch bei kurzzeitigen Abschattungen. Die gemessenen Verarbeitungsraten liegen je nach Szene bei \SIrange{2.5}{3.7}{\hertz} für die Punktwolkenstufen und \SIrange{1.8}{3.0}{\hertz} für die Track-Ausgabe, womit die geforderten Echtzeitanforderungen im Testaufbau eingehalten werden. Die Arbeit schließt mit einer Bewertung der erzielten Leistung und einem Ausblick auf Erweiterungen wie Sensorfusion, semantische Segmentierung und eine engere Integration in das Fahrzeugsystem.

\cleardoublepage

% ====== Inhalts- und Verzeichnisse ======
\tableofcontents
\cleardoublepage

\listoffigures
\addcontentsline{toc}{chapter}{Abbildungsverzeichnis}
\cleardoublepage

\listoftables
\addcontentsline{toc}{chapter}{Tabellenverzeichnis}
\cleardoublepage

\chapter*{Abkürzungsverzeichnis}
\addcontentsline{toc}{chapter}{Abkürzungsverzeichnis}
\begin{acronym}[LiDAR]
    \acro{BEV}{Bird's Eye View}
    \acro{CV}{Constant Velocity}
    \acro{DBSCAN}{Density-Based Spatial Clustering of Applications with Noise}
    \acro{EMA}{Exponentielle gleitende Mittelung}
    \acro{FOV}{Field of View}
    \acro{FPS}{Frames per Second}
    \acro{HOTA}{Higher Order Tracking Accuracy}
    \acro{ICP}{Iterative Closest Point}
    \acro{IDF1}{ID-based F1-Score}
    \acro{IFZN}{Institut für Fahrzeugtechnik Nürnberg}
    \acro{IoU}{Intersection over Union}
    \acro{KF}{Kalman-Filter}
    \acro{LiDAR}{Light Detection and Ranging}
    \acro{MOTA}{Multiple Object Tracking Accuracy}
    \acro{MOTP}{Multiple Object Tracking Precision}
    \acro{NDT}{Normal Distributions Transform}
    \acro{OS1}{Ouster OS1 LiDAR}
    \acro{PCL}{Point Cloud Library}
    \acro{QoS}{Quality of Service}
    \acro{RANSAC}{Random Sample Consensus}
    \acro{ROI}{Region of Interest}
    \acro{ROS}{Robot Operating System}
    \acro{RViz}{ROS Visualization Tool}
    \acro{SDK}{Software Development Kit}
    \acro{SLAM}{Simultaneous Localization and Mapping}
    \acro{ToF}{Time of Flight}
    \acro{TTL}{Time To Live}
\end{acronym}


\cleardoublepage

\pagenumbering{arabic}

% ====== 1 Einleitung ======
\chapter{Einleitung}

\section{Hinführung}

Autonome und hochautomatisierte Fahrfunktionen sind auf eine präzise, robuste und nachvollziehbare Umfelderfassung angewiesen, um sicherheitskritische Entscheidungen treffen zu können. Während Kameras vor allem für semantische Informationen genutzt werden und Radar Distanz sowie Relativgeschwindigkeit zuverlässig erfasst, liefert \ac{LiDAR} dichte, dreidimensionale Punktwolken mit hoher Winkelauflösung und weitgehend konstanter Leistungsfähigkeit bei unterschiedlichen Lichtverhältnissen \parencite{Arnold2019Survey}. Dadurch bildet \ac{LiDAR} in modernen Sensorarchitekturen einen zentralen Baustein der Umfeldwahrnehmung.

Im Projekt \textit{CarCeptionX} wird ein speziell ausgestattetes Versuchsfahrzeug eingesetzt, um die Leistungsfähigkeit automobiler Umfeldsensorik systematisch zu analysieren und neue Einsatzpotenziale zu erschließen (\cite{IFZN_Projekte}). Das Ziel des Projekts ist es, auf Basis realer Messdaten sowohl die sensorische Leistungsgrenzen als auch mögliche Optimierungspotenziale zu identifizieren, um langfristig einen Beitrag zu einer sicheren, effizienten und nachhaltigen Mobilität zu leisten. Der verwendete Versuchsträger (Opel Astra) ist hierfür mit einem umfassenden Sensorsetup ausgestattet, bestehend aus:

\begin{itemize}
    \item Ouster \ac{OS1} 360°-\ac{LiDAR},
    \item Valeo Scala~2 \ac{LiDAR},
    \item Bosch General Purpose Radar,
    \item Valeo Fisheye-Kamera,
    \item Valeo Ultraschallsensor-Kit (12~Sensoren).
\end{itemize}

Am Institut für Fahrzeugtechnik Nürnberg (IFZN) wurde die \ac{LiDAR}-basierte Umfelderfassung bereits in mehreren Arbeiten untersucht. Einen wesentlichen Grundstein legte die Masterarbeit von Wendel~(2025), in der eine Sensorplattform auf dem Opel Astra aufgebaut und eine erste \ac{ROS}\,2-basierte Messkette zur Erfassung und Visualisierung von Punktwolken entwickelt wurde \parencite{Wendel2025}. Diese bestehende Infrastruktur – bestehend aus Datenerfassung, \ac{ROS}\,2-Publish/Subscribe-Architektur, MATLAB-Anbindung über eine GUI sowie Visualisierung in \textit{RViz2} – bildet das Fundament der vorliegenden Arbeit.

Die anschließende Arbeit von Sagdic~(2025) erweiterte diese Grundlage um die vollständige Anbindung und initiale Inbetriebnahme des Ouster~\ac{OS1} auf dem Versuchsträger und dokumentierte die grundlegenden Funktions- und Schnittstellenprüfungen \parencite{Sagdic2025}. Auf dieser Basis setzt die vorliegende Arbeit auf, indem sie die bisher vorhandene Messkette um ein vollständiges, echtzeitfähiges Verarbeitungssystem ergänzt. 

Im Mittelpunkt steht dabei nicht der Sensor selbst, sondern die systematische Entwicklung, Implementierung und Bewertung der einzelnen Verarbeitungsschritte, die notwendig sind, um aus den Rohpunktwolken des Ouster~\ac{OS1} eine robuste Objekterkennung abzuleiten. Hierzu gehören die Vorverarbeitung der Punktwolken, eine zuverlässige Bodensegmentierung, die Clusterbildung zur Objektabgrenzung sowie die anschließende zeitliche Verfolgung der detektierten Objekte. Ziel ist es, die gesamte Verarbeitungskette so auszugestalten, dass sie unter realen urbanen Messbedingungen zuverlässig funktioniert und im Rahmen zukünftiger Forschungsaktivitäten wie \textit{CarCeptionX} nahtlos einsetzbar ist.

Zur Umsetzung dieser modularen und reproduzierbaren Verarbeitung wird das Robot Operating System~2 (\ac{ROS}~2) eingesetzt, das mit standardisierten Nachrichtenformaten, deterministischen Kommunikationsmechanismen und integrierten Visualisierungstools ein etabliertes Softwareframework für mobile Robotersysteme darstellt \parencite{Macenski2022ROS2,ROS2Docs}. Die vorliegende Arbeit erweitert die bestehende Messkette des \ac{OS1} um zusätzlich implementierte Filter-, Segmentierungs- und Trackingmodule, welche die Punktwolke schrittweise in semantisch verwertbare Objektlisten überführen und damit einen funktionsfähigen Beitrag zur Weiterentwicklung des Versuchsträgers leisten.

\section{Themenspezifizierung und Abgrenzung}

Ziel dieser Arbeit ist die Entwicklung und Evaluierung einer modularen, echtzeitfähigen Verarbeitungskette zur \ac{LiDAR}-basierten Umfelderkennung auf Basis des hochauflösenden Ouster~\ac{OS1}. Unter realen Bedingungen soll diese Verarbeitungskette spezifische Verkehrsteilnehmer ab einer Größe von~$\geq 0{,}5\,\text{m}$ mit einer Detektionsrate von mindestens $90\,\%$ bei einer minimalen Ausführungsrate von $10\,\text{Hz}$ erkennen.

 Im Mittelpunkt stehen dabei mehrere klar abgegrenzte Arbeitsschritte: die qualitätssteigernde Vorverarbeitung der Rohpunktwolken zur Reduktion von Rauschen und Ausreißern sowie zur effizienten Datenreduktion, eine verlässliche und szenariounabhängige Bodensegmentierung, die eine stabile Trennung zwischen befahrbarer Fläche und Hindernissen ermöglicht, die extraktionssichere Bildung von Clustern und konsistenten Bounding-Boxen zur Objektabgrenzung sowie die anschließende zeitliche Verfolgung der erkannten Objekte. Die Implementierung erfolgt vollständig in \ac{ROS}~2 mit dem Anspruch, eindeutig definierte Schnittstellen, reproduzierbare Experimente und messbare Leistungskennzahlen hinsichtlich Latenz, Frequenz und Genauigkeit zu gewährleisten.

Nicht Bestandteil der Arbeit sind Themenfelder, die zusätzliche Sensorquellen, umfangreiche Trainingsdaten oder andere Zielsetzungen erfordern. Dazu zählen insbesondere Sensorfusion mit Radar- oder Kamerasystemen, tiefenlernbasierte semantische Segmentierung, hardwareseitige Sensorauslegung sowie herstellerübergreifende Performancevergleiche. Ebenfalls ausgeschlossen wird das Gebiet des \textit{Simultaneous Localization and Mapping} (\ac{SLAM}). \ac{SLAM} umfasst die gleichzeitige Schätzung der Fahrzeugpose und den Aufbau einer konsistenten Umgebungskarte. Obwohl \ac{LiDAR} hierfür ein zentrales Sensormedium darstellt, verfolgt diese Arbeit weder den Aufbau globaler Karten noch die Minimierung von Lokalisierungsfehlern. Der Fokus liegt ausschließlich auf der lokalen Umfeldwahrnehmung zur Objekterkennung, die ohne globale Konsistenzbedingungen oder kartengestützte Optimierung auskommt; die Kalibrierung beschränkt sich daher auf die notwendige Extrinsik und Zeitsynchronisation.

\section{Beitrag der Arbeit, Vorgehen und Aufbau}

Die Arbeit liefert einen wissenschaftlich und technisch fundierten Beitrag zur \ac{LiDAR}-basierten Umfelderfassung, indem eine modular aufgebaute und echtzeitfähige \ac{ROS}~2-Verarbeitungsstrecke entwickelt, implementiert und systematisch untersucht wird. Der Schwerpunkt liegt auf (i) einer qualitätssteigernden Vorverarbeitung, (ii) einer präzisen Trennung von Boden- und Objektpunkten, (iii) einer verlässlichen Cluster- und Bounding-Box-Ermittlung sowie (iv) einer konsistenten Objektverfolgung. Ein weiterer Beitrag besteht in der strukturierten Gegenüberstellung verschiedener Boden- und Clusterverfahren anhand nachvollziehbarer Metriken wie Punktzahlreduktion, Stabilität der Segmentierung, Clustertrennschärfe und zeitlichem Aufwand. Sämtliche Experimente werden reproduzierbar dokumentiert und mit klar definierten Parametern durchgeführt.

Die methodische Herangehensweise integriert diese Beiträge unmittelbar in den Aufbau der Arbeit. Sie beginnt mit einer systematischen Literaturauswertung und Ableitung funktionaler Anforderungen. Darauf folgt die gezielte Datenerhebung mit dem Ouster~\ac{OS1} sowie die Implementierung der einzelnen Funktionsbausteine der Verarbeitungsstrecke. Anschließend werden diese Bausteine in \ac{ROS}~2 integriert und hinsichtlich Ausführungsmodell, Datendurchsatz und \ac{QoS}-Profilen abgestimmt. In der experimentellen Phase werden die Verfahren unter definierten Szenarien untersucht, wobei insbesondere Segmentierungsgenauigkeit, räumliche Trennschärfe, Punktverluste, Stabilität der Objektverfolgung sowie der zeitliche Aufwand pro Verarbeitungsschritt analysiert werden. Dieser integrierte Ansatz ermöglicht eine durchgängige Bewertung der im Rahmen der Arbeit entwickelten Komponenten.

Abbildung~\ref{fig:methodik} zeigt die erweiterte methodische Vorgehensweise, in der die inhaltlichen Beiträge der Arbeit direkt verortet sind.

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=7mm, >=latex]
\node (lit) [rectangle, draw, rounded corners, align=center, inner sep=3pt]
    {Literaturrecherche \\ \& Anforderungsanalyse};
\node (data) [rectangle, draw, rounded corners, below=of lit, align=center, inner sep=3pt]
    {Datenerhebung \\ mit Ouster \ac{OS1}};
\node (impl) [rectangle, draw, rounded corners, below=of data, align=center, inner sep=3pt]
    {Implementierung der \\ Verarbeitungsbausteine};
\node (integ) [rectangle, draw, rounded corners, below=of impl, align=center, inner sep=3pt]
    {Integration in \ac{ROS}\,2};
\node (eval) [rectangle, draw, rounded corners, below=of integ, align=center, inner sep=3pt]
    {Experimentelle Evaluierung \\ nach definierten Metriken};
\node (disc) [rectangle, draw, rounded corners, below=of eval, align=center, inner sep=3pt]
    {Vergleich der Verfahren \\ \& Analyse der Ergebnisse};

\draw[->] (lit) -- (data);
\draw[->] (data) -- (impl);
\draw[->] (impl) -- (integ);
\draw[->] (integ) -- (eval);
\draw[->] (eval) -- (disc);
\end{tikzpicture}
\caption{Methodische Vorgehensweise und Einbettung der Beiträge der Arbeit}
\label{fig:methodik}
\end{figure}

Zur Orientierung zeigt Abbildung~\ref{fig:aufbau} den strukturellen Aufbau der Arbeit. Kapitel~2 stellt den Stand der Technik vor. Kapitel~3 beschreibt die Systemarchitektur und die technische Einbettung der entwickelten Verarbeitungsstrecke. Kapitel~4 widmet sich der Vorverarbeitung der Punktwolken, gefolgt von der Bodensegmentierung in Kapitel~5. Kapitel~6 behandelt die Cluster-Extraktion und die Ermittlung von Bounding-Boxes. Kapitel~7 beschreibt die Objektverfolgung. Kapitel~8 umfasst die Tests und die Bewertung der Ergebnisse, bevor Kapitel~9 die Erkenntnisse zusammenfasst und einen Ausblick gibt.

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=6mm, >=latex]
\node (k2) [rectangle, draw, rounded corners, inner sep=3pt] {Kapitel 2: Stand der Technik};
\node (k3) [rectangle, draw, rounded corners, below=of k2, inner sep=3pt] {Kapitel 3: Systemarchitektur};
\node (k4) [rectangle, draw, rounded corners, below=of k3, inner sep=3pt] {Kapitel 4: Vorverarbeitung};
\node (k5) [rectangle, draw, rounded corners, below=of k4, inner sep=3pt] {Kapitel 5: Bodensegmentierung};
\node (k6) [rectangle, draw, rounded corners, below=of k5, inner sep=3pt] {Kapitel 6: Cluster \& Bounding-Boxes};
\node (k7) [rectangle, draw, rounded corners, below=of k6, inner sep=3pt] {Kapitel 7: Objektverfolgung};
\node (k8) [rectangle, draw, rounded corners, below=of k7, inner sep=3pt] {Kapitel 8: Tests \& Bewertung};
\node (k9) [rectangle, draw, rounded corners, below=of k8, inner sep=3pt] {Kapitel 9: Fazit \& Ausblick};

\draw[->] (k2) -- (k3);
\draw[->] (k3) -- (k4);
\draw[->] (k4) -- (k5);
\draw[->] (k5) -- (k6);
\draw[->] (k6) -- (k7);
\draw[->] (k7) -- (k8);
\draw[->] (k8) -- (k9);
\end{tikzpicture}
\caption{Struktur der Arbeit}
\label{fig:aufbau}
\end{figure}



% ====== 2 Wissenschaftliche Grundlagen ======
\chapter{Stand der Technik und Wissenschaft}
\label{chap:stand-der-technik}

Moderne Fahrerassistenzsysteme und autonome Fahrfunktionen kombinieren Kamera-, Radar- und \ac{LiDAR}-Sensorik. Kameras liefern hochauflösende Bildinformationen für semantische Aufgaben, Radar überzeugt durch robuste Distanz- und Geschwindigkeitsmessungen bei widrigen Wetterbedingungen. \ac{LiDAR} ergänzt diese Systeme um präzise, dreidimensionale Punktwolken mit hoher Winkelauflösung, die auch bei wechselnden Lichtverhältnissen konstant verfügbar sind \parencite{beuth2021handbuch}. Dadurch entsteht ein Sensorverbund, der semantische, dynamische und geometrische Informationen gleichermaßen abdeckt.

Das Akronym \emph{\ac{LiDAR}} beschreibt optische Messverfahren, die Lichtimpulse aussenden, deren Laufzeit bis zur Reflexion erfassen und daraus Entfernungen sowie Strukturen in drei Dimensionen rekonstruieren \parencite{mcmanamon2015fieldguide}. Nach ersten Anwendungen in Geodäsie und Fernerkundung seit den 1960er-Jahren ermöglichte der Fortschritt in Lasertechnik und Miniaturisierung die Übertragung in die Automobilindustrie, wo \ac{LiDAR} heute eine Schlüsselrolle für hochautomatisierte Funktionen einnimmt \parencite{beuth2021handbuch}. Im automobilen Sensorsystemverbund liefert \ac{LiDAR} die geometrische Präzision, mit der Kamera- und Radarinformationen redundant oder ergänzend genutzt werden können.

Damit ist \ac{LiDAR} insbesondere für Hinderniserkennung, Bahnplanung und robuste Umfeldrepräsentation wertvoll. Der folgende Abschnitt beschreibt den Sensoraufbau, anschließend werden Wellenlängenbereiche und Messprinzip erläutert. Diese Bausteine bilden die experimentelle Grundlage der Ouster-Plattform, auf der die Arbeit aufsetzt. Dieses Kapitel stellt die wissenschaftlichen Grundlagen der \ac{LiDAR}-Sensorik zusammen, verortet sie im automobilen Kontext und leitet daraus die für die Verarbeitungskette relevanten Anforderungen ab. Im Mittelpunkt stehen die Messprinzipien, der historische Entwicklungspfad, die Abgrenzung zu Kamera und Radar sowie der Aufbau des in dieser Arbeit genutzten Sensors.

\section{Sensoraufbau}
\label{sec:sensoraufbau}

Ein \ac{LiDAR}-Sensor umfasst Sendeoptik und Strahlablenkung, Empfangsoptik und Lichtsensor sowie Auslese- und Steuerungselektronik (vgl. Abb.~\ref{fig:sensoraufbau}). Die nachfolgenden Abschnitte skizzieren den Beitrag der jeweiligen Komponenten zum Gesamtsystem.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{Bilder/Sensoraufbau.png}
    \caption{Schematischer Aufbau eines \ac{LiDAR}-Sensors mit Sendeoptik, 
    Empfangsoptik, Strahlablenkung sowie Steuer- und Auswerteeinheit (\cite{beuth2021handbuch}).}
    \label{fig:sensoraufbau}
\end{figure}

Vom Ausgang der Laserquelle gelangt das Licht zunächst in die Sendeoptik. 
Diese formt das Intensitätsprofil des Strahls und bestimmt somit indirekt 
die erreichbare räumliche Auflösung des Sensors. Häufig werden refraktive 
Linsen oder diffraktive optische Elemente (DOE) eingesetzt, welche die 
gewünschte Strahlform erzeugen. Für rasternde Systeme übernehmen Spiegel 
oder Prismen die Strahlablenkung durch rotierende oder oszillierende 
Bewegungen. Im Gegensatz dazu nutzen sogenannte Flash-Systeme DOEs oder 
Diffusoren, um die Szene pro Aufnahme vollständig auszuleuchten (\cite{mcmanamon2015fieldguide}).

Die Strahlablenkung erzeugt die Winkelinformation und legt fest, welcher Raumwinkel zu einem bestimmten Zeitpunkt erfasst wird. Sie ermöglicht die sukzessive Abtastung der Umgebung und beeinflusst mit ihrer Genauigkeit und Synchronisation direkt die geometrische Konsistenz der resultierenden Punktwolke (\cite{schuldt2020phd}).

Die Empfangsoptik übernimmt die Aufgabe, das vom Zielobjekt diffus oder spiegelnd 
zurückgestreute Licht möglichst verlustarm einzusammeln und dem Detektorsystem 
zuzuführen. Entscheidend dafür sind \emph{Aperturgröße} und \emph{Transmission}, 
die gemäß der \ac{LiDAR}-Gleichung direkt auf die empfangene Strahlungsleistung wirken 
und damit die Reichweite sowie die Detektierbarkeit schwacher Signale bestimmen. 
Um Reflexionsverluste zu reduzieren, werden optische Komponenten mit 
Antireflexbeschichtungen (AR-Coatings) versehen. Ergänzend sorgen 
sperrbandselektive Filter -- deren Durchlässigkeit exakt auf die Laserwellenlänge 
abgestimmt ist -- für eine wirksame Unterdrückung von Umgebungslicht und erhöhen 
somit die Robustheit gegenüber Sonnenlicht (\cite{wolter2021optoelektronik}).

Als Detektoren kommen in automobilen \ac{LiDAR}-Systemen typischerweise Avalanche-Photodioden (APD) oder Single-Photon-Avalanche-Diodes (SPAD-Arrays) zum Einsatz. APDs bieten durch ihren internen Lawineneffekt eine hohe Verstärkung und eignen sich zur Detektion schwacher Rückstreusignale bei gleichzeitig guter zeitlicher Auflösung. SPAD-Arrays erfassen einzelne Photonen und erreichen extrem kurze Anstiegszeiten, wodurch sie eine sehr präzise Laufzeitmessung unterstützen. Diese Eigenschaften sind entscheidend für die Genauigkeit des Time-of-Flight-Prinzips (vgl. Kap.~\ref{sec:dTOF} und ~\ref{sec:iTOF}) und wirken sich direkt auf die präzise Rekonstruktion der Objektentfernung aus (\cite{lemkin2020spad}).

Die vom Detektor erzeugten elektrischen Signale werden anschließend durch die 
Ausleseelektronik verarbeitet. Diese umfasst Verstärker, Analog-Digital-Wandler 
(ADC), Zeit-zu-Digital-Wandler (TDC) und Multiplexer. Ihre Aufgabe besteht darin, 
die Signale zu digitalisieren und die Laufzeitinformationen für jeden 
Messpunkt zu bestimmen. Moderne Systeme berechnen bereits auf Sensorebene 
Distanzwerte und können weitergehende Vorverarbeitungen wie die Bildung von 
Objektboxen oder erste Klassifikationen durchführen (\cite{behroozpour2017adc}).

Eine zentrale Rolle spielt die Steuerungselektronik, welche die Synchronisation 
aller Komponenten sicherstellt. Sie sorgt dafür, dass der Detektor nur während 
des Aussendens eines Laserpulses aktiv ist, um unnötige Rauschsignale durch 
Umgebungslicht zu vermeiden. Ebenso steuert sie die Strahlablenkung, sodass 
jeder Messpunkt dem korrekten Raumwinkel zugeordnet werden kann (\cite{beuth2021handbuch}).

Der \ac{LiDAR}-Sensoraufbau vereint optische, elektronische und algorithmische 
Bausteine zu einem Gesamtsystem, das die präzise Abbildung der Fahrzeugumgebung 
in Echtzeit ermöglicht (\cite{schuldt2020phd}).

\section{Wellenlängenbereiche}
\label{sec:wellenlaenge}
Zur Umsetzung dieses Prinzips werden überwiegend Laser im nahen Infrarotbereich 
(NIR) eingesetzt, typischerweise mit Wellenlängen zwischen $870\,\text{nm}$ und 
$950\,\text{nm}$. Dieser Bereich ist besonders geeignet, da die spektrale 
Strahlungsintensität des Sonnenlichts dort vergleichsweise gering ist und somit 
ein günstiges Signal-Rausch-Verhältnis erzielt werden kann. Gleichzeitig stehen in diesem Wellenlängenbereich sowohl kosteneffiziente Halbleiterlaser als auch empfindliche Siliziumdetektoren zur Verfügung. Für spezielle Anwendungen werden 
auch Wellenlängen um $1064\,\text{nm}$ oder $1550\,\text{nm}$ eingesetzt, da hier 
höhere Laserleistungen innerhalb der Augensicherheitsgrenzen möglich sind. Diese 
erfordern jedoch alternative Detektormaterialien, was den technischen Aufwand und 
die Kosten erhöht(\cite{beuth2021handbuch}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Bilder/Spektrum der elektromagnetischen Strahlung.png}
    \caption{Spektrum der elektromagnetischen Strahlung(\cite{beuth2021handbuch})}
    \label{fig:ouster}
\end{figure}

\section{Messprinzip}
\label{sec:messprinzip}

\subsection{Direct Time‑of‑Flight (dToF)}
\label{sec:dTOF}

Das Messprinzip moderner \ac{LiDAR}-Systeme basiert überwiegend auf dem 
\emph{Time-of-Flight} (\ac{ToF})-Verfahren, das sich aufgrund seiner Robustheit und 
hohen Genauigkeit als Standard etabliert hat. Beim \ac{ToF}-Verfahren wird ein kurzer 
Laserimpuls ausgesendet, der sich mit Lichtgeschwindigkeit 
$c \approx 3 \cdot 10^{8}\,\text{m/s}$ ausbreitet, von einem Objekt reflektiert wird und schließlich den Empfänger erreicht. Aus der gemessenen Zeitdifferenz $\Delta t$ zwischen Emission und Detektion ergibt sich die Entfernung $d$ nach folgender Gleichung:

\begin{equation}
    d = \frac{c \cdot \Delta t}{2}
\end{equation}

Der Faktor $\tfrac{1}{2}$ berücksichtigt, dass der Lichtimpuls den Weg zweimal zurücklegt – vom Sender zum Objekt und wieder zurück zum Empfänger.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Bilder/Schematisches Beispiel eines direct ToF.png}
    \caption{Schematisches Beispiel eines direct Time-Of-Flight (dToF) \parencite{beuth2021handbuch}
}
    \label{fig:ouster}
\end{figure}

\subsection{Indirect \ac{ToF} (iToF)}
\label{sec:iTOF}

Beim iToF-Messprinzip wird die Entfernung nicht über die direkte Messung der Lichtlaufzeit bestimmt, sondern indirekt über die Veränderung eines kontinuierlich modulierten ausgesendeten Signals. Die zurückkehrende Reflexion weist abhängig von der Distanz charakteristische Änderungen auf, aus denen die Entfernung rekonstruiert werden kann. In der Praxis kommen hierfür hauptsächlich \emph{Amplitudenmodulationen} und \emph{Frequenzmodulationen} zum Einsatz \parencite{beuth2021handbuch}.

\subsubsection{AMCW (Amplitude-Modulated Continuous Wave)}

Bei einem AMCW‑\ac{LiDAR} wird ein kontinuierlich ausgesendeter Lichtstrahl mit einer Amplitudenmodulation versehen, typischerweise sinus‑ oder rechteckförmig.  Das vom Objekt reflektierte Licht weist gegenüber dem ausgesendeten Signal eine Phasenverschiebung~$\Delta\phi$ auf, die proportional zur Laufzeit und damit zur Entfernung ist.  Die Distanz lässt sich aus der gemessenen Phasenverschiebung bestimmen.  Für die Entfernung $d$ ergibt sich in Abhängigkeit von der Modulationsfrequenz~$f_\mathrm{mod}$ des sinusförmig modulierten Strahls
\begin{equation}
    d = \frac{c}{4\pi f_\mathrm{mod}}\, \Delta\phi\,,
\end{equation}
wobei $c$ die Lichtgeschwindigkeit bezeichnet.  Die maximale eindeutige Messdistanz (unambiguous range) ist durch die Periodizität des Modulationssignals begrenzt und beträgt
\begin{equation}
    d_\text{max} = \frac{c}{2 f_\mathrm{mod}}\,,
\end{equation}
so dass eine eindeutige Zuordnung der gemessenen Phasenverschiebung nur innerhalb dieser Entfernung möglich ist.  Höhere Modulationsfrequenzen verbessern zwar die Tiefenauflösung, verkürzen aber die eindeutige Reichweite proportional dazu \parencite{yi2023digital}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Bilder/Schema_AMCW_LiDAR.png}
    \caption{Schema eines amplitudenmodulierten LiDARs (AMCW) \parencite{beuth2021handbuch}.}
    \label{fig:amcw_schema}
\end{figure}

AMCW‑Systeme weisen als Verfahren mehrere Vorteile auf.  Die benötigte Hardware ist vergleichsweise einfach und kostengünstig zu realisieren, was hochintegrierte Sensordesigns ermöglicht.  Durch den kontinuierlichen Betrieb können niedrigere Spitzenleistungen genutzt werden, und bei kurzen Messdistanzen wird eine hohe Tiefenauflösung erreicht.  Aus diesen Gründen sind AMCW‑Sensoren in industriellen 3D‑Kameras, Smartphones und mobilen Endgeräten weit verbreitet \parencite{yi2023digital}.

Dem stehen charakteristische Nachteile gegenüber.  Die periodische Modulation führt zu einer Entfernungsambiguität, da eindeutige Distanzmessungen nur bis zur oben genannten unambiguous range möglich sind.  Zudem reagieren AMCW‑Systeme empfindlich auf Multipath‑Effekte: Mehrfachreflexionen unterschiedlicher Laufzeit überlagern sich und verfälschen die gemessene Phase, was insbesondere bei glänzenden Materialien oder in Innenraumumgebungen problematisch ist.  Weitere Fehlerquellen entstehen durch Phasenverkettung (2\,$\pi$‑Ambiguität), Alias‑Effekte höherer Harmonischer und Bewegungsartefakte, die durch die mehrfache Abtastung pro Bild entstehen \parencite{whyte2015application}.

Insgesamt eignet sich das AMCW‑Verfahren vor allem für kostensensitive und kompakte Sensorsysteme mit begrenzten Messdistanzen.  Seine Stärken liegen in der einfachen Realisierbarkeit und der hohen Präzision auf kurzen Entfernungen, während Nachteile wie begrenzte eindeutige Reichweite und Empfindlichkeit gegenüber Mehrwegeffekten den Einsatz in komplexen oder stark reflektierenden Umgebungen einschränken \parencite{yi2023digital}.

\subsubsection{FMCW (Frequency Modulated Continuous Wave)}
\label{sec:fmcw_lidar}

Ein FMCW‑\ac{LiDAR} nutzt kontinuierlich abgestrahlte Laserstrahlung, deren Frequenz zeitlich über einen sogenannten \emph{Chirp} linear variiert wird. Eine schematische Darstellung des grundlegenden Messprinzips ist in Abb.~\ref{fig:fmcw} gezeigt. Durch die Überlagerung des ausgesandten und des vom Objekt reflektierten Signals entsteht ein niederfrequentes \emph{Beat‑Signal}, dessen Frequenz proportional zur Laufzeitdifferenz ist. Aus diesem Beat‑Signal lässt sich die Entfernung mit hoher Präzision bestimmen\parencite{pinto2025fundamentals}.

Im Gegensatz zu klassischen Pulssystemen erlaubt FMCW zudem die gleichzeitige Ermittlung der Relativgeschwindigkeit: Eine durch die Objektbewegung verursachte Doppler‑Verschiebung verändert die Beat‑Frequenz, sodass Distanz‑ und Geschwindigkeitsinformation in einem einzigen Messvorgang separierbar sind. Dies führt zu einer besonders robusten Bewegungsanalyse, selbst bei schwachen Reflexionen oder geringem Signal‑Rausch‑Verhältnis\parencite{piggott2022physics}.

Ein zentraler Vorteil dieser Technik ist die hohe Störrobustheit gegenüber Fremdlicht und anderen \ac{LiDAR}‑Quellen. Da FMCW auf kohärenter Detektion basiert, werden ausschließlich Signale mit identischer Modulationscharakteristik ausgewertet. Dadurch können selbst sehr schwache Rückstreusignale zuverlässig erfasst werden, was die Performance bei schlechten Sichtbedingungen verbessert. Zudem ermöglicht die phasenbasierte Auswertung eine hohe Entfernungsauflösung bis in den Millimeterbereich \parencite{piggott2022immunity}.

Demgegenüber erfordert FMCW‑\ac{LiDAR} eine aufwändigere opto‑elektronische Architektur, einschließlich laserfrequenzstabiler Quellen, präziser Modulation, schneller Detektoren und leistungsfähiger digitaler Signalverarbeitung. Diese Anforderungen führen aktuell zu höheren Herstellungskosten sowie erhöhtem Rechenaufwand in der Echtzeit‑Auswertung \parencite{bianconi2025challenges}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Bilder/fmcw.png}
    \caption{Schematisches Messprinzip eines FMCW‑LiDARs (\cite{beuth2021handbuch}).}
    \label{fig:fmcw}
\end{figure}

\subsection{Fehlerquellen}
\label{sec:fehlerquellen}

Die Leistungsfähigkeit optischer \ac{LiDAR}-Sensoren wird von mehreren systematischen und stochastischen Fehlerquellen beeinflusst. Eine zentrale Rolle spielen Schwankungen in der Zeitmessung (\emph{Zeitjitter}) innerhalb der TDC- bzw. ADC-Elektronik, die direkt die Laufzeitbestimmung und damit die Distanzauflösung beeinträchtigen \parencite{rapp2020lidar}. Weitere typische Fehlerursachen sind \emph{Mehrwegeffekte (Multipath)}, bei denen reflektiertes Licht über indirekte Wege zum Detektor gelangt und so zu verfälschten oder doppelten Entfernungswerten führt \parencite{glennie2013multichannel}. 

Auch der Einfallswinkel des Laserstrahls sowie die optische Reflektivität der Oberfläche wirken sich auf die Signalstärke aus: Flache Winkel oder dunkle, absorbierende Materialien reduzieren die Signalenergie und erhöhen damit das Rauschen bzw. die Wahrscheinlichkeit von Fehlmessungen \cite{beuth2021handbuch}. Zusätzlich begrenzt die Strahldivergenz des Lasers die räumliche Auflösung, da der Lichtfleck mit zunehmender Entfernung größer wird und Reflexionen von benachbarten Objekten überlagert werden können.

Neben optischen Einflüssen spielen auch umwelt- und systembedingte Faktoren eine Rolle. temperaturbedingte Messabweichungen im Lasersystem oder in der Empfangselektronik können den Messoffset verschieben oder die Verstärkung verändern. Mechanische Komponenten – insbesondere bei rotierenden LiDARs – unterliegen Toleranzen und Vibrationen, beispielsweise durch Schwankungen der Rotordrehzahl, was zu nichtlinearen Abtastwinkeln oder zeitlichen Versätzen führt (\cite{levinson2011towards}). Diese Effekte wirken sich unmittelbar auf Reichweite, Präzision und Datenkonsistenz aus und können nachfolgende Verarbeitungsschritte wie Segmentierung, Objekterkennung oder Klassifikation beeinträchtigen.

Um in sicherheitsrelevanten Anwendungen robuste Umfeldmodelle zu gewährleisten, sind daher geeignete Maßnahmen zur Signalkonditionierung, Filterung und Fehlerkompensation notwendig. Dazu gehören Rauschreduktion, adaptive Schwellenwertverfahren, geometrische Konsistenzprüfungen sowie modellbasierte Korrekturen, um messtechnische Artefakte zu minimieren und Fehlinterpretationen im Gesamtsystem zu vermeiden.

\section{Typische Anwendungsszenarien im Fahrzeugumfeld}
\label{sec:anwendung}

\ac{LiDAR}-Sensoren spielen im Fahrzeugumfeld eine zentrale Rolle bei klassischen Fahrerassistenzsystemen (ADAS). Ein wichtiger Anwendungsbereich ist die \emph{Freiraum- und Belegtheitsdetektion}, bei der die Punktwolken genutzt werden, um befahrbare Bereiche zu bestimmen und Hindernisse im Nah- und Fernbereich präzise zu lokalisieren. Dies bildet die Grundlage für Funktionen wie automatische Notbremsung (AEB), Kollisionsvermeidung, Abstandsregelung sowie Querführung in niedrigen Automatisierungsstufen \parencite{Arnold2019Survey}.

Für höher automatisierte Fahrfunktionen (Level~3–4) dient \ac{LiDAR} als wesentliche Komponente zur \emph{3D-Umfeldmodellierung}. Dabei werden aus den Punktwolken statische Elemente der Infrastruktur (z.\,B. Leitplanken, Begrenzungslinien, geparkte Fahrzeuge) sowie dynamische Verkehrsteilnehmer wie Fahrzeuge, Radfahrer oder Fußgänger extrahiert und klassifiziert. Die hohe geometrische Auflösung von \ac{LiDAR} ermöglicht die Erstellung präziser Objektlisten, Konturen und Bewegungsmodelle, die von der Trajektorienplanung und Entscheidungslogik weiterverarbeitet werden \cite{beuth2021handbuch}.

Ein weiterer wichtiger Anwendungsbereich liegt in der \emph{Lokalisierung und Kartenerstellung (Mapping)}. \ac{LiDAR}-basierte \ac{SLAM}-Verfahren (Simultaneous Localization and Mapping) und Scan-Matching-Algorithmen nutzen die 3D-Punktwolken, um die Fahrzeugpose kontinuierlich zu schätzen und lokale Karten der Umgebung aufzubauen. Dies ist insbesondere in unbekannten oder unstrukturierten Gebieten relevant und dient als Ergänzung zu GPS, Inertialsensorik und Kamera \cite{cadena2016slam}.

Darüber hinaus wird \ac{LiDAR} in komplexen und dynamischen Verkehrsszenarien eingesetzt, etwa im \emph{Baustellen- und Engstellenmanagement}. Dort ermöglicht die präzise 3D-Geometrie das Erkennen veränderter Fahrkorridore, temporärer Barrieren, verschwenkter Fahrspuren oder unstrukturierter Objektumgebungen. Diese Information bildet die Grundlage für robuste Pfadplanung, sichere Trajektoriengenerierung und automatisierte Spurwahl \cite{levinson2011towards}.

Schließlich unterstützt \ac{LiDAR} Funktionen der \emph{Situations- und Kontextinterpretation}, etwa die Analyse von Verkehrsfluss, Abstandsdynamiken, Manövererkennung anderer Verkehrsteilnehmer sowie die Ableitung semantischer Strukturen der Umgebung (z.\,B. Fahrbahnränder, drivable space, Objekttypen). Die detaillierten 3D-Daten dienen dabei als hochpräzise geometrische Referenzschicht innerhalb der Sensorsuite \cite{mcauliffe2019scaling}.

\section{Algorithmischer Stand der Technik}
\label{sec:stand-der-technik}

Die algorithmische Verarbeitung hochauflösender 3D-\ac{LiDAR}-Daten folgt in der Literatur einem weitgehend etablierten Verarbeitungskettenmodell, das aus Vorverarbeitung, Bodensegmentierung, Segmentierung (Clustering), Merkmalsextraktion sowie Objektverfolgung besteht. Jedes Modul adressiert dabei spezifische Herausforderungen roher Punktwolken wie Rauschen, variable Punktdichten, Mehrwegeffekte oder dynamische Verkehrsszenen. Im Folgenden werden die einzelnen Bausteine detailliert beschrieben und jeweils mit relevanter Literatur untermauert.

% =====================================================================
\subsection{Vorverarbeitung}
\label{subsec:vorverarbeitung}

Die Vorverarbeitung dient der Reduktion der Datenmenge, der Stabilisierung der Punktwolke sowie der Vorbereitung der Daten für die nachfolgenden Module. Häufig eingesetzte Schritte umfassen:
\paragraph{Downsampling}
Ein verbreitetes Verfahren zur kontrollierten Reduktion der Punktanzahl ist das \emph{VoxelGrid}-Downsampling. Die Punktwolke wird in dreidimensionale Voxel unterteilt, wobei alle Punkte eines Voxels durch ihren Schwerpunkt ersetzt werden. Dadurch wird die räumliche Auflösung gezielt skaliert, ohne wesentliche geometrische Strukturen zu verlieren \parencite{RusuCousins2011}.

\paragraph{Ausreißerentfernung}
Zur Rauschreduktion werden in der Praxis der \emph{Statistical Outlier Removal (SOR)} sowie der \emph{Radius Outlier Removal (ROR)} eingesetzt.
SOR entfernt Punkte, deren mittlere Distanz zu ihren Nachbarn stark vom Erwartungswert abweicht, während ROR Punkte eliminiert, die innerhalb eines definierten Radius zu wenige Nachbarn besitzen \parencite{RusuCousins2011}. In der vorliegenden Implementierung wurde auf beide Verfahren verzichtet, weil die Punktwolken des Ouster-Sensors im Testumfeld ausreichend stabil waren und die zusätzliche Rechenzeit nicht gerechtfertigt erschien.

\paragraph{Bewegungskompensation (Deskewing)}
Da \ac{LiDAR}-Sensoren Punktwolken zeilenweise aufnehmen, entstehen bei Fahrzeugbewegungen Verzerrungen. Durch Deskewing unter Verwendung von IMU- oder Odometrie-Daten werden zeitbezogene Positionsfehler korrigiert \parencite{Behley2018}.

\paragraph{Intensitätsnormalisierung}
Die Intensität eines Punktes wird durch Materialeigenschaften und Auftreffwinkel beeinflusst. Eine Distanz- und Winkelkorrektur verbessert die Clusterqualität sowie nachfolgende Klassifikationsschritte \parencite{LevinsonThrun2011}.

\paragraph{Koordinatentransformationen}
Für bestimmte Verfahren werden die Punktwolken in Polarkoordinaten oder \emph{Bird’s-Eye-View}-Raster überführt, beispielsweise für scanline-basierte Segmentierer oder CNN-Detektoren \parencite{Lang2019PointPillars}.

\paragraph{Mehrfachechos}
Moderne LiDARs bieten mehrere Echos pro Laserpuls. Das erste Echo eignet sich zur Bodenerkennung, spätere Echos unterstützen die Erkennung hinter teiltransparenten Strukturen wie Vegetation \parencite{Glennie2010}.

% =====================================================================
\subsection{Bodensegmentierung}
\label{subsec:bodenseg}

Die Trennung von Boden und darüberliegenden Objekten ist ein entscheidender Schritt zur Reduktion des Suchraums. Zu den wichtigsten Verfahren gehören:

\paragraph{\ac{RANSAC}-Ebene}
Eine globale Bodenebene kann mittels \ac{RANSAC} robust geschätzt werden. Das Verfahren ist tolerant gegenüber Ausreißern, stößt jedoch bei geneigten Szenen oder unebenem Gelände an Grenzen \parencite{FischlerBolles1981}.

\paragraph{Progressive Morphological Filter (PMF)}
Der PMF nutzt morphologische Operationen auf einer Höhenkarte und wurde ursprünglich für Airborne-\ac{LiDAR} entwickelt. Er ist besonders robust gegenüber hügeligem Gelände \parencite{Zhang2003PMF}.

\paragraph{Grid-basierte Höhenkarten}
Hierbei wird die Punktwolke in ein zweidimensionales Raster projiziert, aus dem pro Zelle statistische Höhenmerkmale extrahiert werden. Diese Methode ist effizient und gut für Echtzeitsysteme geeignet.

\paragraph{Cloth Simulation Filtering (CSF)}
Ein virtuelles Tuch wird auf die invertierte Punktwolke “fallen gelassen”, wodurch die Bodenform approximiert wird. CSF hat sich als robust gegenüber Vegetation und komplexen Geländeformen erwiesen \parencite{Zhang2016CSF}.

\paragraph{Probabilistische Modelle}
Gaussian-Process-Modelle modellieren lokale Höhenverteilungen probabilistisch und sind besonders geeignet für urbane Szenarien mit Bordsteinen \parencite{Vetter2017}.

% =====================================================================
\subsection{Clustering und Segmentierung}
\label{subsec:clustering}

Nach Entfernung des Bodens wird die verbleibende Punktwolke segmentiert. Wichtige Verfahren sind:

\paragraph{Euklidische Clusterung}
Das klassische \ac{PCL}-Verfahren gruppiert Punkte, die innerhalb eines Distanzschwellwertes verbunden sind. Diese Methode ist einfach und schnell, reagiert jedoch empfindlich auf variable Punktdichten.

\paragraph{\ac{DBSCAN} und HDBSCAN}
Dichtebasierte Verfahren erkennen Cluster beliebiger Form und klassifizieren isolierte Punkte als Rauschen. \ac{DBSCAN} ist in vielen \ac{LiDAR}-Verarbeitungsketten Standard \parencite{Ester1996DBSCAN}.  
HDBSCAN erweitert das Konzept um hierarchische Dichteanalysen und ist robuster gegenüber Dichteunterschieden.

\paragraph{Region Growing}
Unter Verwendung lokaler Normale und Krümmungen segmentiert Region Growing größere, glatte Flächen. Dies ist besonders nützlich für Infrastruktur und Fassaden.

% =====================================================================
\subsection{Detektion und Klassifikation}
\label{subsec:detektion}

Ziel dieses Moduls ist die Identifikation potenzieller Objekte.

\paragraph{Geometrische Merkmale}
Aus den Clustern werden typische Merkmale wie Achslängen, PCA-Formfaktoren, Punktdichte oder orientierte Bounding Boxes extrahiert. Diese Verfahren benötigen keine Trainingsdaten und sind für Echtzeit geeignet \parencite{Douillard2011}.

\paragraph{Lernbasierte 3D-Detektoren}
Moderne Methoden wie \emph{PointPillars}, \emph{SECOND} oder \emph{CenterPoint} repräsentieren Punktwolken als \ac{BEV}-Raster und nutzen CNNs, um Fahrzeug-, Fußgänger- und Fahrradklassen zu erkennen \parencite{Lang2019PointPillars}.  
Diese Verfahren liefern hohe Genauigkeiten, erfordern jedoch umfangreiche Trainingsdaten und GPU-Ressourcen.

% =====================================================================
\subsection{Tracking}
\label{subsec:tracking}

Bewegte Objekte werden über mehrere Zeitschritte verfolgt. Dafür sind zwei Teilkomponenten notwendig:

\paragraph{Bewegungsmodelle}
Mehrere Filterverfahren kommen zum Einsatz:
\begin{itemize}
    \item Kalman-Filter (\ac{KF}) für lineare Bewegungsmodelle,
    \item Extended Kalman Filter (EKF) für leicht nichtlineare Modelle,
    \item Unscented Kalman Filter (UKF) für stärker nichtlineare Bewegungen.
\end{itemize}
Sie modellieren Objektzustände wie Position und Geschwindigkeit \parencite{WelchBishop1995}.

\paragraph{Datenassoziation}
Um neue Detektionen bestehenden Objekten zuzuordnen, kommen Verfahren wie \emph{Nearest Neighbor}, \emph{Joint Probabilistic Data Association (JPDAF)} oder \emph{Multiple Hypothesis Tracking (MHT)} zum Einsatz.  
In vielen automotive-nahen Arbeiten wird der \emph{Ungarische Algorithmus} zur optimalen Zuordnung verwendet \parencite{Kuhn1955Hungarian}.

\paragraph{Track-Management}
Typische Regeln umfassen:
\begin{itemize}
    \item Erzeugung neuer Tracks ab stabiler Beobachtung,
    \item Löschen nicht bestätigter Tracks,
    \item Lebenszeitkriterien zur Validität.
\end{itemize}
Dies verhindert spontane Fehldetektionen und stabilisiert die Szeneinterpretation.

\section{Ouster \ac{OS1} am Opel Astra der TH Nürnberg}

\subsection{Spezifische Eigenschaften des Ouster \ac{OS1}}
\label{sec:eigenschaften}

Der \emph{Ouster \ac{OS1}} (vgl. Abbildung~\ref{fig:ouster_os1}) ist ein rotationssymmetrischer 360°-\ac{LiDAR}, der eine hochauflösende dreidimensionale Erfassung der Umgebung ermöglicht. Die im Projekt verwendete Variante verfügt über eine vertikale Auflösung von 128 Kanälen. In der maximalen Konfiguration erreicht der Sensor eine horizontale Auflösung von 2{,}048 Abtastungen pro Umdrehung, was zu einer Punktwolke mit bis zu 262\,144 Messpunkten pro Scan führt.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Bilder/Ouster.png}
    \caption{Ouster \ac{OS1} \parencite{sensalyticsRetailAnalytics}}
    \label{fig:ouster_os1}
\end{figure}

In der derzeitigen Konfiguration sind 1{,}024 horizontale Winkelkanäle aktiv, sodass pro Umdrehung rund 131\,072 Datenpunkte generiert werden. Über das Webinterface des Sensors lassen sich sämtliche relevanten Parameter flexibel anpassen, darunter die horizontale Auflösung (512, 1{,}024 oder 2{,}048 Kanäle) sowie die Framerate (10 bis 20\,Hz). Die Datenübertragung erfolgt über eine Gigabit-Ethernet-Schnittstelle, über die sowohl die UDP-Punktwolkendaten als auch Konfigurationsbefehle übertragen werden \parencite{Ouster2024}.

Weitere technische Details zur Integration und Konfiguration des Sensors sind ausführlich in \textcite{Sagdic2025} dokumentiert.


\begin{table}[htb]
    \centering
    \label{tab:ouster_os1_specs}
    \begin{tabular}{p{0.35\textwidth}p{0.55\textwidth}}
        \hline
        \textbf{Hersteller:}              & Ouster Inc. \\ 
        \textbf{Gerätebezeichnung:}       & \ac{OS1} Mid-Range Imaging \ac{LiDAR} \\ 
        \textbf{Gewicht:}                 & ca.\ 0{,}50 kg (mit Gehäusekappe) \\ 
        \textbf{Abmaße (Ø x H):}          & 87 mm $\times$ 74{,}2 mm \\ 
        \textbf{Wellenlänge Laser:}       & 865 nm \\ 
        \textbf{Framerate:}               & 10 Hz oder 20 Hz (konfigurierbar) \\ 
        \textbf{Reichweite (80 \% Reflektivität):} 
                                          & 170 m bei $> 90\,\%$ Positivdetektion bei 100 klx Sonnenlicht \\ 
        \textbf{Reichweite (10 \% Reflektivität):} 
                                          & 90 m bei $> 90\,\%$ Positivdetektion bei 100 klx Sonnenlicht \\ 
        \textbf{Fehler Distanzbestimmung:}& typ. $\pm 2{,}5$ cm (Lambert-Ziel), bis $\pm 5$ cm (retroreflektiv) \\ 
        \textbf{Horizontales FoV:}        & $360^\circ$ \\ 
        \textbf{Vertikales FoV:}          & $42{,}4^\circ \pm 1^\circ$ \\ 
        \textbf{Horizontale Auflösung:}   & 512, 1\,024 oder 2\,048 Winkelabtastungen pro Umdrehung \\ 
        \textbf{Vertikale Auflösung:}     & 128 Kanäle \\ 
        \textbf{Punktzahl pro Sekunde:}   & bis zu 5{,}24 Mio.\ Punkte/s (128 Kanäle) \\ 
        \textbf{Betriebsspannung:}        & 9{,}5 V bis 51 V DC \\ 
        \textbf{Leistungsaufnahme:}       & ca.\ 16 W (nominal, bis 28 W beim Kaltstart) \\ 
        \textbf{Datenschnittstelle:}      & UDP über Gigabit-Ethernet (1000BASE-T / 1000BASE-T1) \\ 
        \hline
    \end{tabular}
    \caption{Spezifikationen \ac{LiDAR}-Sensor \emph{Ouster \ac{OS1}}}
\end{table}


\subsection{Sensorpositionierung am Opel Astra}
Der im Rahmen dieser Arbeit verwendete Ouster~\ac{OS1} ist auf dem Versuchsträger Opel Astra des Instituts montiert. Der Sensor, laut \textcite{Sagdic2025}, wurde mittig auf einer Trägerstruktur über der Fahrzeugdachkante positioniert. Diese erhöhte Montage ermöglicht ein möglichst großes 360°-Sichtfeld ohne Abschattungen durch Fahrzeugkarosserie oder Anbauteile. Die Ausrichtung des Sensors gewährleistet, dass sowohl Bereiche vor dem Fahrzeug als auch seitliche und rückwärtige Zonen erfasst werden, was für die spätere Bodenfilterung, Clusterbildung und Objektverfolgung essenziell ist. Die Abbildung~\ref{fig:sensorposition_astra} zeigt die Montageposition auf dem Fahrzeug im Labor des IFZN mit Abmessungen.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.83\textwidth]{Bilder/sensorposition_astra.png}
    \caption{Montageposition des Ouster~\ac{OS1} auf dem Opel Astra mit Abmessungen (\textcite{Wendel2025}).}
    \label{fig:sensorposition_astra}
\end{figure}

Abbildung~\ref{fig:position_sensor} veranschaulicht den Versuchsträger Opel Astra sowie die auf der Trägerstruktur montierte Position des Ouster~\ac{OS1}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.83\textwidth]{Bilder/position_sensor.png}
    \caption{Montageposition des Ouster~\ac{OS1} auf dem Opel Astra}
    \label{fig:position_sensor}
\end{figure}

\chapter{Systemarchitektur}
\label{chap:systemarchitektur}

Dieses Kapitel zeichnet die technische Systemarchitektur der \ac{LiDAR}-basierten Umfelderkennung nach. Dazu werden zunächst alle Software- und Hardwarekomponenten in eine durchgängige \ac{ROS}~2-Verarbeitungskette eingeordnet. Anschließend folgen die eingesetzten Frameworks und Bibliotheken, die Arbeitsumgebung inklusive Datenaufnahme und -verarbeitung, die Deployment-Strategie der Nodes sowie die Bewertungsmetriken, mit denen die Verarbeitungskette in den folgenden Kapiteln überprüft wird.

\section{Systemübersicht}
\subsection{Anforderungen an den Algorithmus}

Die Punktwolken des Ouster~\ac{OS1} bilden die Ausgangsbasis einer durchgängigen Verarbeitungskette, die Bodenpunkte unterdrückt, Objekte als Cluster mit Bounding-Boxen herausarbeitet und deren Bewegung verfolgt. Die hohe Winkelauflösung des Sensors liefert dafür detaillierte Geometrien \parencite{OusterOS1}; zugleich verlangt sie nach einer stabilen Vorverarbeitung, die Ausreißer und Rauschen entfernt, die Punktdichte gezielt reduziert und trotzdem genügend Struktur für verlässliche Objektgrenzen erhält. Dieses Vorgehen folgt etablierten Empfehlungen der Fahrzeugautomatisierungs-Literatur: Deterministische Filterung und kontrollierte Latenz erhöhen die funktionale Sicherheit \parencite{Arnold2019Survey,Macenski2022ROS2}, während eine konsequente Bodenentfernung die Falsch-Positiv-Rate reduziert und das Clustering präziser macht \parencite{gomes2023survey}. Konfigurierbare Parameter stellen sicher, dass die gleiche Pipeline sowohl in engen urbanen Szenen als auch auf offenen Flächen oder geneigten Fahrbahnen zuverlässig arbeitet.

Die Hardware- und Softwareanforderungen bauen auf dieser Pipeline auf: Ziel ist eine End-to-End-Latenz von unter \(100\,\text{ms}\) bei \(10\,\text{Hz}\) mit moderatem Verbrauch von CPU- und RAM-Ressourcen. Alle \ac{ROS}~2-Nachrichten sind klar dokumentiert, und die Ausgaben lassen sich maschinell weiterverarbeiten oder im Terminal lesen. Die Zielplattform sind mobile Rechner oder Embedded-PCs mit mindestens sechs CPU-Kernen (\(\geq 2{,}5\,\text{GHz}\)) und \(\geq 16\,\text{GB}\) RAM, ohne spezialisierte GPU-Anforderungen. Die Integration in die GUI von \emph{Carception~X} \parencite{IFZN_Projekte} stellt sicher, dass alle Betriebsmodi (Sensor, Sensor + PC, Sensor + PC + Embedded PC) unterstützt werden \parencite{Wendel2025}.

Die Leistungsbewertung nutzt klar definierte Kriterien: Der Restbodenanteil soll je Rasterzelle (\(2\,\text{m}\) \(\times\) \(2\,\text{m}\)) unter \(5\,\%\) bleiben, während tragende Objektpunkte im Frontbereich (\(30\,\text{m}\)) höchstens um \(10\,\%\) reduziert werden dürfen. Ein Downsampling gilt als verlustarm, wenn mindestens \(70\,\%\) der Ausgangsdichte in einem \(3\,\text{m}\)\(\times\) \(3\,\text{m}\)-Gitter erhalten bleibt und Bounding-Box-Kanten höchstens um \(0{,}1\,\text{m}\) vom Referenzwert abweichen. Laufzeitparameter wie Voxelgröße, \ac{RANSAC}-Abstandsgrenze und Iterationszahl, Cluster-Toleranz sowie Minimal- und Maximalpunktzahlen werden über Launch-Dateien oder \texttt{ros2 param set} ohne Neustart angepasst. Eine Echtzeitvisualisierung in RViz2 oder der GUI macht Rohdaten, Bodenfilter, Cluster und Tracks bei mindestens \(10\,\text{\ac{FPS}}\) sichtbar.

Die detaillierten Anforderungen und dazugehörigen Messkriterien sind in Tabelle~\ref{tab:anforderungen_messkette} zusammengefasst.

  \begin{table}[H]
    \centering
\begin{tabularx}{\textwidth}{|c|X|X|}
\hline
\textbf{Nr.} & \textbf{Anforderung} & \textbf{Messkriterium} \\
\hline

1 &
\ac{OS1}-Punktwolken werden empfangen &
Topic \texttt{/ouster/points}; Rate $\geq 10$ Hz, Paketverlust $<1\,\%$ \\[0.15cm]
\hline

2 &
Objektliste mit Pose und Abmessungen &
Aktualisierungsrate $\geq 10$ Hz; Track-IDs konsistent über $\geq 5$ Frames \\[0.15cm]
\hline

3 &
Bodenpunkte und Störsignale entfernen &
Restbodenanteil $<5\,\%$ je $2\,\text{m}$-Raster; Verlust tragender Objektpunkte $<10\,\%$ im Frontbereich \\[0.15cm]
\hline

4 &
Reduzierte Punktdichte ohne wesentlichen Geometrieverlust &
Voxelgröße 5--10 cm; Punktdichte $\geq70\,\%$ im $3\,\text{m}$-Gitter; Kantenabweichung $<0{,}1$ m \\[0.15cm]
\hline

5 &
Laufzeitparameter anpassbar &
Voxelgröße, \ac{RANSAC}-Abstand/Iterationen, Cluster-Toleranz und Clustergrößen via Launch oder \texttt{ros2 param set} ohne Neustart \\[0.15cm]
\hline

6 &
End-to-End-Latenz <100 ms bei 10 Hz &
Zeitstempel-Differenz Ein-/Ausgang; stabile 10 Hz \\[0.15cm]
\hline

7 &
Stabiler Betrieb auf Standardhardware &
CPU/RAM-Auslastung $\leq 60\,\%$ auf 6-Kern-CPU (\(\geq 2{,}5\,\text{GHz}\), \(\geq 16\,\text{GB}\) RAM) bei stabilen $10\,\mathrm{Hz}$ \\[0.15cm]
\hline

8 &
Maschinen- und terminallesbare Ausgabe &
\ac{ROS}-Themen mit dokumentierten Typen (\texttt{sensor\_msgs/PointCloud2}, \texttt{vision\_msgs/Detection3DArray}); Konsolenstatus $\leq 1$ Zeile/s \\[0.15cm]
\hline

9 &
Integration in bestehende GUI &
Algorithmus ausführbar bei den Moden Sensor, Sensor + PC, Sensor + PC + Embedded PC \\[0.05cm]
\hline

10 &
Echtzeitvisualisierung &
Darstellung von Rohdaten, Bodenfilter, Clustern und Tracks in RViz2/GUI mit $\geq 10\,\text{\ac{FPS}}$ \\[0.05cm]
\hline

\end{tabularx}
\caption{Anforderungen an die Verarbeitungsstrecke mit zugehörigen Messkriterien}
\label{tab:anforderungen_messkette}
\end{table}


\subsection{Datenerfassung}
\label{chap:datenerfassung}

Die Datenerfassung umfasst sämtliche Schritte von der Initialisierung des Ouster~\ac{OS1} über die Netzwerkkommunikation bis hin zur Bereitstellung der Punktwolken innerhalb des \ac{ROS}~2-Systems. Der grundlegende Aufbau orientiert sich an der Messkette nach \cite{Wendel2025}. 

\paragraph{Netzwerkarchitektur und Übertragungspfad}
Der Ouster~\ac{OS1} wird über eine dedizierte Ethernetverbindung mit dem Embedded-PC dSPACE MAB~III verbunden. Da der Embedded-PC nur begrenzte Schnittstellen besitzt (vgl. \cite{Wendel2025}), erfolgt die physische Anbindung über einen USB-A-auf-Ethernet-Adapter des Typs \emph{Renkforce RF-4708614}. Dieser unterstützt Datenraten bis \(1\,\text{Gbit/s}\) und stellt damit sicher, dass die maximal mögliche Netto-Datenrate des \ac{LiDAR}-Sensors zuverlässig übertragen werden kann (vgl. \cite{conrad2025}). Die Netzwerkkommunikation arbeitet in einem isolierten Subnetz, in dem nur Embedded-PC und Sensor verbunden sind. Dies minimiert Latenzen und verhindert Paketverlust durch konkurrierende Netzwerkdienste.

\paragraph{Publikation der Sensordaten}
Nach erfolgreicher Initialisierung publiziert der Ouster~\ac{OS1} die Daten als \ac{ROS}~2-Nachrichten des Typs \texttt{sensor\_msgs/PointCloud2} auf dem Topic \texttt{/ouster/points}. Die Punktwolke enthält für jeden Messpunkt kartesische Koordinaten \((x,y,z)\), die Intensität der rückgestreuten Strahlung sowie den Ring-Index des Laserkanals. Der Zeitstempel wird direkt in der Firmware generiert und ermöglicht eine präzise Synchronisation mit späteren Verarbeitungsschritten. Durch die feste Schichtstruktur (Ringe) der \ac{OS1}-Architektur bleibt die vertikale Struktur der Punktwolke konsistent, was insbesondere für die spätere Bodenextraktion und Clusterbildung essenziell ist.

\paragraph{Weiterleitung im \ac{ROS}~2-Netzwerk}
Der Embedded-PC leitet die Punktwolken über das interne \ac{ROS}~2-Netzwerk an den Laptop weiter. Dies erfolgt über eine DDS-basierte Publish/Subscribe-Kommunikation, deren \ac{QoS}-Einstellungen so gewählt wurden, dass die hohe Datenrate des \ac{OS1} zuverlässig transportiert werden kann (vgl. \cite{Wendel2025}). Auf dem Laptop können die Daten entweder direkt in \emph{RViz2} visualisiert oder über die entwickelte GUI weiterverarbeitet werden. Die Entkopplung von Embedded-PC (Datenerfassung) und Laptop (Analyse/Visualisierung) ermöglicht eine stabile Erfassung, selbst wenn die Visualisierung oder GUI zeitweise höhere Rechenlast erzeugt.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{Bilder/messkette_sensordaten.png}
  \caption{Aufbau der Datenerfassung mit dem Ouster~\ac{OS1}-\ac{LiDAR} \cite{Wendel2025}).}
  \label{fig:messkette_sensordaten}
\end{figure}

\subsection{Umsetzung}
Abbildung~\ref{fig:ros2-verarbeitungskette-alltopics} zeigt die \ac{ROS}~2-Verarbeitungskette von der Aufnahme über Vorverarbeitung, Bodensegmentierung und Cluster/Bounding-Boxes bis zur Objektverfolgung. Jeder Knoten hat klar definierte Ein-/Ausgänge und Topics.

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=2.2cm,
    process/.style={rectangle, rounded corners, draw=black, fill=blue!5,
                    text centered, minimum width=4.2cm, minimum height=1cm},
    startstop/.style={ellipse, draw=black, fill=gray!15,
                      text centered, minimum width=3cm, minimum height=1cm},
    rviz/.style={rectangle, draw=black, fill=green!10, rounded corners,
                 text centered, minimum width=3.5cm, minimum height=0.9cm,
                 font=\scriptsize},
    arrow/.style={thick,->,>=stealth},
    every node/.style={font=\small}
  ]

  % Hauptlinie
  \node (start)   [startstop] {Start};
  \node (input)   [process, below of=start] {Messdaten einlesen};
  \node (crop)    [process, below of=input] {CropBox-Filter};
  \node (voxel)   [process, below of=crop] {VoxelGrid-Filter};
  \node (ground)  [process, below of=voxel] {Bodensegmentierung};
  \node (cluster) [process, below of=ground] {Cluster-Extraktion \& Bounding Boxes};
  \node (track)   [process, below of=cluster] {Objektverfolgung};
  \node (end)     [startstop, below of=track] {Ende};

  % \ac{RViz}-Knoten (rechts)
  \node (rviz_input)  [rviz, right=4.1cm of input]  {\ac{RViz}};
  \node (rviz_crop)   [rviz, right=4.1cm of crop]   {\ac{RViz}};
  \node (rviz_voxel)  [rviz, right=4.1cm of voxel]  {\ac{RViz}};
  \node (rviz_ground) [rviz, right=4.1cm of ground] {\ac{RViz}};
  \node (rviz_cluster)[rviz, right=2.8cm of cluster]{\ac{RViz}};
  \node (rviz_track)  [rviz, right=4.1cm of track]  {\ac{RViz}};

  % Vertikale Pfeile (Verarbeitungskette)
  \draw[arrow] (start) -- (input);
  \draw[arrow] (input) -- node[right]{\scriptsize \texttt{/ouster/points}} (crop);
  \draw[arrow] (crop)  -- node[right]{\scriptsize \texttt{/points\_cropped}} (voxel);
  \draw[arrow] (voxel) -- node[right]{\scriptsize \texttt{/points\_voxel}} (ground);
  \draw[arrow] (ground)-- node[right]{\scriptsize \texttt{/obstacle\_points}} (cluster);
  \draw[arrow] (cluster)-- node[right]{\scriptsize \texttt{/detections\_raw}} (track);
  \draw[arrow] (track) -- node[right]{\scriptsize \texttt{/tracks\_raw}} (end);

  % Pfeile zu \ac{RViz} (alle Schritte)
  \draw[arrow] (input.east)  -- node[above,sloped]{\scriptsize \texttt{/ouster/points}} (rviz_input.west);
  \draw[arrow] (crop.east)   -- node[above,sloped]{\scriptsize \texttt{/points\_cropped}} (rviz_crop.west);
  \draw[arrow] (voxel.east)  -- node[above,sloped]{\scriptsize \texttt{/points\_voxel}} (rviz_voxel.west);
  \draw[arrow] (ground.east) -- node[above,sloped]{\scriptsize \texttt{/obstacle\_points}} (rviz_ground.west);
  \draw[arrow] (cluster.east)-- node[above,sloped]{\scriptsize \texttt{/detections\_markers}} (rviz_cluster.west);
  \draw[arrow] (track.east)  -- node[above,sloped]{\scriptsize \texttt{/tracks\_markers}} (rviz_track.west);

  % Caption & Label
  \end{tikzpicture}
  \caption{\ac{ROS}~2-Algorithmus mit allen Nodes, ihren Topic-Verbindungen und den Ausgaben zur Visualisierung in \ac{RViz}.}
  \label{fig:ros2-verarbeitungskette-alltopics}
\end{figure}

\section{Grundlagen von \ac{ROS}~2}
\label{sec:ros2_basics}
Das \emph{Robot Operating System~2 (\ac{ROS}~2)} ist ein quelloffenes Framework für modulare, verteilte Robotiksysteme. Es stellt Middleware-basierte Kommunikation (DDS), wiederverwendbare Komponenten und Entwicklungswerkzeuge bereit. Dank Skalierbarkeit und Plattformunabhängigkeit wird \ac{ROS}~2 in Forschung und Industrie eingesetzt (z.\,B. autonome Fahrzeuge, mobile Robotik, Inspektion).

\paragraph{Nodes}
\label{sec:ros2_nodes}

In ROS2 stellen \textit{Nodes} die grundlegenden Ausführungseinheiten des Systems dar. 
Jeder Node repräsentiert einen eigenständigen Prozess, der eine bestimmte Funktion erfüllt – beispielsweise das Erfassen von Sensordaten, die Datenverarbeitung oder die Ansteuerung von Aktoren. 
Die Modularisierung erlaubt klar definierte Schnittstellen und Wiederverwendung (vgl. \cite{ros2_docs}).

\paragraph{Topics}
\label{sec:ros2_topics}

Die Kommunikation zwischen Nodes erfolgt in ROS2 hauptsächlich über \textit{Topics}. Sie transportieren Nachrichten nach dem Publish/Subscribe-Prinzip.
Ein Node kann Daten auf einem Topic veröffentlichen (\textit{publish}) oder von diesem empfangen (\textit{subscribe}). 
Publisher und Subscriber sind entkoppelt, was flexible, verteilte Architekturen ermöglicht (vgl. \cite{ros2_docs}).

\paragraph{Nachrichten}
\label{sec:ros2_messages}

Die über Topics ausgetauschten Informationen werden in Form von \textit{Nachrichten (Messages)} übertragen. 
Eine Nachricht besteht aus einer definierten Datenstruktur, die verschiedene Datentypen wie Ganzzahlen, Gleitkommazahlen, Arrays oder benutzerdefinierte Typen enthalten kann. 
Diese klar definierte Struktur ermöglicht einen standardisierten und sicheren Datenaustausch zwischen Nodes, unabhängig von der zugrunde liegenden Programmiersprache(~\cite{ros2_docs}).

\paragraph{Bags}
\label{sec:ros2_bags}

\textit{ROS2 Bags} dienen zur Aufzeichnung, Speicherung und Wiederverwendung von Nachrichten, die über Topics ausgetauscht werden.  
Sie unterstützen Analyse, Debugging und reproduzierbare Experimente (vgl. \cite{ros2_docs}).

\paragraph{RViz2}
\label{sec:ros2_rviz2}

\textit{RViz2} ist ein Visualisierungstool, das zur Darstellung und Analyse der in ROS2 verarbeiteten Daten verwendet wird. 
Es ermöglicht die dreidimensionale Visualisierung von Punktwolken, Robotermodellen, Trajektorien und Sensordatenströmen in Echtzeit und unterstützt Entwicklung und Fehlersuche (vgl. \cite{ros2_docs}).


\paragraph{DDS, Discovery und Transports}

\ac{ROS}~2 verwendet das \emph{Data Distribution Service} (DDS) als Kommunikationsmiddleware, wobei die RMW-Schicht die Kommunikation über DDS-basierte Implementierungen wie Fast~DDS und Cyclone~DDS abstrahiert. DDS übernimmt die zuverlässige Nachrichtenverteilung sowie das Teilnehmermanagement innerhalb eines \ac{ROS}~2-Netzwerks \parencite{rmw_implementations}.

\begin{itemize}
  \item \textbf{Discovery:}  
  Die automatische Erkennung von Teilnehmern erfolgt über die DDS-Mechanismen SPDP und SEDP, die Multicast-Traffic zur Detektion neuer Knoten verwenden. Die \emph{Domain-ID} trennt unterschiedliche DDS-Netze logisch und bestimmt die verwendeten Ports \parencite{ros2_traffic}.
  
  \item \textbf{Transports:}  
  Standardmäßig nutzt DDS UDP für die Netzwerkommunikation. Für datenintensive Anwendungen stehen jedoch optimierte Transportpfade wie Shared-Memory und intra-process-Kommunikation zur Verfügung, die Kopien reduzieren und Latenzen minimieren \parencite{fastdds_shm}.
  
  \item \textbf{Konfiguration:}  
  DDS-Einstellungen wie genutzte Netzwerkschnittstellen, Whitelists, Zeitlimits oder Transportparameter können über XML-Profile oder Umgebungsvariablen konfiguriert werden. Cyclone DDS lädt Konfigurationen über die Variable \texttt{CYCLONEDDS\_URI} \parencite{cyclonedds_config}.
\end{itemize}

Praktisch empfiehlt es sich, bei Systemen mit hoher Punktdichte (z.\,B.\ \ac{LiDAR}-Verarbeitung) Shared-Memory- oder intra-process-Kommunikation zu aktivieren und die Netzwerkschnittstelle explizit zu wählen, um Discovery-Traffic zu reduzieren und eine stabile Datenübertragung sicherzustellen \parencite{fastdds_shm}.

\paragraph{Quality of Service (\ac{QoS})}
\label{sec:ros2_qos}
In \ac{ROS}~2 definieren Quality-of-Service-Profile (\ac{QoS}) die Kommunikationssemantik zwischen Publishern und Subscribern. Sie beeinflussen maßgeblich Latenz, Paketverluste, Speicherbedarf und Synchronisation entlang der Verarbeitungskette. Besonders bei hochfrequenten Sensorströmen wie \ac{LiDAR} ist eine konsistente und angepasste \ac{QoS}-Konfiguration entscheidend für die Systemstabilität.

Die Konfiguration der \ac{QoS}-Profile in \ac{ROS}~2 umfasst mehrere Parameter, von denen insbesondere \textbf{Reliability}, \textbf{History/Depth} und \textbf{Durability} die Übertragungseigenschaften von Sensordaten maßgeblich bestimmen. Eine kompakte Übersicht der Vor- und Nachteile findet sich in Tab.~\ref{tab:qos_vt_nt}. Die Definitionen basieren auf den DDS-Spezifikationen gemäß OMG-Standard \parencite{OMG_DDS_Spec}.

\begin{table}[H]
\centering
\caption{Wesentliche \ac{QoS}-Parameter und ihre Vor- und Nachteile}
\label{tab:qos_vt_nt}
\begin{tabularx}{\textwidth}{l X X}
\toprule
\textbf{Parameter} & \textbf{Vorteile} & \textbf{Nachteile} \\
\midrule
\textbf{Reliability: Reliable} & Garantierte Zustellung; keine Datenverluste & Höhere Latenz bei hoher Last; höherer Ressourcenbedarf \\
\textbf{Reliability: BestEffort} & Niedrige Latenz; ressourcenschonend & Nachrichten können verloren gehen \\
\textbf{History: KeepLast(N)} & Begrenzter Speicherverbrauch; stabil vorhersehbar & Ältere Nachrichten werden verworfen \\
\textbf{History: KeepAll} & Keine Nachricht geht verloren & Hoher Speicherbedarf; ungeeignet bei hohen Datenraten \\
\textbf{Durability: Volatile} & Ideal für Echtzeitströme; keine Verzögerung & Späte Subscriber erhalten keine alten Daten \\
\textbf{Durability: TransientLocal} & Neue Subscriber erhalten letzte Nachricht sofort & Erhöhter Speicheraufwand; nicht geeignet für \ac{LiDAR}-Daten \\
\bottomrule
\end{tabularx}
\end{table}

Die \textbf{Reliability-Policy} legt fest, wie mit Nachrichtenverlusten umgegangen wird. Bei \emph{Reliable} garantiert DDS die Zustellung aller Pakete, was für deterministische Verarbeitungsschritte erforderlich ist, jedoch bei hohen Sensordatenraten zu Latenzsteigerungen führen kann. Dies entspricht dem von \ac{ROS}~2 empfohlenen Verhalten für kritische Datenströme \parencite{Maruyama2016ROS2}. \emph{BestEffort} verzichtet dagegen auf Garantien und ist insbesondere für Visualisierungstopics wie \texttt{/ouster/points} geeignet, da dort gelegentliche Verluste tolerierbar sind.

Die \textbf{History-Einstellung} bestimmt, wie viele Nachrichten gepuffert werden. Für \ac{LiDAR}-Anwendungen mit hohen Datenraten empfiehlt sich \emph{KeepLast} mit geringer Tiefe (5--10), da kontinuierlich neue Punktwolken eintreffen und ältere Daten obsolet werden. Diese Empfehlung findet sich ebenfalls in \ac{ROS}~2-Profilingstudien, die den hohen Speicherbedarf von \emph{KeepAll} dokumentieren \parencite{Buhlmann2022QoSStudy}.

Die \textbf{Durability} sollte bei Echtzeitdaten grundsätzlich \emph{Volatile} sein, da nur aktuelle Sensordaten relevant sind. \emph{TransientLocal} eignet sich hingegen für selten publizierte Konfigurations- oder Statusnachrichten, für die neue Subscriber die zuletzt bekannte Information sofort erhalten sollen \parencite{DDS_Primer2018}.

Weitere \ac{QoS}-Parameter wie \emph{Deadline}, \emph{Liveliness} und \emph{Lifespan} spielen im \ac{LiDAR}-Kontext eine untergeordnete Rolle. Sie können jedoch zur Laufzeitüberwachung eingesetzt werden, beispielsweise zur Erkennung ausgefallener Publisher oder verzögerter Messungen \parencite{ROS2_Design_QoS}.


In der Praxis ergeben sich daraus folgende empfohlenen Einstellungen: 
Für die Verarbeitungskette (z.\,B. \texttt{/points\_cropped}, \texttt{/detections\_raw}) 
ist ein Profil mit \emph{Reliable}, \emph{KeepLast(10)} und \emph{Volatile} zweckmäßig, 
um Datenverluste bei gleichzeitig moderater Pufferung zu vermeiden. 
Für Visualisierungstopics wie \texttt{/ouster/points} genügt meist \emph{BestEffort} 
mit geringer Tiefe, da verpasste Nachrichten tolerierbar sind.

Die Tabelle ~\ref{tab:qos-topics} zeigt die \ac{QoS}-Profile der in dieser Arbeit erstellten Topics. Publisher und Subscriber handeln bei Verbindungsaufbau die Schnittmenge ihrer \ac{QoS}-Profile aus. Dabei können Fehlermodi auftreten: etwa wenn ein \emph{Reliable}-Publisher mit einem \emph{BestEffort}-Subscriber verbunden wird – in diesem Fall wird die Kommunikation zwar aufgebaut, aber der Publisher puffert gegebenenfalls unkontrolliert. Ebenso führt ein Mismatch bei der \emph{Durability} dazu, dass historische Nachrichten nicht übertragen werden. Eine zu geringe \emph{Depth} wiederum kann zu Paketverlusten oder wachsender Latenz führen. Daher sollten \ac{QoS}-Parameter explizit je Topic gesetzt und aktiv überwacht werden.

\begin{table}[htbp]
\centering
\footnotesize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}

\begin{tabularx}{\textwidth}{|l|c|l|l|X|}
\hline
\textbf{Knoten} & \textbf{Rolle} & \textbf{Topic} & \textbf{Typ} & \textbf{\ac{QoS}} \\
\hline

crop\_box\_node & Sub & \texttt{/ouster/points} & PointCloud2       & SensorDataQoS (Best Effort) \\ \hline
crop\_box\_node & Pub & \texttt{/points\_cropped} & PointCloud2     & SensorDataQoS (Best Effort) \\ \hline

voxel\_filter\_node & Sub & \texttt{/points\_cropped} & PointCloud2    & SensorDataQoS (Best Effort) \\ \hline
voxel\_filter\_node & Pub & \texttt{/points\_voxel} & PointCloud2      & SensorDataQoS (Reliable) \\ \hline

ransac\_ground\_node & Sub & \texttt{/points\_voxel} & PointCloud2      & SensorDataQoS (Best Effort) \\ \hline
ransac\_ground\_node & Pub & \texttt{/obstacle\_points} & PointCloud2   & rclcpp::\ac{QoS}(10) (Reliable) \\ \hline

cluster\_extraction\_node & Sub & \texttt{/obstacle\_points} & PointCloud2  & SensorDataQoS (Best Effort) \\ \hline
cluster\_extraction\_node & Pub & \texttt{/detections\_markers} & MarkerArray & rclcpp::\ac{QoS}(10) (Reliable) \\ \hline
cluster\_extraction\_node & Pub & \texttt{/detections\_raw} & Detection3DArray & SensorDataQoS (Best Effort) \\ \hline

sort\_tracker\_node & Sub & \texttt{/detections\_raw} & Detection3DArray & SensorDataQoS (Best Effort) \\ \hline
sort\_tracker\_node & Pub & \texttt{/tracks\_raw} & Detection3DArray  & rclcpp::\ac{QoS}(10) (Reliable) \\ \hline
sort\_tracker\_node & Pub & \texttt{/tracks\_markers} & MarkerArray     & rclcpp::\ac{QoS}(10) (Reliable) \\ \hline

\end{tabularx}

\caption{\ac{QoS}-Profile der im Code verwendeten Topics (Default-Werte).}
\label{tab:qos-topics}
\end{table}

\section{Datenrepräsentation und Bibliotheken}
\subsection{\texttt{sensor\_msgs/PointCloud2}: Felder, Koordinaten und Layout}
Ouster~\ac{OS1}-Daten werden als \texttt{sensor\_msgs/PointCloud2} publiziert. Tabelle~\ref{tab:pointcloud2-fields} fasst die wichtigsten Felder und deren Semantik zusammen; die konkreten Datentypen (z.\,B. \texttt{float32} oder \texttt{uint16}) hängen von Treiber- und Konfiguration ab.

\begin{table}[H]
  \centering
  \small % réduit la taille
  \setlength{\tabcolsep}{6pt} % réduit l'espace horizontal
  \renewcommand{\arraystretch}{1.05} % compacte verticalement

  \begin{tabular}{|p{2.2cm}|p{13cm}|}
    \hline
    \textbf{Feld} & \textbf{Bedeutung} \\ \hline

    \texttt{x, y, z} &
    Kartesische Koordinaten (m) im \texttt{frame\_id}. 
    Rechtsdrehendes System nach REP~103: \(x\) vorwärts, \(y\) links, \(z\) oben. \\ \hline

    \texttt{intensity} &
    Reflektierte Signalstärke (\texttt{uint16}); abhängig von Sensorkalibrierung und daher nur innerhalb eines Scans vergleichbar. \\ \hline

    \texttt{ring} &
    Vertikaler Kanalindex (Laser/Detektor). Beim \ac{OS1}-64: 64 Ringe von 0 (unten) bis 63 (oben). \\ \hline

    \texttt{time} &
    Zeitversatz eines Punktes relativ zu \texttt{header.stamp} (s); wichtig für Motion Compensation innerhalb eines Scans. \\ \hline

  \end{tabular}

  \caption{Zentrale Felder von \texttt{sensor\_msgs/PointCloud2} für den Ouster~\ac{OS1}.}
  \label{tab:pointcloud2-fields}
\end{table}


Alle Felder werden typischerweise little-endian kodiert (\texttt{is\_bigendian = false}). Das Speicherlayout folgt einem linearen Puffer \texttt{data} mit \texttt{point\_step} (Byte pro Punkt) und \texttt{row\_step} (Byte pro Zeile). Die Dimensionen \texttt{width}/\texttt{height} beschreiben eine 2D-Anordnung: Bei rotierenden LiDARs entspricht \texttt{height} der Ringanzahl, \texttt{width} der Punktzahl pro Umdrehung. \texttt{is\_dense} signalisiert, ob \enquote{NaN}-Einträge vorkommen. Eine organisierte Wolke (Ring~\(\times\) Azimut) ermöglicht effiziente nachbarschaftsbasierte Filter und Segmentierung sowie ringweise Verarbeitung (z.\,B. Bodenfilter nur auf unteren Ringen).

\subsection{Point Cloud Library (\ac{PCL})}
\label{chap:pcl}
Die \emph{Point Cloud Library (\ac{PCL})} bildet die Grundlage der Umfelderkennung. Sie ist eine freie C++-Bibliothek (BSD-Lizenz), die ursprünglich 2011 bei Willow~Garage als Teil des \ac{ROS}-Ökosystems entstand und seitdem von der Community rund um Open~Perception und industrielle Anwender weiterentwickelt wird (vgl. \cite{pcl_docs_2025}). \ac{PCL} bietet eine umfangreiche Sammlung modularer Algorithmen: Punktfilterung (Downsampling, Zuschneiden, Rauschunterdrückung), Normalen- und Merkmalsextraktion, \ac{RANSAC}-basierte Segmentierung, Registrierung (z.\,B. \ac{ICP}), Oberflächenrekonstruktion, Nachbarschaftssuchen auf k-d-Baum/Octree-Basis, Clusteranalyse sowie Visualisierung und Dateiformate (PCD). In \ac{ROS}~2 erfolgt die Einbindung über \texttt{pcl\_conversions} und ggf. \texttt{pcl\_ros}; \texttt{sensor\_msgs/PointCloud2} wird bidirektional in \texttt{pcl::PointCloud<T>} konvertiert.

\subsection{Genutzte Module und Algorithmen}
Die Verarbeitungskette stützt sich auf etablierte \ac{PCL}-Module, die in \ac{ROS}~2 stabil verfügbar sind und nachvollziehbare Laufzeiteigenschaften besitzen:
\begin{itemize}
  \item \textbf{Filter}: \texttt{CropBox}/\texttt{PassThrough} begrenzen den Arbeitsbereich auf das relevante Fahrumfeld und reduzieren die Punktzahl frühzeitig (vgl. Abschnitt~\ref{sec:cropbox}); \texttt{VoxelGrid} homogenisiert die Punktdichte und senkt den Rechenaufwand nachgelagerter Schritte ohne signifikanten Geometrieverlust (Abschnitt~\ref{sec:voxelgrid}). Auf zusätzliche Ausreißerfilter (z.\,B. \texttt{StatisticalOutlierRemoval}) wird in der implementierten Verarbeitungskette zugunsten geringerer Latenz verzichtet; das Rohsignal erwies sich im Testfeld als hinreichend stabil (Kapitel~\ref{chap:vorverarbeitung}).
  \item \textbf{Segmentierung}: Eine plane \ac{RANSAC}-Segmentierung trennt Boden- von Hindernispunkten. \texttt{pcl::SACSegmentation} mit \texttt{SACMODEL\_PLANE} ist robust gegenüber Ausreißern, erfordert nur wenige Parameter (Abstands- und Iterationsschranke) und lässt sich effizient in Echtzeit ausführen. Die methodische Begründung und Parametertabellen finden sich in Kapitel~\ref{chap:bodensegmentierung}.
  \item \textbf{Clustering}: \texttt{EuclideanClusterExtraction} auf Basis eines \texttt{pcl::search::KdTree} gruppiert die bodenfreien Punkte. Der Ansatz liefert reproduzierbare Ergebnisse, benötigt nur \(\varepsilon\)-Nachbarschaft und Min/Max-Clustergröße als Stellgrößen und ist in zahlreichen \ac{LiDAR}-Verarbeitungsketten erprobt. Die konkrete Umsetzung und Parameterstudie stehen in Kapitel~\ref{chap:clustering}.
  \item \textbf{Konvertierung}: \texttt{pcl::fromROSMsg}/\texttt{pcl::toROSMsg} sowie \texttt{pcl\_conversions} bilden die Brücke zwischen \ac{ROS}~2-Nachrichten und \ac{PCL}-Datenstrukturen. Sie sind erforderlich, damit die oben genannten \ac{PCL}-Algorithmen in den \ac{ROS}-Knoten der Vorverarbeitung, Bodensegmentierung und Clusterung arbeiten können.
\end{itemize}
\textbf{Performance}: Durch Vorallokation (z.\,B. Puffergrößen für KdTree und Clusterlisten), die Vermeidung redundanter Kopien und die Wahl leichter Punkttypen wie \texttt{pcl::PointXYZI} wird eine konstante Verarbeitung bei 10~Hz ermöglicht; die Wirkung der Einstellungen wird in den Kapitelabschnitten zu Vorverarbeitung (Kapitel~\ref{chap:vorverarbeitung}) und Clustering (Kapitel~\ref{chap:clustering}) gemessen und diskutiert.

Die genannten Module tauchen entlang der gesamten Verarbeitungskette wieder auf: Die Filterstufe in Kapitel~\ref{chap:vorverarbeitung} bereitet die Daten für die \ac{RANSAC}-Bodensegmentierung in Kapitel~\ref{chap:bodensegmentierung} vor; deren Ergebnis dient als Ausgangspunkt für die Cluster-Extraktion und Bounding-Box-Ermittlung in Kapitel~\ref{chap:clustering}. Dadurch ist nachvollziehbar, warum jede Komponente ausgewählt wurde und wie sie mit den nachfolgenden Modulen interagiert.
Für Entwicklung und Ausführung wird \textbf{Ubuntu~22.04~LTS (Jammy Jellyfish)} verwendet. Als Referenzplattform für \ac{ROS}~2 ermöglicht Ubuntu eine nahtlose Integration von Bibliotheken, Treibern und Werkzeugen (vgl. \cite{ubuntu_docs_2025}). Die Distribution bietet stabile C++/Python-Toolchains, hohe Sicherheit und breite Unterstützung in wissenschaftlicher wie industrieller Softwareentwicklung. Die enge Verzahnung mit der in dieser Arbeit eingesetzten \ac{ROS}-Distribution \emph{Humble~Hawksbill} vereinfacht die Einrichtung der Abhängigkeiten. Die aktive Entwicklergemeinschaft sorgt durch regelmäßige Updates, umfassende Dokumentation und eine große Auswahl an Open-Source-Paketen für einen reibungslosen Entwicklungsprozess. Dank der modularen Struktur von Linux lässt sich die Arbeitsumgebung flexibel an die spezifischen Anforderungen der Sensorintegration und der \ac{ROS}-Module anpassen.

\subsection{Bash-Skripte}
Bash-Skripte automatisieren wiederkehrende Aufgaben (Workspace laden, Launch starten, Aufzeichnung) und sind ein zentrales Werkzeug in Linux-basierten Entwicklungsumgebungen. In dieser Arbeit fungiert ein Skript als Bindeglied zwischen der GUI und dem Python-\texttt{launch}-File der Verarbeitungskette. Der typische Ablauf umfasst folgende Schritte:
\begin{enumerate}
  \item Laden der \ac{ROS}~2-Umgebung (\texttt{source /opt/ros/humble/setup.bash}) und des projektinternen Workspace (\texttt{source \$\{WORKSPACE\}/install/setup.bash}) über \texttt{set -euo pipefail}, damit Fehler frühzeitig abbrechen.
  \item Übergabe benutzerdefinierter Parameter (z.\,B. Topic-Namen, Ausgabeordner, Logging-Level) aus der GUI via Shell-Argumente; diese werden an das Python-Launch-File durchgereicht.
  \item Start des zentralen Launch-Files (\texttt{ros2 launch...}) als Hintergrundprozess
  \item Persistente Ablage von Logs und Bags in einer datierten Ordnerstruktur (z.\,B. \texttt{logs/\$\{DATE\}}) sowie Ausgabe standardisierter Statusmeldungen (\texttt{echo}, \texttt{printf}).
\end{enumerate}
  Dadurch werden alle erforderlichen Knoten in definierter Reihenfolge aktiviert, die GUI entlastet und ein reproduzierbarer Start der Verarbeitungskette ermöglicht (inklusive optionaler Aufzeichnung und definierter Bereinigung bei Abbruch). Der Ablauf orientiert sich an den offiziellen \ac{ROS}~2-Empfehlungen für Launch-Dateien und Bash-Wrapper-Skripte\parencite{ros2_launch_docs}.

%\subsection*{\ac{ROS}~2 CLI und Monitoring}
%Zur Inspektion der Laufzeitumgebung werden \texttt{ros2}-Werkzeuge genutzt: \texttt{topic list/info/hz/bw/delay}, \texttt{node list/info}, \texttt{interface show}, \texttt{doctor}. Sie unterstützen Durchsatz-/Latenzschätzung, Interface-Inspektion und Grunddiagnostik.

\section{Deployment und Packaging}
Die Umsetzung erfolgt im \ac{ROS}~2-Workspace (\texttt{colcon}); \texttt{src/} enthält die Pakete, \texttt{build/}/\texttt{install/}/\texttt{log/} werden erzeugt. Abhängigkeiten werden in \texttt{package.xml} deklariert und in \texttt{CMakeLists.txt} gebunden. Eine Trennung in Bibliotheken (Kernlogik) und Executables (\ac{ROS}~2-Anbindung) erhöht die Wiederverwendbarkeit.

Start und Parametrisierung erfolgen über Launch-Dateien. Namensräume strukturieren Topics (z.\,B. Sensorpfad, Verarbeitungspfad) und erlauben parallele Instanziierung. Die Visualisierung erfolgt über \textbf{RViz2}.

\section{Leistungsmetriken und Verifikation}

Die im Folgenden betrachteten Metriken sind kein Selbstzweck, sondern direkt aus der
Anforderungsdefinition und dem Stand der Technik abgeleitet. Veröffentlichte Arbeiten zu
Echtzeit-Objekterkennung evaluieren systematisch Ende-zu-Ende-Latenz sowie CPU/RAM-Bedarf
als zentrale Gütekriterien (vgl. \cite{feng2021survey}). Genau diese Größen entsprechen den in
Tabelle~\ref{tab:anforderungen_messkette} formulierten Messkriterien: \textbf{Anforderung~6}
fordert eine Ende-zu-Ende-Latenz von unter \(100\,\text{ms}\) bei stabilen \(10\,\text{Hz}\),
\textbf{Anforderung~7} begrenzt die Auslastung auf \(\leq 60\,\%\) CPU/RAM auf Standardhardware.
Die gewählten Verifikationsschritte (Zeitstempel-Instrumentierung für Latenz, Profiler für
Ressourcennutzung) knüpfen somit unmittelbar an die definierten Anforderungen an und machen
nachvollziehbar, wie deren Erfüllung gemessen und nachgewiesen wird.

\subsection{E2E-Latenz}
Für das betrachtete System wird eine Ende-zu-Ende-Latenz von weniger als \(100\,\text{ms}\) bei einer Verarbeitung mit \(10\,\text{Hz}\) angestrebt. Zur Verifikation werden an den Modulgrenzen Zeitstempel gesetzt (Eingang und Ausgang), die anschließend hinsichtlich Mittelwert, 95. Perzentil und 99. Perzentil über lange Sequenzen ausgewertet werden. Die Messungen erfolgen bei der Wiedergabe identischer Datensätze aus \texttt{rosbag2}; die zugrundeliegenden Aufzeichnungen sind im digitalen Anhang dokumentiert (Abschnitt~\ref{anhang:messaufzeichnungen}). Die Instrumentierung erfolgt anhand des Eingangszeitstempels aus \texttt{header.stamp}, während Zwischenzeiten innerhalb der Module mitgeführt und protokolliert werden. Eine gemeinsame Zeitbasis (z.\,B. deaktivierte Simulationszeit oder externe Synchronisation) wird vorausgesetzt, um Vergleichbarkeit und Konsistenz zu gewährleisten.

\subsection{Ressourcennutzung (CPU/RAM)}
Die Auslastung der Rechen- und Speicherressourcen wird für jeden Knoten unter repräsentativer Last erfasst. Optimierungsmaßnahmen wie ein angepasstes Datenlayout, die Vermeidung unnötiger Kopien oder die Vorallokation von Puffern werden dokumentiert und quantitativ bewertet. Zur Analyse kommen Werkzeuge wie \texttt{ros2 topic hz /ouster/points} und \texttt{ros2 topic hz /tracks\_raw} zum Einsatz.
Außerdem wird ein Python Skript ausgeführt zur Berechnung der CPU Auslastung pro Node (resource\_probe.py); das vollständige Skript ist im digitalen Anhang hinterlegt (Abschnitt~\ref{anhang:skripte}).

\subsection{Objekterkennung und -verfolgung}
Die Güte der Objektdetektion und -verfolgung wurde primär mit zwei ROS-Tools bewertet: \texttt{ids\_probe.py} und \texttt{frame\_loss\_probe.py}. Erstere verfolgt die Lebenszyklen einzelner Track-IDs und meldet pro Auswertefenster die Anzahl neu erzeugter IDs (\emph{births/min}), abgeschlossener Tracks (\emph{completed/min}) sowie die mediane aktive Lebensdauer sowohl laufender als auch bereits finalisierter Tracks. Damit lassen sich übermäßige ID-Wechsel oder instabile Verfolgungen unmittelbar erkennen. \texttt{frame\_loss\_probe.py} misst hingegen für jede Stufe der Verarbeitungskette die effektive Nachrichtenrate und berechnet Durchlassraten zwischen den benachbarten Topics (\emph{pass\%}) sowie gezählte Lücken (\emph{gaps}) basierend auf ausbleibenden Nachrichten über eine adaptive Schwellwertlogik. So wird transparent, an welcher Stelle Frames verloren gehen oder die Frequenz einbricht, und wie sich dies auf die nachgelagerten Detektions- und Tracking-Ergebnisse auswirkt.

\chapter{Vorverarbeitung}
\label{chap:vorverarbeitung}

Dieses Kapitel bündelt die Vorverarbeitungsschritte der \ac{LiDAR}-Punktwolken. Ziel ist es, Datenvolumen und Rauschen vor den nachfolgenden Modulen zu reduzieren und gleichzeitig eine gleichmäßig verteilte Datenbasis für Segmentierung und Clustering zu schaffen. Im Zentrum stehen die räumliche Begrenzung per CropBox und die Ausdünnung mittels VoxelGrid, jeweils mit den verwendeten Parametern und der Umsetzung in der \ac{ROS}~2-Pipeline.

\section{CropBox-Filter}
\label{sec:cropbox}

\subsection{Prinzip}
Der \textit{CropBox-Filter} begrenzt eine Punktwolke auf eine achsenparallel ausgerichtete Begrenzungsbox (Axis-Aligned Bounding Box, AABB) und entfernt sämtliche Punkte außerhalb des Volumens. Formal sei \(\mathcal{P}=\{\mathbf{p}_i\}\) eine Punktwolke mit \(\mathbf{p}_i=[x_i,y_i,z_i,1]^\top\) in homogenen Koordinaten. Der Filter prüft für jede Stichprobe, ob
\begin{align}
\mathbf{b}_\text{min} \leq \mathbf{R}\,\mathbf{p}_i + \mathbf{t} \leq \mathbf{b}_\text{max}
\label{eq:cropbox}
\end{align}
gilt, wobei \(\mathbf{R}\in\mathbb{R}^{3\times4}\) die ersten drei Zeilen der \ac{PCL}-internen Transformationsmatrix sind, \(\mathbf{t}\) eine optionale Translation darstellt und \(\mathbf{b}_\text{min/max}=[x_{\min/\max},y_{\min/\max},z_{\min/\max}]^\top\) die parametrisierte Box definieren. Punkte, die diese Ungleichung nicht erfüllen, werden verworfen. Die Operation besitzt eine lineare Laufzeit \(\mathcal{O}(|\mathcal{P}|)\) und reduziert das Datenvolumen vor nachgelagerten Schritten signifikant (vgl. \cite{rusu2011pcl,pcl_cropbox_2025}). Durch die optionale Transformationsmatrix lassen sich zusätzlich gekippte Sensorkoordinaten oder Fahrzeugversätze berücksichtigen, wodurch die AABB effektiv einer beliebig orientierten Bounding Box im globalen Rahmen entspricht \cite{pcl_docs_2025}.

Die Grenzen wurden schrittweise verengt: Ausgangspunkt war eine großzügige Box \(\mathbf{b}_\text{min}=[-20,-15,-3]^\top\), \(\mathbf{b}_\text{max}=[20,15,5]^\top\), die anschließend entlang \(x\), \(y\) und \(z\) in \(0{,}5\,\mathrm{m}\)-Schritten reduziert wurde. Jede Variante wurde anhand der Detektionspräzision nach dem Clustering, der verbleibenden Punktzahl und des abgeschnittenen Fahrbahnrands in \ac{RViz} bewertet. Dieses iterative \ac{ROI}-Tuning folgt gängigen Empfehlungen zur Falsch-Alarm-Reduktion \cite{hu2013robust,himmelsbach2010fast}. Die finalen Grenzen \(\mathbf{b}_\text{min}=[-10,-6,-3]^\top\) und \(\mathbf{b}_\text{max}=[10,6,5]^\top\) balancieren Datenreduktion und stabile Objektabdeckung im urbanen Messfeld.

\subsection{Implementierung}
Der \texttt{crop\_box\_node} wurde in C++ unter \ac{ROS}~2 umgesetzt und nutzt die \ac{PCL}-Schnittstellen, um die Ungleichung aus Gleichung~\eqref{eq:cropbox} Punkt für Punkt anzuwenden. Der Node abonniert \texttt{/ouster/points} mit SensorDataQoS, lädt die Grenzen \texttt{min\_bound} und \texttt{max\_bound} sowie die Topics \texttt{input\_topic} und \texttt{output\_topic} und publiziert das Ergebnis auf \texttt{/points\_cropped}. Tabelle~\ref{tab:cropbox_params} fasst das Parameter-Set zusammen; der produktive Code unter \texttt{ouster\_cpp/src/crop\_box\_node.cpp} ist in Anhang~\ref{lst:cropbox_node} vollständig abgedruckt.

\begin{verbatim}
// Ausschnitt aus src/ouster_cpp/src/crop_box_node.cpp
const float xmin = min_bound_[0], xmax = max_bound_[0];
const float ymin = min_bound_[1], ymax = max_bound_[1];
const float zmin = min_bound_[2], zmax = max_bound_[2];

for (; ix != ix.end(); ++ix, ++iy, ++iz) {
  const float x = *ix, y = *iy, z = *iz;
  // AABB-Test
  if (x >= xmin && x <= xmax &&
      y >= ymin && y <= ymax &&
      z >= zmin && z <= zmax) {
    // Punkt in Ausgabepuffer übernehmen
  }
}
\end{verbatim}

Neben den Pflichtfeldern \(x,y,z\) unterstützt der Node optionale Felder (\texttt{intensity}, \texttt{ring}) und erlaubt über die Callback-API \texttt{add\_on\_set\_parameters\_callback} eine Laufzeitänderung der Grenzen ohne Neustart des Nodes. Tabelle~\ref{tab:cropbox_params} fasst die Startparameter zusammen.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|c|}
    \hline
    \textbf{Parametername} & \textbf{Startwert} \\ \hline
    \texttt{input\_topic}  & \texttt{/ouster/points} \\ \hline
    \texttt{output\_topic} & \texttt{/points\_cropped} \\ \hline
    \texttt{min\_bound}    & \([-20.0, -15.0, -3.0]\) \\ \hline
    \texttt{max\_bound}    & \([20.0, 15.0, 5.0]\) \\ \hline
  \end{tabular}
  \caption{Startparameter des CropBox-Filters}
  \label{tab:cropbox_params}
\end{table}

Das Zuschneidevolumen bezieht sich auf das Sensorkoordinatensystem des Ouster-LiDARs und folgt der in \ac{ROS} etablierten Konvention nach REP~103: \(x\) zeigt in Fahrtrichtung, \(y\) nach links und \(z\) nach oben; der Ursprung liegt im \ac{LiDAR}-Frame auf Fahrzeughöhe. Der Node unterstützt Pflichtfelder (\texttt{x}, \texttt{y}, \texttt{z}) sowie optionale Attribute wie \texttt{intensity} (FLOAT32/UINT16) und \texttt{ring}. Für jeden Punkt wird geprüft, ob er innerhalb der parametrierten Box liegt; bei Erfolg wird er übernommen. Über \texttt{set\_parameters} lassen sich die Grenzen zur Laufzeit anpassen, ohne die \ac{ROS}~2-Verbindungen neu aufzubauen.

\subsection{Test und Ergebnis}
Die Wirkung des CropBox-Filters wird nicht allein über Visualisierungen überprüft, sondern durch eine
schrittweise Verifikation. Dafür wird das aufgezeichnete \ac{ROS}~2-Bag einer T-Kreuzung auf ebenem Asphaltboden
(bewusst gewählt, um symmetrische Sichtfelder und eine klare Bodenreferenz zu erhalten) mehrfach wiedergegeben.
Pro Wiederholung werden drei Messgrößen protokolliert: (1) die Zahl der Eingangspunkte, (2) die Zahl der
beibehaltenen Punkte innerhalb der AABB sowie (3) die zeitliche Stempelverschiebung zwischen Eingangs- und
Ausgangsnachricht als Indikator der Knotendauer. Ein Skript fasst die Ergebnisse als CSV zusammen und
ermöglicht den Vergleich verschiedener Parameterkombinationen.

Das Testszenario „Kreuzung auf ebenem Boden“ ist gewählt, weil es sowohl freie Sichtflächen als auch klar
begrenzte Hindernisse (Randbebauung, Bordsteine) bietet und damit typische urbanen Einsatzbedingungen
repräsentiert, ohne dass Bodenunebenheiten die Filterbewertung verfälschen. In der finalen Konfiguration
reduziert der Filter die Punktanzahl reproduzierbar um rund zwei Drittel, während der Zeitstempel im Header
(vgl. Listing~\ref{lst:cropbox_node}, Zeilen 90ff.) sicherstellt, dass nachgelagerte Latenzmessungen den korrekten
Sendezeitpunkt erhalten.
Da sich im Messbereich Gebäude und weitere störende Objekte befanden, welche die Anzahl falscher Detektionen signifikant erhöhen konnten, wurden die Grenzen des \texttt{crop\_box\_node} sukzessive angepasst. Durch iterative Parameteroptimierung ergaben sich folgende final verwendete Werte: \texttt{min\_bound = [-10.0, -6.0, -3.0]} sowie \texttt{max\_bound = [10.0, 6.0, 5.0]}.
Physikalisch orientieren sich diese Grenzen an der Testumgebung: Mit \(\pm10\,\text{m}\)
in \(x\)-Richtung wird der relevante Bereich vor und hinter dem Fahrzeug für Manöver im
innerstädtischen Geschwindigkeitsbereich erfasst, ohne Gebäude an weit entfernten
Fassaden mitzunehmen. \(\pm6\,\text{m}\) in \(y\)-Richtung decken eine Fahrspur sowie
Gegen- oder Parkspur ab (typisch \(3{-}3{,}5\,\text{m}\) je Spur) und schließen Gehwege
ein, während die Vertikalgrenzen von \(-3\,\text{m}\) bis \(5\,\text{m}\) sowohl den
Fahrbahnbereich unterhalb des Sensors als auch überhängende Objekte wie Lkw-Aufbauten
oder niedrige Brücken berücksichtigen.

Abbildung~\ref{fig:cropbox_compare} zeigt die Wirkung des Filters im Vergleich:
Links ist die ursprüngliche Punktwolke mit vollständiger Umgebung dargestellt,
während rechts nur der relevante Bereich nach Anwendung des CropBox-Filters zu sehen ist. 

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{Bilder/cropbox_compare.png}
  \caption{Vergleich der Punktwolke vor (links) und nach (rechts) Anwendung des 
  \textit{CropBox-Filters} (2025-10-15-Messdaten/2025-10-15-VK-OL-005).}
  \label{fig:cropbox_compare}
\end{figure}


\section{VoxelGrid-Filter}
\label{sec:voxelgrid}

\subsection{Prinzip}
Der \textit{VoxelGrid-Filter} reduziert die Punktanzahl, indem der Raum in kubische Voxel der
Kantenlänge $l=\texttt{voxel\_size}$ diskretisiert wird und pro Voxel nur ein repräsentativer Punkt
(z.\,B. der erste oder ein Schwerpunkt) beibehalten wird. Dadurch sinken Datenrate und
Rechenaufwand nachgelagerter Schritte (Bodenebensegmentierung, Clustering, Tracking), bei
gleichzeitiger Kontrolle des Informationsverlustes über die Wahl von $l$.
Mathematisch erfolgt zunächst eine Quantisierung der Punktkoordinaten $(x,y,z)$ auf ganzzahlige
Gitterindizes $(i,j,k)$ über
\begin{equation}
  (i,j,k) = \left(\left\lfloor \tfrac{x}{l} \right\rfloor, \left\lfloor \tfrac{y}{l} \right\rfloor, \left\lfloor \tfrac{z}{l} \right\rfloor\right),
  \label{eq:voxel_index}
\end{equation}
wodurch alle Punkte innerhalb eines Voxels dieselben Indizes teilen. Für jedes belegte Voxel wird
genau ein Repräsentant behalten. In der vorliegenden Implementierung ist das der erste verarbeitete
Punkt pro Voxel; alternativ könnte der Schwerpunkt
\begin{equation}
  \bar{p}_v = \tfrac{1}{|V|} \sum_{p \in V} p
  \label{eq:voxel_centroid}
\end{equation}
als repräsentativer Punkt gewählt werden. Beide Varianten begrenzen die Anzahl der verbleibenden
Punkte auf höchstens ein Sample pro Voxel und machen den Aufwand proportional zur Anzahl belegter
Voxel. Die Wahl der Voxelkante $l$ steuert damit direkt den Kompromiss zwischen Detailtreue und
Rechenaufwand.

% préambule : \usepackage{graphicx}
\begin{figure}[H]
  \centering
  \includegraphics[width=.82\textwidth]{Bilder/voxelgrid_prinzip.png}%
  \caption{Funktionsprinzip des VoxelGrid-Filters: Aufteilung des Raums in Voxel und
  Beibehaltung eines repräsentativen Punktes pro Voxel (nach Lyu~u.\,a., 2024).}
  \label{fig:voxel_principle}
\end{figure}


\subsection{Implementierung}
Der \textit{VoxelFilterNode} liest die
zuvor per \textit{CropBox} zugeschnittene Punktwolke auf \texttt{points\_cropped}, setzt in
der \ac{PCL}-Implementierung \texttt{pcl::VoxelGrid<pcl::PointXYZI>} die Blattgröße
\texttt{voxel\_size} in allen drei Achsen und führt anschließend die Unterabtastung aus.
Dadurch wird pro belegtem Voxel exakt ein Punkt in die ausgegebene, unterabgetastete
Punktwolke \texttt{points\_voxel} übernommen; die Rechenkomplexität sinkt damit auf
\(\mathcal{O}(n_\text{voxel})\), was
insbesondere die nachgelagerte k-d-Baum-Suche beim Clustering entlastet. Eine zu große
Voxelkante würde jedoch feine Strukturen (z.\,B. Fahrradspeichen oder dünne Leitpfosten)
verschmieren, während eine zu kleine Kante kaum Rechenzeit spart und die Cluster-Parameter
\(\varepsilon\) anheben würde (vgl. Abschnitt~\ref{chap:clustering}).

Die Startwahl \texttt{voxel\_size} = \SI{0.20}{\meter} orientiert sich sowohl an Literatur als
auch an den Tests aus Kapitel~\ref{chap:test_und_bewertung}. Lyu~u.\,a. \parencite{lyu2024voxel} untersuchen das
Voxel-Downsampling in urbanen Karten und zeigen, dass Blattgrößen im Bereich
\SI{0.15}{\meter}--\SI{0.25}{\meter} das Rauschen signifikant reduzieren, ohne Fußgänger und
Fahrräder zu verlieren; \SI{0.20}{\meter} erweist sich dort als stabiler Kompromiss. Die wichtigsten Startparameter
sind in Tabelle~\ref{tab:voxel_params} zusammengefasst.

% préambule : \usepackage{booktabs} (optionnel, pour un rendu plus pro)
\begin{table}[H]
  \centering
  \begin{tabular}{|l|c|}
    \hline
    \textbf{Parametername} & \textbf{Startwert} \\ \hline
    \texttt{input\_topic}  & \texttt{points\_cropped} \\ \hline
    \texttt{output\_topic} & \texttt{points\_voxel} \\ \hline
    \texttt{voxel\_size}   & \(\,0{,}20\,\mathrm{m}\,\) \\ \hline
  \end{tabular}
  \caption{Startparameter des VoxelGrid-Filters}
  \label{tab:voxel_params}
\end{table}

\subsection{Ergebnis}

Wie in Abbildung~\ref{fig:voxel_compare} dargestellt, führt die Anwendung des
VoxelGrid-Filters zu einer gleichmäßigeren und deutlich reduzierten Punktdichte,
ohne dass dabei wichtige Strukturen der Szene verloren gehen.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{Bilder/voxel_compare.png}
  \caption{Vergleich nach \textit{CropBox} (links) und nach \textit{VoxelGrid-Filter} (rechts) (2025-10-15-Messdaten/2025-10-15-VK-OL-005)}
  \label{fig:voxel_compare}
\end{figure}


\chapter{Bodensegmentierung}
\label{chap:bodensegmentierung}

Die Bodensegmentierung ist ein essenzieller Vorverarbeitungsschritt innerhalb der Verarbeitungskette zur Objekterkennung.
Ihr Ziel besteht darin, Bodenpunkte zuverlässig von Hindernissen und anderen Objekten zu trennen, um die nachfolgenden Schritte – insbesondere die Cluster- und Objekterkennung – zu erleichtern.
Dieses Kapitel führt die Anforderungen an eine robuste Bodensegmentierung ein, vergleicht gängige Verfahren anhand klarer Bewertungskriterien und stellt die implementierte \ac{RANSAC}-Variante im Detail vor. Dadurch ergibt sich ein roter Faden von den Anforderungen über den Methodenvergleich bis hin zur konkreten Umsetzung.


\section{Anforderungen an die Bodensegmentierung}
\label{sec:anforderungen_bodenseg}

Die wichtigsten Anforderungen an ein Bodensegmentierungsverfahren lassen sich nach~\cite{gomes2023survey} in mehreren Kriterien zusammenfassen, die in Tabelle~\ref{tab:anforderungen_bodensegmentierung} dargestellt sind.

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.3} % augmente l'espacement vertical
\setlength{\tabcolsep}{8pt}       % ajuste l'espacement horizontal
\begin{tabular}{|p{4cm}|p{9.5cm}|}
\hline
\textbf{Kriterium} & \textbf{Beschreibung} \\ \hline

\textbf{Echtzeitfähigkeit} & 
Das Verfahren muss Punktwolken in Echtzeit verarbeiten können (z.\,B. <100\,ms pro Frame), 
um eine kontinuierliche Fahrzeugsteuerung zu ermöglichen. \\ \hline

\textbf{Rechenaufwand} & 
Geringe Rechen- und Speicheranforderungen sind notwendig, 
da die Algorithmen häufig auf eingebetteten Systemen mit begrenzten Ressourcen laufen. \\ \hline

\textbf{Segmentierungs\-robustheit} & 
Das Verfahren sollte robust gegenüber Über- und Untersegmentierung sein 
und Bodenpunkte korrekt von Hindernissen trennen. \\ \hline

\textbf{Leistung bei steigenden Hindernissen} & 
Die Methode sollte sanft ansteigende Strukturen (z.\,B. Rampen oder Bordsteine) 
korrekt als Teil des Bodens erkennen. \\ \hline

\textbf{Leistung bei unebenem Gelände} & 
Auch bei Neigungen oder unregelmäßigen Bodenoberflächen 
muss die Bodenschätzung stabil bleiben. \\ \hline

\textbf{Leistung bei spärlichen Daten} & 
Das Verfahren sollte bei geringer Punktdichte (z.\,B. großer Abstand zum Sensor) 
zuverlässige Ergebnisse liefern. \\ \hline
\end{tabular}

\vspace{5pt}
\caption{Hauptanforderungen an die Bodensegmentierung (nach~\cite{gomes2023survey})}
\label{tab:anforderungen_bodensegmentierung}
\end{table}

Aus den sechs Anforderungen werden für die weitere Analyse vier Bewertungskriterien abgeleitet:

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|c|p{4cm}|p{9cm}|}
\hline
\textbf{Nr.} & \textbf{Kriterium} & \textbf{Beschreibung} \\ \hline
1 & Echtzeitfähigkeit & Bewertet, ob die Methode unter den gegebenen Rechenbedingungen (10\,Hz Sensorrate) eine kontinuierliche Verarbeitung der Punktwolke in Echtzeit ermöglicht. \\ \hline
2 & Robustheit & Misst die Widerstandsfähigkeit gegenüber Rauschen, Ausreißern und unterschiedlichen Geländetypen (z.\,B. unebenes Terrain oder variable Punktdichten). \\ \hline
3 & Genauigkeit & Beschreibt die Fähigkeit der Methode, die Bodenpunkte präzise von Nicht-Bodenpunkten zu trennen und somit eine zuverlässige Segmentierung zu gewährleisten. \\ \hline
4 & Rechenaufwand & Bewertet den benötigten Ressourcenverbrauch (CPU/GPU-Zeit und Speicherbedarf) für die Durchführung der Bodensegmentierung. \\ \hline
\end{tabular}
\caption{Bewertungskriterien zur Analyse der Bodensegmentierungsmethoden.}
\label{tab:bewertungskriterien}
\end{table}

Die Auswahl dieser vier Bewertungskriterien orientiert sich an den Einsatzbedingungen urbaner \ac{LiDAR}-Szenarien: komplexe Geometrien und bewegte Objekte erfordern Robustheit und Genauigkeit, das Bordzeitbudget verlangt Echtzeitfähigkeit, und eingeschränkte Ressourcen machen einen geringen Rechenaufwand nötig. Tabelle~\ref{tab:bewertungskriterien} fasst die Verdichtung der sechs Anforderungen aus Tabelle~\ref{tab:anforderungen_bodensegmentierung} zusammen, indem inhaltlich verwandte Punkte gebündelt werden:
\begin{itemize}
    \item \textbf{Robustheit} fasst die Leistungsbewertung bei steigenden Hindernissen, unebenem Gelände und spärlichen Daten zusammen,
    weil alle drei Aspekte die Widerstandsfähigkeit gegenüber unterschiedlichen Gelände- und Sensordichtevariationen abprüfen.
    \item \textbf{Genauigkeit} nimmt den Aspekt der korrekten Trennung von Boden- und Nicht-Bodenpunkten aus der Segmentierungsrobustheit auf,
    um die resultierende Segmentierungsqualität klar zu bewerten.
    \item \textbf{Echtzeitfähigkeit} und \textbf{Rechenaufwand} bleiben als eigenständige Kriterien erhalten,
    da sie die Laufzeit- und Ressourcenanforderungen unabhängig von den Szenarioeigenschaften adressieren.
\end{itemize}

\section{Methoden der Bodensegmentierung}
\label{chap:bodenmethoden}

Nach~\cite{gomes2023survey} lassen sich die gängigen Verfahren zur Bodensegmentierung
in fünf Hauptkategorien einteilen (siehe Abbildung~\ref{fig:boden_taxonomie}):

\begin{itemize}
    \item 2.5D-Gitterbasierte Verfahren
    \item Bodenmodellierung (Ground Modelling)
    \item Methoden auf Basis benachbarter Punkte und lokaler Merkmale
    \item Verfahren höherer Ordnung (Higher-Order Inference)
    \item Lernbasierte Verfahren (Deep Learning)
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{Bilder/boden_taxonomie.png}
  \caption{Klassifizierung und Systematisierung bestehender Bodensegmentierungsmethoden}
  \label{fig:boden_taxonomie}
\end{figure}


Diese Klassifikation deckt sowohl klassische geometrische Ansätze als auch moderne, neuronale Verfahren ab. 
Im Folgenden werden die wichtigsten Prinzipien und repräsentativen Algorithmen jeder Kategorie erläutert.

\subsection{2.5D-Gitterbasierte Verfahren}
% ============================

Ein verbreiteter Ansatz zur Bodensegmentierung besteht darin, die dreidimensionale Punktwolke in eine zweidimensionale Rasterdarstellung zu überführen.  
Dabei werden die Punkte nach ihren Koordinaten in diskrete Zellen eingeteilt, sodass jede Zelle statistische Höheninformationen über die in ihr enthaltenen Punkte speichert.  
Dieser sogenannte 2.5D-Ansatz reduziert die Komplexität der Verarbeitung erheblich, da die Analyse nicht mehr im vollen 3D-Raum, sondern auf einer strukturierten Gitterebene erfolgt.

Douillard et al.~\cite{douillard2011segmentation} präsentieren ein solches Verfahren auf Basis von \textit{Elevation Maps}, bei dem jede Gitterzelle durch den Mittelwert der Höhenwerte ihrer Punkte beschrieben wird.  
Zellen mit geringer Höhenvarianz werden als Boden klassifiziert, während größere Abweichungen auf Objekte oder Hindernisse hinweisen.  
Durch diese Reduktion auf lokale Höhenstatistiken kann die Methode Bodenflächen effizient und robust in urbanen Umgebungen identifizieren, ohne den gesamten Punktwolkenraum verarbeiten zu müssen.


% ============================
\subsection{Bodenmodellierung (Ground Modelling)}
% ============================

Diese Methoden approximieren die Bodenfläche durch mathematische Modelle, typischerweise in Form von Ebenen oder Linien, um die Trennung zwischen Boden- und Nichtbodenpunkten zu ermöglichen.

\textbf{Plane Fitting:}  
In \cite{hu2013robust} wird die Identifikation von Bodenpunkten mithilfe der \textit{Random Sample Consensus (\ac{RANSAC})}-Methode beschrieben, 
bei der eine Ebene an die niedrigsten Punkte angepasst und Punkte mit geringem orthogonalen Abstand als \textit{Inlier} klassifiziert werden.  
Ein ähnlicher Ansatz teilt die Punktwolke in konzentrische Zonen auf und passt für jede Zone lokal angepasste Ebenen an, 
wobei die \textit{Principal Component Analysis (PCA)} eingesetzt wird, um die Hauptrichtung der Punktverteilung zu bestimmen und daraus die bestmögliche Ebenenorientierung zu berechnen \cite{lim2020fast}.  
Dieser Ansatz erreicht eine hohe Präzision (F1-Score $\approx 0.93$) bei gleichzeitigem Echtzeitverhalten.

\textbf{Linienextraktion:}
Ein auf lokalen Linienanpassungen basierendes Verfahren modelliert die entlang eines Laserstrahls erfassten Punkte als nahezu linearen Verlauf \cite{himmelsbach2010fast}.
Dieses Verfahren ist recheneffizient und eignet sich für Echtzeitverarbeitung, weist jedoch Schwächen in stark unebenem oder komplexem Gelände auf.

\textbf{Gaussian Process Regression (GPR):}  
Die \textit{Gaussian Process Regression (GPR)} wurde zur Modellierung der Bodenhöhe eingeführt, 
wobei der Höhenverlauf als kontinuierliche Funktion mit zugehöriger Unsicherheitsabschätzung beschrieben wird \cite{douillard2011segmentation}.  
Durch die Verwendung nichtstationärer Kovarianzfunktionen kann sich das Modell lokal an unterschiedliche Geländestrukturen anpassen und dadurch eine hohe Genauigkeit auch in unregelmäßigem Terrain erreichen \cite{chen2014real}.

% ============================
\subsection{Benachbarte Punkte und lokale Merkmale}
% ============================

Diese Algorithmen analysieren geometrische Beziehungen zwischen benachbarten Punkten in der Punktwolke.  
Dabei wird häufig die vertikale Kanalstruktur moderner \ac{LiDAR}-Sensoren (z.\,B. Velodyne VLP-16 oder Ouster \ac{OS1}-64) ausgenutzt, um lokale Abhängigkeiten entlang der Scanlinien zu erkennen.

\textbf{Kanalbasierte Verfahren:}  
In \cite{chu2019ground} wird die Bodenextraktion entlang vertikaler Scans beschrieben, 
bei der lokale Höhenunterschiede und Gradienten ausgewertet werden.  
Punkte zwischen einem Startbodenpunkt und einem definierten Schwellenwert werden als Boden klassifiziert.  
Das Verfahren ist recheneffizient, reagiert jedoch empfindlich auf eine ungenaue Parametrierung, etwa bei der Wahl des Höhen- oder Gradientschwellenwerts.

\textbf{Region-Growing und Clustering:}  
In \cite{moosmann2009segmentation} wird ein graphenbasiertes Region-Growing-Verfahren vorgestellt, 
bei dem benachbarte Punkte iterativ zu Regionen zusammengefügt werden, 
sofern lokale geometrische Kriterien (z.\,B. Konvexität) erfüllt sind.  
Ein alternativer Ansatz kombiniert voxelbasiertes Clustering mit statistischer Analyse, 
um Bodencluster zuverlässig zu isolieren \cite{douillard2011segmentation}.  
Solche Methoden liefern stabile Ergebnisse auch in komplexen Szenen, 
sind jedoch rechenintensiver und daher weniger für Echtzeitanwendungen geeignet.

\textbf{Range-Image-Methoden:}  
In \cite{bogoslavskyi2016fast} wird die Punktwolke in ein zweidimensionales Entfernungsbild (\textit{Range Image}) projiziert, 
bei dem jeder Pixel den Abstand eines Messpunkts zum Sensor repräsentiert.  
Diese Repräsentation ermöglicht eine effiziente Definition von Nachbarschaften und vereinfacht die anschließende Segmentierung erheblich.  
Mit diesem Ansatz kann die Bodenextraktion in wenigen Millisekunden pro Frame durchgeführt werden, was eine Echtzeitverarbeitung erlaubt.

% ============================
\subsection{Verfahren höherer Ordnung}
% ============================

Ansätze dieser Kategorie verwenden probabilistische Graphmodelle wie 
\textit{Markov Random Fields (MRF)} oder \textit{Conditional Random Fields (CRF)}, 
um Abhängigkeiten zwischen benachbarten Punkten explizit zu modellieren.  
Durch die Berücksichtigung solcher räumlicher Korrelationen können Fehlklassifikationen, 
insbesondere in spärlichen oder verrauschten Punktwolken, deutlich reduziert werden.

In \cite{guo2011ground} wird ein MRF-Modell mit dem \textit{Belief Propagation (BP)}-Verfahren kombiniert, 
das die Wahrscheinlichkeiten einzelner Punktzuordnungen iterativ aktualisiert, 
um eine konsistente Bodenfläche auch in unebenem Gelände zu rekonstruieren.  
Ein weiterentwickelter Ansatz integriert zeitliche Abhängigkeiten in ein CRF-Modell, 
wodurch die Konsistenz zwischen aufeinanderfolgenden Frames verbessert 
und die Stabilität der Segmentierung bei Bewegungen erhöht wird \cite{rummelhard2015temporal}.

% ============================
\subsection{Lernbasierte Verfahren}
% ============================

In den letzten Jahren haben sich tief neuronale Netze als besonders leistungsfähige Ansätze für die Punktwolkensegmentierung etabliert.  
Je nach Architektur werden die Sensordaten entweder direkt in Punktform verarbeitet oder zuvor in strukturierte Darstellungen überführt, um eine effiziente Merkmalsextraktion zu ermöglichen.

\textbf{PointNet- und Voxel-basierte Modelle:}  
Das in \cite{qi2017pointnet} vorgestellte PointNet-Framework ermöglicht die direkte Verarbeitung unstrukturierter Punktwolken, indem es für jeden Punkt Merkmale extrahiert und diese global aggregiert.  
Zur Erfassung lokaler geometrischer Abhängigkeiten wurde das Konzept in PointNet++ erweitert.  
Alternativ unterteilen voxelbasierte Verfahren wie VoxelNet \cite{zhou2018voxelnet} oder PointPillars \cite{lang2019pointpillars} die Punktwolke in diskrete 3D-Zellen (Voxel) bzw. Säulen und verwenden dreidimensionale Faltungsnetzwerke (3D-CNNs) zur Merkmalsanalyse.  

\textbf{Bildbasierte Ansätze:}  
In \cite{wu2018squeezeseg} und \cite{milioto2019rangenet++} werden Punktwolken in zweidimensionale Entfernungsbilder (\textit{Range Images}) projiziert, sodass konventionelle 2D-Faltungsnetzwerke auf \ac{LiDAR}-Daten angewendet werden können.  
Diese Repräsentation ermöglicht eine Echtzeitverarbeitung auf GPUs bei gleichzeitig hoher Segmentierungsgenauigkeit.  

\textbf{Spezialisierte Netze für Bodensegmentierung:}  
Ein speziell für die Bodenerkennung entwickeltes neuronales Modell ist GndNet \cite{paigwar2020gndnet}.  
Das Netz basiert auf einem zweidimensionalen Gittermodell, in dem für jede Zelle die Bodenhöhe vorhergesagt wird.  
Dieses Verfahren erreicht eine mittlere Intersection-over-Union (\ac{IoU}) von 83,6\,\% bei einer Laufzeit von nur 17,9\,ms pro Frame und ist somit für Echtzeitanwendungen geeignet.

% ============================
\subsection{Zusammenfassung}
% ============================

Tabelle~\ref{tab:vergleich_bodenmethoden} fasst die wesentlichen Eigenschaften der vorgestellten Bodensegmentierungsmethoden zusammen.
Sie verdeutlicht die jeweiligen Stärken und Grenzen der Ansätze sowie ihre typischen Einsatzgebiete in der mobilen Robotik und Umfelderkennung.

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|p{3.5cm}|p{4cm}|p{4cm}|p{3cm}|}
\hline
\textbf{Methodenkategorie} & \textbf{Vorteile} & \textbf{Nachteile} & \textbf{Typisches Einsatzgebiet} \\ \hline
2.5D-Gitterbasierte Verfahren & Geringe Rechenlast, robust auf ebenem Gelände & Begrenzte Genauigkeit bei Überhängen oder Brücken & Stadt- und Straßenszenarien \\ \hline
Bodenmodellierung & Hohe Präzision, einfache mathematische Umsetzung & Geringe Robustheit bei komplexen Oberflächenformen & Flaches bis leicht geneigtes Terrain \\ \hline
Lokale Merkmalsanalyse & Unempfindlich gegenüber Dichteänderungen & Starke Abhängigkeit von Parameterwahl und Sensorgeometrie & Dynamische oder unstrukturierte Umgebungen \\ \hline
Probabilistische Graphmodelle (MRF/CRF) & Hohe Konsistenz durch Nachbarschaftsbeziehungen & Hohe Rechenkomplexität, geringe Echtzeitfähigkeit & Forschungs- und Entwicklungsumgebungen \\ \hline
Lernbasierte Verfahren & Sehr hohe Genauigkeit, anpassungsfähig durch Training & Großer Trainings- und Hardwareaufwand & Autonomes Fahren, Echtzeit-Perzeption \\ \hline
\end{tabular}
\caption{Vergleich der Bodensegmentierungsmethoden in Bezug auf Rechenaufwand, Robustheit und Einsatzgebiet (nach~\cite{gomes2023survey}).}
\label{tab:vergleich_bodenmethoden}
\end{table}

\begin{table}[h!]
  \centering
  \renewcommand{\arraystretch}{1.2}
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|c|c|c|c|}
      \hline
      \textbf{Methode} & \textbf{Echtzeitfähigkeit} & \textbf{Robustheit} & \textbf{Genauigkeit} & \textbf{Rechenaufwand} \\
      \hline
      2.5D-Gitterbasierte Verfahren & Hoch (Zellstatistiken pro Raster) & Mittel (stabil bei moderaten Höhenänderungen) & Mittel (verlustbehaftete 2.5D-Projektion) & Gering \\ \hline
      \ac{RANSAC}-Ebenenanpassung (Plane Fitting) & Mittel (Iterationsbudget begrenzt Laufzeit) & Hoch (ausreißerresistent) & Hoch (präzise Ebenenmodellierung) & Mittel bis hoch \\ \hline
      Linienextraktion entlang Scanlinien & Hoch (lineare Fits pro Strahl) & Niedrig bis mittel (empfindlich auf unebenes Gelände) & Mittel & Sehr gering \\ \hline
      Gaussian Process Regression (GPR) & Niedrig (aufwändige Kernelberechnung) & Hoch (passt sich lokalem Terrain an) & Hoch & Hoch \\ \hline
      Kanalbasierte Verfahren (gradientenbasiert) & Hoch (schlanke Schwellenwertlogik) & Mittel (parametergängig, sensorgeometrieabhängig) & Mittel & Gering \\ \hline
      Region-Growing / graphenbasiertes Clustering & Niedrig bis mittel (iterative Nachbarschaftsprüfung) & Mittel bis hoch (reduziert Fehlklassifikationen) & Hoch & Mittel bis hoch \\ \hline
      Range-Image-Methoden & Hoch (2D-Nachbarschaften in Millisekunden) & Mittel (robust gegen Rauschen, sensibel bei Überhängen) & Mittel bis hoch & Gering bis mittel \\ \hline
      Probabilistische Graphmodelle (MRF/CRF) & Niedrig (iterative Inferenz) & Hoch (konsistente Nachbarschaftsmodellierung) & Hoch & Hoch \\ \hline
      Point-/Voxel-basierte Netze (PointNet, PointPillars) & Mittel (GPU-Echtzeit bei optimierter Implementierung) & Hoch (lernbasierte Generalisierung) & Sehr hoch & Hoch \\ \hline
      Bildbasierte CNNs auf Range Images (z.B. RangeNet++) & Hoch (2D-CNN-Inferenz auf GPU) & Mittel bis hoch (robust auf strukturierten Projektionen) & Hoch & Mittel bis hoch \\ \hline
      Spezialisierte Netze (GndNet) & Hoch (Inference $\approx$ 18\,ms/Frame) & Mittel bis hoch (geländeadaptiv trainierbar) & Hoch (\ac{IoU} $\approx$ 84\,\%) & Mittel \\ \hline
    \end{tabular}%
  }
  \caption{Vergleich der in Abschnitt~\ref{chap:bodenmethoden} beschriebenen Bodensegmentierungsmethoden nach Echtzeitfähigkeit, Robustheit, Genauigkeit und Rechenaufwand (nach~\cite{gomes2023survey}).}
  \label{tab:boden-methodenvergleich-ransac}
\end{table}

\section{RANSAC-Verfahren}
\label{sec:ransac_verfahren}

Das \textit{Random Sample Consensus} (\ac{RANSAC}) bietet eine robuste Ebenenabschätzung, indem es Ausreißer konsequent ignoriert und lediglich Punkte berücksichtigt, die innerhalb eines orthogonalen Abstands zur Modellhypothese liegen.
Im Unterschied zu raster- oder morphologiebasierten Ansätzen entsteht so eine präzise Modellierung auch bei schrägen oder partiell verdeckten Fahrbahnen, zu Lasten eines moderaten Iterationsaufwands und einer sorgfältigen Schwellenwertwahl.
Aufgrund dieses Gleichgewichts zwischen Genauigkeit und Implementierungskomplexität dient \ac{RANSAC} in dieser Arbeit als Grundlage der Bodensegmentierung.

Das Verfahren wiederholt folgende Schritte, bis entweder das Iterationsbudget ausgeschöpft ist oder ein Modell die gewünschte Sicherheit erreicht:
\begin{enumerate}
  \item \textbf{Stichprobenbildung:} Aus der Punktwolke werden minimal viele Punkte (\(n=3\) für Ebenen) zufällig gewählt, um einen ersten Ebenenvorschlag zu erzeugen.
  \item \textbf{Modellhypothese:} Aus den Stichprobenpunkten wird eine Ebenengleichung \(a x + b y + c z + d = 0\) berechnet, die Normalenrichtung und Lage der Fahrbahn repräsentiert.
  \item \textbf{Inlier-Prüfung:} Jeder Punkt wird über seinen orthogonalen Abstand
    \[\mathrm{dist}(\mathbf{p}, \Pi) = \frac{|a x + b y + c z + d|}{\sqrt{a^2 + b^2 + c^2}}\]
    zum Ebenenmodell bewertet. Punkte unterhalb des Schwellenwerts \texttt{distance\_threshold} gelten als Inlier.
  \item \textbf{Bewertung und Wiederholung:} Die Hypothese mit den meisten Inliern wird gemerkt; anschließend werden neue Stichproben gezogen, bis die gewünschte Erfolgswahrscheinlichkeit erreicht ist. Aus dem Verhältnis von Inlier-Anteil \(w\) und Maximaliterationen \(k\) ergibt sich \(1-(1-w^n)^k\) als Wahrscheinlichkeit, mindestens eine ausreißerfreie Stichprobe gesehen zu haben.
\end{enumerate}
Die resultierende Ebene trennt Boden- von Hindernispunkten durch ihre senkrechte Distanz. Der iterative Stichprobencharakter geht auf den ursprünglichen \ac{RANSAC}-Vorschlag von Fischler und Bolles zurück, der explizit eine robuste Parameterschätzung bei hohem Ausreißeranteil adressiert \parencite{fischler1981ransac}. Für Punktwolken und Ebenenextraktion wurde das Verfahren unter anderem von Schnabel et~al. für große 3D-Datensätze adaptiert \parencite{Schnabel2007Efficient}. Abbildung~\ref{fig:plane_fitting} zeigt die geometrische Interpretation dieser Distanzklassifizierung nach \textcite{gomes2023survey}.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{Bilder/plane_fitting.png}
  \caption{Visuelle Darstellung einer orthogonalen Distanzklassifizierung (~\cite{gomes2023survey})}
  \label{fig:plane_fitting}
\end{figure}

\section{Implementierung}

\label{sec:implementierung_ransac}

Die Bodensegmentierung wurde als eigenständiger \ac{ROS}~2-Knoten in \texttt{C++} mit der \textit{Point Cloud Library (\ac{PCL})} implementiert. 
Der Knoten \texttt{ransac\_ground\_node} abonniert gefilterte \ac{LiDAR}-Punktwolken, schätzt eine Bodenebene mittels \textit{SAC-\ac{RANSAC}} und veröffentlicht eine Hindernis-Punktwolke, aus der die Bodenpunkte entfernt wurden. 
Die Architektur ist strikt streaming-orientiert (Callback-basiert) und verzichtet auf Blockierungen, wodurch eine niedrige Latenz erzielt wird.

Der Knoten deklariert die Ein- und Ausgabetopics als Parameter und nutzt \texttt{SensorDataQoS}:
\begin{itemize}
  \item \textbf{Eingabe (\texttt{input\_topic})}: \texttt{/points\_voxel} \ \emph{(Typ: \texttt{sensor\_msgs/PointCloud2})}, enthält bereits per VoxelGrid ausgedünnte Punkte (\texttt{pcl::PointXYZI}).
  \item \textbf{Ausgabe (\texttt{output\_topic})}: \texttt{/obstacle\_points} \ \emph{(Typ: \texttt{sensor\_msgs/PointCloud2})}, enthält ausschließlich Nicht-Boden-Punkte.
\end{itemize}
Leere Eingaben werden verworfen, um unnötige Rechenarbeit zu vermeiden.

Die wesentlichen Laufzeitparameter werden als \ac{ROS}-Parameter deklariert und können über Launch-Dateien oder \texttt{ros2 param} angepasst werden (Default-Werte aus dem Code):
\begin{table}[h!]
\centering
\begin{tabular}{|l|c|l|}
\hline
\textbf{Parameter} & \textbf{Default} & \textbf{Bedeutung} \\ \hline
\texttt{input\_topic} & \texttt{/points\_voxel} & Eingangs-Punktwolke (vorsegmentiert per VoxelGrid) \\ \hline
\texttt{output\_topic} & \texttt{/obstacle\_points} & Ausgabe ohne Bodenpunkte \\ \hline
\texttt{distance\_threshold} & $0{,}15\,\mathrm{m}$ & maximaler Punkt-zu-Ebene-Abstand für Inlier \\ \hline
\texttt{max\_iterations} & $1000$ & maximale \ac{RANSAC}-Iterationen \\ \hline
\end{tabular}
\caption{\ac{RANSAC}-Parametrisierung des \texttt{ransac\_ground\_node}.}
\label{tab:ransac_params}
\end{table}

Zur Ebenenschätzung wird \texttt{pcl::SACSegmentation} mit \texttt{SACMODEL\_PLANE} und \texttt{SAC\_RANSAC} verwendet, inkl.\ Koeffizientenoptimierung:
\begin{enumerate}
  \item \textbf{Konvertierung}: \texttt{sensor\_msgs/PointCloud2} $\rightarrow$ \texttt{pcl::PointCloud<pcl::PointXYZI>}.
  \item \textbf{\ac{RANSAC}-Konfiguration}: \texttt{setOptimizeCoefficients(true)}, \texttt{setModelType(PLANE)}, \texttt{setMethodType(\ac{RANSAC})}, \texttt{setDistanceThreshold}, \texttt{setMaxIterations}.
  \item \textbf{Segmentierung}: \texttt{seg.segment(inliers, coefficients)} liefert Inlier-Indizes der Bodenpunkte und Ebenenparameter $\mathbf{n}=(a,b,c)$, $d$ der Ebene
  \[
    a x + b y + c z + d = 0.
  \]
  \item \textbf{Extraktion der Hindernisse}: \texttt{pcl::ExtractIndices} mit \texttt{setNegative(true)} filtert alle Punkte \emph{außerhalb} der Bodenenebene (\,$>$ \texttt{distance\_threshold}\,).
  \item \textbf{Publikation}: Rückkonvertierung nach \texttt{PointCloud2} und Veröffentlichung auf \texttt{/obstacle\_points} (mit ursprünglichem Header/Frame).
\end{enumerate}
Ein Punkt $\mathbf{p}=(x,y,z)^\top$ zählt als Inlier, wenn sein orthogonaler Abstand zur Ebene
\[
  \mathrm{dist}(\mathbf{p}, \Pi) \;=\; \frac{|a x + b y + c z + d|}{\sqrt{a^2+b^2+c^2}}
\]
kleiner gleich \texttt{distance\_threshold} ist.

\section{Ergebnis}
Als Ergebnis ergibt sich eine Punktwolke, die frei von Bodenpunkten ist (siehe Abbildung~\ref{fig:ransac_compare}). 
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{Bilder/ransac_compare.png}
  \caption{Vergleich der ursprünglichen Punktwolke (links) und der nach dem \ac{RANSAC}-Algorithmus gefilterten Punktwolke (rechts) (2025-10-15-Messdaten/2025-10-15-VK-OL-005)}
  \label{fig:ransac_compare}
\end{figure}

\chapter{Cluster-Extraktion und Ermittlung von Bounding-Boxes}
\label{chap:clustering}
\label{chap:cluster_extraction}

Nach der Entfernung der Bodenpunkte (vgl. Kapitel~\ref{chap:bodensegmentierung}) werden die verbleibenden Punktwolken in zusammenhängende Punktmengen (Cluster) gruppiert und mit Begrenzungsboxen beschrieben. Die Stufe bildet somit die Brücke zwischen Bodensegmentierung und Tracking: Sie fasst Punkte zu Objekten zusammen, bestimmt deren Lage und Abmessungen und stellt diese als Detektionen für die Verfolgung bereit. Der Schwerpunkt liegt auf einer robusten, echtzeitfähigen Verarbeitung, die auch bei variabler Punktdichte und Teilokklusion konsistente Boxen liefert.

\section{Cluster-Extraktion}
\label{sec:cluster_extraction}

Zwei Punkte \(p_i=(x_i,y_i,z_i)^\top\) und \(p_j\) gehören demselben Cluster an, wenn sie in einem geeigneten Nachbarschaftsbegriff als verbunden gelten. Daraus lässt sich ein ungerichteter Graph \(G=(V,E)\) konstruieren, dessen Knoten \(V\) die Punkte und dessen Kanten \(E\) die Paarverbindungen darstellen. Die Zusammenhangskomponenten dieses Graphen entsprechen den Clustern; Traversierungen (BFS/DFS) über einen k-d-Baum liefern in \(O(n \log n)\) die Komponenten \parencite{rusu2011pcl,Douillard2011}. Im gängigen euklidischen Ansatz bildet die Distanz
\[
 d(p_i,p_j) = \sqrt{(x_i-x_j)^2+(y_i-y_j)^2+(z_i-z_j)^2}
\]
die Grundlage; eine feste Verknüpfungsschwelle \(\varepsilon\) definiert die Kanten von \(G\). \ac{LiDAR}-Punktwolken besitzen aufgrund ihres winkelbasierten Abtastgitters eine von der Entfernung abhängige, anisotrope Punktdichte \parencite{himmelsbach2010fast}. Deshalb beeinflussen Vorverarbeitung (Voxelgröße) und die Wahl von \(\varepsilon\) die Fragmentierung/Verschmelzung besonders stark: Ein zu kleiner Schwellenwert lässt weit entfernte Objekte zerfallen, ein zu großer führt zum Verschmelzen benachbarter Objekte im Nahbereich. Abbildung~\ref{fig:cluster_principle} skizziert das Vorgehen und verdeutlicht, dass die Clusterung die eigentliche Objektbildung für die nachgelagerte Verfolgung bereitstellt.

Für jede identifizierte Komponente werden im Anschluss Lage (Zentrum) und Ausdehnung über Begrenzungsboxen bestimmt. Achsenparallele Boxen (AABB) greifen auf Min-/Max-Koordinaten zurück, orientierte Boxen (OBB) nutzen Trägheitstensor oder Hauptachsentransformation (PCA), um die Orientierung der Punktverteilung widerzuspiegeln \parencite{gottschalk1996obb}. Beide Varianten liefern damit ein abstrahiertes, metrisches Objektmodell, das als \emph{Detektion} publiziert wird und die Initialisierung der Tracking-Stufe ermöglicht.

In dieser Arbeit wird die euklidische Clusterung der \emph{Point Cloud Library (\ac{PCL})} eingesetzt (\texttt{pcl::EuclideanClusterExtraction}) in Verbindung mit einem k-d-Baum (\texttt{pcl::search::KdTree}) \parencite{pcl_docs_2025}. Der Algorithmus folgt dem in der Literatur etablierten Ablauf: Aufbau eines Suchbaums, breadth-first/ depth-first Traversierung und komponentenweise Markierung der Punkte \parencite{Douillard2011,rusu2011pcl}. Gegenüber dichtebasierten Verfahren wie \ac{DBSCAN} sind die Parameterzahl und die Rechenzeit deterministischer, was die Integration in eine Echtzeit-Pipeline erleichtert \parencite{Ester1996DBSCAN}. Zusätzlich ist das Verfahren trainingsfrei und benötigt nur wenige, gut interpretierbare Parameter:

\begin{itemize}
  \item \textbf{\texttt{cluster\_tolerance}} \(\varepsilon\) [m]: maximaler euklidischer Punktabstand innerhalb eines Clusters (räumliche Verknüpfung).
  \item \textbf{\texttt{min\_cluster\_size}} / \textbf{\texttt{max\_cluster\_size}}: untere/obere Schranke der Punktanzahl pro Cluster (Rauschen verwerfen, Ausreißer begrenzen).
  \item \textbf{\texttt{max\_clusters}}: harte Obergrenze pro Frame (Determinismus, Zeitbudget).
\end{itemize}

Richtwerte im Zusammenspiel mit einem Voxel-Filter (Voxelkantenlänge \(\approx\)\SI{0.15}{\meter}–\SI{0.25}{\meter}):
\(\varepsilon \in [\SI{0.3}{\meter},\SI{0.7}{\meter}]\), \texttt{min\_cluster\_size} \(\approx 30\)–\(60\), \texttt{max\_clusters} \(\approx 200\). Solche Werte liegen im Bereich typischer Automotive-Studien, in denen \(\varepsilon\) zwischen \SI{0.4}{\meter} und \SI{0.8}{\meter} gewählt wird, um Fahrzeuge und Fußgänger bei mittlerer Voxelung zu trennen \parencite{himmelsbach2010fast,Douillard2011}. Kleinere \(\varepsilon\) vermeiden Verschmelzen benachbarter Objekte, größere \(\varepsilon\) reduzieren Fragmentierung in der Ferne. Alternativen wie dichtebasierte Verfahren (\ac{DBSCAN}/HDBSCAN) sind möglich, werden hier zugunsten der Echtzeitfähigkeit und einfachen Parametrik jedoch nicht eingesetzt \parencite{Ester1996DBSCAN}.

\paragraph{Ablauf (Algorithmus)}
\begin{enumerate}
  \item \textbf{Vorbereitung}: optionales Downsampling per Voxel (gleichmäßigere Dichte, O(\(n\))).
  \item \textbf{k-d-Baum}: Aufbau für die reduzierte Punktwolke (O(\(n\log n\))).
  \item \textbf{Region Growing}: iteratives Durchlaufen unbesuchter Punkte; Nachbarsuche innerhalb \(\varepsilon\) über k-d-Baum, Zusammenfassen zu Komponenten (typisch \(\in O(n\log n)\)).
  \item \textbf{Selektion}: Verwerfen zu kleiner/großer Komponenten per \texttt{min/max\_cluster\_size}.
\end{enumerate}

\paragraph{Praktische Aspekte}
- \emph{Komplexität}: der Flaschenhals ist die Nachbarsuche; Voxel-Dowsampling reduziert \(n\) und beschleunigt die Suche signifikant \parencite{rusu2011pcl}.
- \emph{Parameterkopplung}: sinnvolle Werte von \(\varepsilon\) korrelieren mit der Voxelgröße. Faustregel: \(\varepsilon\approx 2\dots 3\)\,\(\times\)\,\texttt{voxel\_size}; bei anisotroper Punktdichte (winkelbasiertes Abtastgitter) verstärkt sich dieser Effekt \parencite{himmelsbach2010fast}.
- \emph{Reichweiten-Adaption (optional)}: \(\varepsilon(r)=\alpha r+\beta\) kann Fernbereichs-Fragmentierung reduzieren, wie in Reichweiten-bewussten Segmentierungen gezeigt \parencite{bogoslavskyi2016fast}; in dieser Arbeit verwenden wir für Determinismus und Einfachheit eine konstante \(\varepsilon\).

\begin{table}[H]
  \centering
  \begin{tabular}{|l|c|p{8cm}|}
    \hline
    \textbf{Parameter} & \textbf{Beispiel} & \textbf{Bedeutung} \\ \hline
    \texttt{cluster\_tolerance} & \(0{,}5\,\mathrm{m}\) & Verknüpfungsschwelle in der euklidischen Nachbarschaft \\ \hline
    \texttt{min\_cluster\_size} & 40 & Rauschunterdrückung, minimale Punktzahl \\ \hline
    \texttt{max\_cluster\_size} & 8000 & Obergrenze zur Begrenzung großer Flächen/Hecken \\ \hline
    \texttt{max\_clusters} & 200 & Harte Obergrenze pro Frame (Zeitbudget/Determinismus) \\ \hline
  \end{tabular}
  \caption{Parameter der euklidischen Clusterung (Richtwerte in den Experimenten).}
  \label{tab:cluster_params_new}
\end{table}

\section{Ermittlung von Bounding-Boxes}
\label{sec:boxes}
Für jeden Cluster werden Zentrum und Ausdehnung über eine Begrenzungsbox bestimmt. Unterschieden werden achsenparallele Boxen (AABB) und orientierte Boxen (OBB). Beide Varianten liefern ein Zentrum \(\mathbf{c}\) und die Kantenlängen \((L, W, H)\); OBB enthält zusätzlich eine Orientierung \(\mathbf{q}\) (z.\,B. als Quaternion). Abbildung~\ref{fig:aabb_obb_compare} zeigt den praktischen Unterschied.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{Bilder/aabb_obb_vs.png}
  \caption{Vergleich der Bounding-Box-Modelle \parencite{DarkRock2023AABB_OBB}}
  \label{fig:aabb_obb_vs}
\end{figure}

Die Boxermittlung folgt direkt auf die Punktzusammenfassung und nutzt ausschließlich Clusterpunkte; zusätzliche Transformationen oder Hilfsdaten sind nicht notwendig. Dadurch bleibt der gesamte Schritt deterministisch und gut kontrollierbar.

\subsection{AABB}
\label{sec:aabb}
Die AABB wird durch die minimalen und maximalen Koordinaten je Achse bestimmt:
\(x_{\min},x_{\max},y_{\min},y_{\max},z_{\min},z_{\max}\). Zentrum und Kantenlängen ergeben sich zu
\[\mathbf{c}=\tfrac{1}{2}\bigl((x_{\min},y_{\min},z_{\min})^\top+(x_{\max},y_{\max},z_{\max})^\top\bigr),\quad
 L=x_{\max}-x_{\min},\; W=y_{\max}-y_{\min},\; H=z_{\max}-z_{\min}.
\]

In der Implementierung besteht der Aufwand nur aus sechs Min-/Max-Suchen und drei Differenzen, skaliert damit linear mit der
Punktanzahl und bleibt deterministisch reproduzierbar – selbst bei grobem Downsampling. Ausreißer lassen sich vorab durch
einfache Filter entfernen, sodass die Min-/Max-Suche stabile Boxen liefert.

Nachteil der AABB ist, dass sie die Orientierung des Objekts nicht abbildet und bei stark gedrehten Objekten in der Draufsicht
(\ac{BEV}) eine größere Grundfläche einnimmt als nötig. Für Fahrszenarien mit überwiegend längs ausgerichteten Verkehrsteilnehmern
ist dieser Effekt jedoch beherrschbar, solange nachgelagerte Filter (z.\,B. Größenprüfung) geeignete Toleranzen besitzen.

\subsection{OBB}
\label{sec:obb}
Die OBB richtet die Box entlang der Hauptachsen des Punktverteilungstensors aus. Praktisch wird dies in \ac{PCL} über die Trägheits-/PCA-Schätzung realisiert (z.\,B. \texttt{pcl::MomentOfInertiaEstimation}) \parencite{pcl_docs_2025}. Vorteile: kompaktere Umhüllung bei deutlich orientierten, elongierten Objekten (Fahrzeuge, Leitplanken); die abgeleitete Yaw kann nachgelagert genutzt werden. Nachteile: höhere Rechenkosten und potenzielle Instabilitäten bei kleinen, teilverdeckten oder nahezu isotropen Clustern (Rauscheinfluss, Sprünge der Hauptachsen). Eine alternative, zweidimensionale Näherung ist die Bestimmung eines Minimalrechtecks in der Draufsicht (\ac{BEV}), welches besonders stabile Yaw-Werte für straßennahe Objekte liefert; diese Option ist mit geringem Mehraufwand realisierbar und lässt sich mit AABB kombinieren (AABB für \(L,W,H\), Yaw aus \ac{BEV}). Abbildung~\ref{fig:bev_minarea} reserviert Platz für eine entsprechende Darstellung.

\subsection{Vergleich AABB vs. OBB und Wahl in dieser Arbeit}
\textbf{AABB} arbeiten sehr schnell, stabil und deterministisch, bilden jedoch keine Orientierung ab und neigen bei stark gedrehten Objekten zu einer größeren Grundfläche. \textbf{OBB} liefern dagegen eine orientierte und bei elongierten Strukturen meist kompaktere Hülle, erfordern aber mehr Rechenaufwand, reagieren empfindlicher auf Ausdünnung oder Teilsicht und können dadurch sprunghaft wechseln.

Für die Versuchsumgebung wurden beide Ansätze implementiert und mit denselben Punktwolken ausgewertet. Die OBB-Variante liefert
für gut ausgeprägte, längliche Fahrzeuge tatsächlich kompaktere Hüllen und eine nutzbare Yaw-Schätzung. Bei teilverdeckten oder
kleinen Clustern (z.\,B. Poller, Fußgänger) kam es jedoch zu instabilen Hauptachsen und sprunghaften Orientierungen, was nachgelagert
die Datenassoziation im Tracking erschwerte.

Die AABB erreichte demgegenüber konsistent stabile Abmessungen und reagierte kaum empfindlich auf Ausdünnung oder temporäre
Okklusionen. In Kombination mit einem BEV-Minimalrechteck (nur für die Yaw-Schätzung) ließe sich der Orientierungsnachteil
weiter reduzieren, ohne die AABB-Orientierung vollständig zu ersetzen. Aus Gründen der Robustheit und Echtzeitfähigkeit wird
daher in dieser Arbeit \textbf{AABB} als Standard gewählt; OBB bleibt optional für spezielle Auswertungen verfügbar.
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{Bilder/aabb_obb_compare.png}
  \caption{Vergleich der Bounding-Box-Modelle: oriented bounding boxes (OBB, Mitte) gegenüber axis-aligned bounding boxes (AABB, rechts) anhand derselben Punktwolke (links). (2025-10-15-VK-OL-005)}
  \label{fig:aabb_obb_compare}
\end{figure}

Hauptgründe für diese Wahl sind die hohe Stabilität der Boxen unter realen Sensorbedingungen (variable Punktdichte, Okklusionen) und die geringen Rechenkosten, die die Echtzeitfähigkeit der Gesamtkette sichern. Die Orientierung kann optional aus nachfolgenden Schritten (z.\,B. in \ac{BEV}) geschätzt werden, ohne die Boxbildung zu destabilisieren. Eine OBB-Erweiterung bleibt als künftige Verbesserung vorgesehen, sobald robuste Orientierungsmerkmale (etwa ein \ac{BEV}-MinArea-Rechteck) zuverlässig verfügbar sind. Abbildung~\ref{fig:aabb_obb_compare} zeigt den praktischen Unterschied.

\section{Implementierung}
\label{sec:implementierung_cluster_boxes}

Die Umsetzung erfolgt auf Basis von \ac{PCL} und \ac{ROS}~2 als durchgängige Verarbeitungskette. Auf dem Topic \texttt{/obstacle\_points} wird die \emph{EuclideanClusterExtraction} mit einer KdTree-Struktur angewendet, um Punktwolken in einzelne Objekte zu segmentieren. Die resultierenden Cluster werden anschließend sowohl als Marker für \ac{RViz} visualisiert als auch in Form eines \texttt{vision\_msgs/Detection3DArray} veröffentlicht. Die Verarbeitungskette ist darauf ausgelegt, deterministische Latenzen und eine robuste Plausibilisierung der Objekte sicherzustellen.

Als Eingabe dient das Topic \texttt{/obstacle\_points} mit dem Nachrichtentyp \texttt{sensor\_msgs/PointCloud2} unter Verwendung des \emph{SensorDataQoS}-Profils. Als Ausgabe werden die geschätzten Objektboxen in einem \texttt{vision\_msgs/Detection3DArray} bereitgestellt, welches Zentrum, Ausmaße und optional die Orientierung umfasst. Zusätzlich erfolgt eine parallele Veröffentlichung als \ac{RViz}-Marker zur visuellen Kontrolle. Für den Sensorpfad wird ein \emph{BestEffort}-Profil mit \emph{KeepLast} (Tiefe \(\leq 4\)) verwendet, während die Ergebnis-Topics zuverlässig (\emph{reliable}) übertragen werden. Eine harte Obergrenze \texttt{max\_clusters} sowie die Vorallokation der Speicherstrukturen verhindern Latenzspitzen bei dichter Punktbelegung.

Zur Unterdrückung von Rauschen und großflächigen Artefakten werden größenbasierte Filter eingesetzt. Objekte, deren geschätzte Ausmaße außerhalb plausibler Grenzen liegen – etwa extrem flache Fassaden oder unrealistisch große Boxen – werden verworfen. Die Parameter \(\varepsilon\), \texttt{min\_cluster\_size} und die Grenzen der Größenfilter sind direkt mit der Auflösung des Voxel-Downsamplings zu koppeln. Eine praxisnahe Faustregel ist \(\varepsilon \approx 2 \dots 3 \times \texttt{voxel\_size}\). Für die Schätzung der Höhe \(H\) eignen sich robuste Perzentilschätzer, um Ausreißer in vertikaler Richtung zu kompensieren.

Die Kombination aus Voxel-Downsampling, KdTree-Suche und einer festen Maximalzahl an Clustern ermöglicht eine stabile Verarbeitung in Echtzeit. Zudem wird bewusst auf Axis-Aligned Bounding Boxes (AABB) statt auf rechenintensivere OBBs zurückgegriffen, da AABBs bei Fahrzeugumgebungen als numerisch stabil und ausreichend präzise gelten. Insgesamt führt dieser Aufbau zu einer effizienten Segmentierung mit konstanten Antwortzeiten und gut kontrollierbaren Ressourcenanforderungen.

\section{Ergebnis}
\label{sec:ergebnis_cluster_boxes}
In den Experimenten liefert die Verarbeitungskette konsistente Boxen bei moderatem Rechenaufwand. Die AABB-Wahl erweist sich als robust gegenüber variabler Punktdichte und Teilokklusion.

Abbildung~\ref{fig:clustering_result} zeigt den Cluster eines detektierten Pkw mit AABB-Dimensionen.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\textwidth]{Bilder/clustering_result.png}
  \caption{Detektierter Fahrzeug-Cluster mit AABB-Dimensionen (3.18 × 2.01 × 1.17 m) (2025-10-15-VK-OL-005)}
  \label{fig:clustering_result}
\end{figure}

\chapter{Objektverfolgung}

Dieses Kapitel beschreibt, wie die zuvor extrahierten Detektionen über die Zeit hinweg zu konsistenten Objektspuren zusammengeführt werden. Der Fokus liegt auf dem zweidimensionalen Kalman-Filter mit passendem Bewegungsmodell, der Mess- und Zuordnungslogik mittels Ungarischem Algorithmus sowie dem Track-Management. Abschließend werden die Testschritte und die beobachtete Stabilität der Track-IDs zusammengefasst.

\section{Prinzip}

Kern des Trackers ist ein zweidimensionaler Kalman-Filter, der Positions- und Geschwindigkeitsanteile in der Straßenebene fortschreibt und bei neuen Messungen aktualisiert. Jeder Tracking-Zyklus durchläuft denselben Ablauf: (1) \textbf{Prädiktion} aller bestehenden Tracks anhand des Bewegungsmodells, (2) \textbf{Gating} der eingehenden Detektionen über Mahalanobis-Distanzen, (3) \textbf{globale Zuordnung} der verbleibenden Paare per Ungarischem Algorithmus sowie (4) \textbf{Track-Management}. Dieses verwaltet Neuinitialisierungen aus nicht zugeordneten Detektionen, lässt bestehende Tracks für eine begrenzte \textit{Time-to-Live} ohne Messung weiterlaufen und entfernt überalterte Spuren. Die Beschränkung auf die 2D-Ebene reduziert den Rechenaufwand, da Höhenvariationen bereits in der Clustering-Stufe bewertet werden. Die folgenden Abschnitte erläutern Kalman-Filter und Zuordnungslogik im Detail.

\subsection{2D-Kalman-Filter}
Der Kalman-Filter schätzt den Zustandsvektor eines Objekts und aktualisiert ihn bei jeder neuen Messung (\cite{kalman1960}). Im hier verwendeten 2D-Modell lautet der Zustandsvektor
\[
\mathbf{x} =
\begin{bmatrix}
x \\ y \\ v_x \\ v_y
\end{bmatrix},
\]
wobei $(x,y)$ die Position und $(v_x, v_y)$ die Geschwindigkeit in der Ebene beschreiben. Als Bewegungsmodell wird konstante Geschwindigkeit angenommen, d.\,h. die Position entwickelt sich proportional zur Geschwindigkeit, während die Geschwindigkeit unverändert bleibt. Dieses lineare Modell ist für Tracking-Aufgaben im Verkehrsumfeld weit verbreitet \parencite{bar2004estimation}.

Die Zustandsvorhersage erfolgt mittels Übergangsmatrix
\[
\mathbf{F} =
\begin{bmatrix}
1 & 0 & \Delta t & 0\\
0 & 1 & 0 & \Delta t\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{bmatrix},
\]
wobei $\Delta t$ die Zeitdifferenz zwischen zwei Messungen darstellt. Die Kovarianzmatrix wird entsprechend
\[
\mathbf{P}_{k|k-1} = \mathbf{F}\,\mathbf{P}_{k-1|k-1}\,\mathbf{F}^T + \mathbf{Q}
\]
fortgeschrieben, wobei $\mathbf{Q}$ das Prozessrauschen modelliert und Beschleunigungen oder Modellabweichungen abdeckt. In der Implementierung wird $\mathbf{Q}$ auf Basis der empirisch beobachteten Geschwindigkeitsänderungen gewählt (typisch $\sigma_a \approx 1\,\mathrm{m/s^2}$), um kurzfristige Manöver abzufangen, ohne dass der Filter instabil wird.

Liegt eine neue Detektion vor, wird der Filter aktualisiert. Die erwartete Messung wird über
\[
\mathbf{z}_{k|k-1} = \mathbf{H}\,\mathbf{x}_{k|k-1}, \quad
\mathbf{H} =
\begin{bmatrix}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0
\end{bmatrix}
\]
berechnet. Der Innovationsvektor lautet
\[
\mathbf{y}_k = \mathbf{z}_k - \mathbf{H}\,\mathbf{x}_{k|k-1},
\]
und mit der Innovationskovarianz
\[
\mathbf{S}_k = \mathbf{H}\,\mathbf{P}_{k|k-1}\,\mathbf{H}^T + \mathbf{R}
\]
ergibt sich der Kalman-Gewinn:
\[
\mathbf{K}_k = \mathbf{P}_{k|k-1}\,\mathbf{H}^T\,\mathbf{S}_k^{-1}.
\]

Damit wird der Zustand korrigiert zu
\[
\mathbf{x}_{k|k} = \mathbf{x}_{k|k-1} + \mathbf{K}_k\,\mathbf{y}_k,
\]
und die Unsicherheit reduziert sich entsprechend
\[
\mathbf{P}_{k|k} = (\mathbf{I} - \mathbf{K}_k\mathbf{H})\,\mathbf{P}_{k|k-1}.
\]

Die normierte Innovationsgröße
\[
d^2 = \mathbf{y}_k^T\,\mathbf{S}_k^{-1}\,\mathbf{y}_k
\]
entspricht dem Mahalanobis-Abstand und dient als statistisches Maß, ob eine Messung konsistent zu einem Track passt. Große Abweichungen führen zur Ablehnung der Zuordnung (\textit{Gating}) (\cite{bar2004estimation}). Als Gate wird ein $\chi^2$-Schwellwert (typisch $d^2 \leq 9$ für 95~\% Konfidenz in 2D) genutzt. Nicht geupdatete Tracks behalten ihre Prädiktion, ihre Kovarianz wächst jedoch, wodurch der Gate-Radius über die Zeit zunimmt und späte Wiedererkennungen erleichtert.

\subsection{Datenassoziation mit dem Ungarischen Algorithmus}

Um in jedem Zeitschritt neue Detektionen den bestehenden Objekt-Spuren (Tracks) korrekt zuzuordnen, wird ein Zuordnungsproblem gelöst. Hierfür wird zunächst eine Kostenmatrix
\[
\mathbf{C} \in \mathbb{R}^{N_\text{Track} \times N_\text{Detektion}}
\]
aufgebaut, deren Eintrag $c_{ij}$ die Kosten einer möglichen Zuordnung zwischen Track $i$ und Detektion $j$ beschreibt. In dieser Arbeit wird als Kostenmaß der quadratische Mahalanobis-Abstand verwendet,
\[
c_{ij} = (\mathbf{z}_j - \hat{\mathbf{z}}_i)^\mathrm{T}\, \mathbf{S}_i^{-1}\, (\mathbf{z}_j - \hat{\mathbf{z}}_i),
\]
wobei $\hat{\mathbf{z}}_i$ die vom Kalman-Filter vorhergesagte Position des Tracks und $\mathbf{S}_i$ die Innovationskovarianz ist \parencite{bar2004estimation}. Kleine Werte stehen für hohe Übereinstimmung. 

Um unrealistische Paarungen auszuschließen, wird ein \textit{Gating} durchgeführt: liegt $c_{ij}$ oberhalb eines Schwellenwerts $\tau$, wird diese Zuordnung verworfen. Praktisch wird dies durch Setzen des Matrixeintrags auf einen sehr großen Wert erreicht:
\[
c_{ij} =
\begin{cases}
(\mathbf{z}_j - \hat{\mathbf{z}}_i)^\mathrm{T}\, \mathbf{S}_i^{-1}\, (\mathbf{z}_j - \hat{\mathbf{z}}_i), & \text{falls } c_{ij} \le \tau,\\
\infty, & \text{sonst}.
\end{cases}
\]
Dadurch werden nur Messungen berücksichtigt, die statistisch konsistent zum jeweiligen Track sind.

Im Anschluss bestimmt der Ungarische Algorithmus \cite{kuhn1955} eine optimale 1-zu-1-Zuordnung, welche die Summe der Kosten minimiert:
\[
\min_{\pi} \sum_{i} c_{i,\pi(i)}, \qquad \pi \text{ ist eine Permutation auf } \{1,\dots,N_\text{Track}\}.
\]
Das Verfahren garantiert eine eindeutige Zuordnung, bei der jeder Track höchstens eine Detektion erhält und jede Detektion höchstens einem Track zugewiesen wird. Tracks ohne passende Messung werden nur durch die Prädiktion weitergeführt, während nicht zugeordnete gültige Detektionen als neue Objekte initialisiert werden können. Durch diese Formulierung wird verhindert, dass mehrere Objekte derselben Detektion zugeordnet werden oder sich Objektidentitäten überschneiden – ein bekanntes Problem bei rein lokalem Nearest-Neighbor-Matching (\cite{bewley2016sort}).

\section{Implementierung}
\label{sec:implementierung_tracking}
Die Umsetzung erfolgt als \ac{ROS}~2\,--\,Knoten \texttt{tracking\_node}. Eingänge sind \texttt{/detections\_raw} (\texttt{vision\_msgs/Detection3DArray}) aus der Clusterstufe, Ausgaben \texttt{/tracks\_raw} (Detektionen mit stabilisierten Zentren/Größen/IDs) sowie \texttt{/tracks\_markers} für \ac{RViz}. Sensorpfade nutzen \emph{SensorDataQoS} (BestEffort/KeepLast), Ergebnis\,--\,Topics \emph{reliable}. 

\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|p{8.5cm}|}
    \hline
    \textbf{Parameter} & \textbf{Typ} & \textbf{Bedeutung} \\ \hline
    \texttt{gate\_dist\_max} & [m] & euklidische Obergrenze des Gates (z.\,B. 3\,--\,4\,m) \\ \hline
    \texttt{min\_hits} & [Frame] & Bestätigungsschwelle (2\,--\,3) \\ \hline
    \texttt{max\_missed} & [Frame] & Löschschwelle ohne Zuordnung (6\,--\,10) \\ \hline
    \texttt{max\_size\_L,W,H} & [m] & Plausibilitätsobergrenzen (z.\,B. L\,\(\leq\,8\), W\,\(\leq\,5\), H\,\(\leq\,4.2\)) \\ \hline
  \end{tabular}
    \caption{Wesentliche Parameter des Trackers .}
    \label{tab:tracking_params}
  \end{table}

Die Wahl der Startwerte folgt gängigen Empfehlungen aus der Tracking-Literatur. Für das \texttt{gate\_dist\_max} wird ein euklidischer Gate-Radius von rund 4~m genutzt, was bei typischen Positionsvarianzen ($\sigma \approx 1.5$--$2$~m) in etwa einem 95\,\%-Konfidenzintervall des Mahalanobis-Gates entspricht und damit den in \textcite{bar2004tracking,blackman1999design} empfohlenen Kompromiss zwischen Fehlassoziationen und Wiedererkennung abdriftender Tracks abbildet. \texttt{max\_missed}\,$=10$ erlaubt es einem Track, bei der hier verwendeten Sensorrate von 10~Hz etwa eine Sekunde ohne Messung zu überbrücken (z.\,B. bei kurzzeitigen Abschattungen), liegt damit im oberen Bereich der in \textcite{bewley2016sort} vorgeschlagenen Time-to-Live von 1~s, führt in Tests aber zu deutlich stabileren Trajektorien in dichtem Verkehr. \texttt{min\_hits}\,$=2$ stellt sicher, dass neue Spuren mindestens zweimal bestätigt werden müssen, bevor sie als valide Track-IDs ausgegeben werden; dies folgt der Heuristik von \textcite{bewley2016sort}, wonach eine niedrige Bestätigungsschwelle Reaktionsgeschwindigkeit wahrt, aber Einzelmessungen und Sensorartefakte unterdrückt.

Die in dieser Arbeit betrachteten Objektklassen unterscheiden sich deutlich in ihren
typischen geometrischen Abmessungen. Für die parametrische Auslegung der
Cluster-Extraktion sowie für die Bewertung der Detektionsgüte ist es daher
unerlässlich, realistische Größenbereiche zu berücksichtigen. 
Tabelle~\ref{tab:objektgroessen} zeigt die in der Literatur üblichen Längen-, Breiten-
und Höhenintervalle der relevanten Objektkategorien, basierend auf den im 
\textit{nuScenes}-Datensatz dokumentierten Durchschnittswerten.

\begin{table}[H]
\centering
\begin{tabular}{l|c|c|c}
\hline
\textbf{Objektklasse} & \textbf{Länge [m]} & \textbf{Breite [m]} & \textbf{Höhe [m]} \\
\hline
Mensch            & 0.4 -- 0.8 & 0.3 -- 0.6 & 1.5 -- 2.0 \\
Fahrradfahrer     & 1.2 -- 1.8 & 0.5 -- 0.8 & 1.4 -- 2.0 \\
Pkw               & 3.8 -- 5.0 & 1.6 -- 2.0 & 1.4 -- 1.8 \\
Lkw               & 6.0 -- 12.0 & 2.3 -- 2.6 & 3.0 -- 4.0 \\
\hline
\end{tabular}
\caption{Typische Abmessungen der Objektklassen. Wertebereich basierend auf dem \textit{nuScenes} Dataset \cite{nuscenes}.}
\label{tab:objektgroessen}
\end{table}

Auf Basis dieser Literaturwerte wurden für die vorliegende Arbeit praxisnahe
Abmessungsbereiche definiert, die direkt für die Plausibilisierung der extrahierten
Cluster sowie für die heuristische Klassenzuordnung eingesetzt wurden.
Diese Bereiche berücksichtigen ausschließlich den Objektumfang oberhalb der
segmentierten Bodenfläche und haben sich in den Versuchen als robuste Grenzen
für typische Straßenszenen erwiesen.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|c|c|c|}
    \hline
    \textbf{Objektklasse} & \textbf{Länge [m]} & \textbf{Breite [m]} & \textbf{Höhe [m]} \\ \hline
    Mensch         & 0.3 -- 1.6  & 0.4 -- 1.0  & 1.0 -- 2.0 \\ \hline
    Fahrradfahrer  & 0.6 -- 2.0  & 0.6 -- 1.5  & 0.4 -- 1.6 \\ \hline
    Pkw            & 2.0 -- 4.5  & 1.4 -- 3.0  & 0.6 -- 2.1 \\ \hline
    Lkw            & 4.5 -- 8.0  & 2.0 -- 5.0  & 2.0 -- 4.2 \\ \hline
  \end{tabular}
  \caption{Abmessungsbereiche der Zielklassen nach Bodenentfernung (Plausibilisierung und Heuristik).}
  \label{tab:class_dims}
\end{table}


Boxen, deren Abmessungen über globalen Obergrenzen liegen (\enquote{größer als Lkw}), werden vor Anlage/Aktualisierung verworfen; dies reduziert Fehlassoziationen und senkt Rechenlast. Außerdem werden nur vordefinierte Klassen (Person, Fahrradfahrer, Pkw, Lkw) getrackt; \enquote{Unbekannt} kann zur Unterdrückung von Artefakten verworfen werden.

Tracks werden als \texttt{vision\_msgs/Detection3D} ausgegeben (Zentrum, Größe, Orientierung, ID über Label/Namespace). Die Orientierung wird bei AABB aus der Detektion (z.\,B. \ac{BEV} oder Cluster) übernommen, nicht aus dem \ac{KF} geschätzt. Für Debug wird ein \texttt{MarkerArray} (/tracks\_markers)publiziert (Box und Textlabel) für die Visualisierung in Rviz. Abbildung~\ref{fig:tracking_flow} skizziert den Ablauf.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[
      box/.style={
        rectangle,
        rounded corners,
        draw=black,
        thick,
        minimum width=4.0cm,
        minimum height=1.2cm,
        align=center
      },
      node distance=1.6cm
    ]

    % Nodes (vertical chain)
    \node[box] (det) {Detektionen};
    \node[box, below of=det] (gate) {Gating /\\ Zuordnung};
    \node[box, below of=gate] (kf) {\ac{KF}-Update};
    \node[box, below of=kf] (manage) {Track-\\Management};
    \node[box, below of=manage] (out) {Ausgabe};

    % Arrows
    \draw[->, thick] (det) -- (gate);
    \draw[->, thick] (gate) -- (kf);
    \draw[->, thick] (kf) -- (manage);
    \draw[->, thick] (manage) -- (out);

  \end{tikzpicture}
  \caption{Ablauf der Objektverfolgung.}
  \label{fig:tracking_flow}
\end{figure}

\section{Test und Ergebnis}
\label{sec:ergebnis_tracking}

\texttt{ids\_probe.py} beobachtet das Marker-Topic \texttt{tracks\_markers} und führt ein Lebensdauerkonto pro Track-ID. Für jede Spur werden Geburt und Abschluss (bei Leerlauf länger als \texttt{idle\_timeout}) gezählt, aktive sowie abgeschlossene mittlere Lebensdauern berechnet und die Geburts- bzw. Abschlussrate pro Minute ausgegeben. Der Skript läuft als eigener \ac{ROS}~2-Node, greift auf die bestehenden Topics zu und liefert so eine objektive Einschätzung der ID-Stabilität. Alle zugehörigen Auswerteskripte befinden sich zentral im digitalen Anhang (Abschnitt~\ref{anhang:skripte}).

Die Messergebnisse der Verifikation sind in Abschnitt~\ref{label:gerade_strecke} zusammengestellt und belegen eine stabile Zuordnung auch bei kurzzeitigen Okklusionen.

\chapter{Integration in der GUI}
\label{chap:integration_gui}

Dieses Kapitel beschreibt die Einbindung der \ac{LiDAR}-Verarbeitungskette in die bestehende IFZN-GUI. Es fasst die vorhandenen Funktionen zusammen, leitet die notwendigen Erweiterungen für den Ouster~\ac{OS1} ab, dokumentiert Anpassungen an Startskripten und Namespaces und diskutiert beobachtete Einschränkungen sowie Verbesserungsvorschläge für die zukünftige Bedienlogik.

\section{Anforderungen: Bestehende Funktionen und notwendige Erweiterungen}

Die bestehende IFZN-GUI folgt der modularen und erweiterbaren Architektur nach Wendel \cite{Wendel2025} und bietet bereits eine stabile Bedienoberfläche für verschiedene Sensorsysteme. Zu den vorhandenen Funktionen gehören:

\begin{itemize}
    \item eine stabile GUI-Struktur zur Steuerung und Überwachung von Sensoren,
    \item die Nutzung zentraler Bash-Skripte und Launch-Files zum Starten der Sensorik,
    \item die Unterstützung mehrerer Betriebsmodi (Embedded PC + Laptop, Laptop mit Sensoren, Laptop),
    \item grundlegende Funktionen wie Messdatenaufzeichnung, Wiedergabe von \ac{ROS}~2-Bag-Dateien und Visualisierung in \ac{RViz},
    \item eine automatisierte Startlogik über MATLAB und \texttt{tmux} für bestehende Sensorketten.
\end{itemize}

Diese Infrastruktur bildet die Grundlage für die Integration des Ouster~\ac{OS1} und kann ohne Bruch der bisherigen Bedienphilosophie erweitert werden. Für eine vollständige Einbindung sind jedoch noch zusätzliche Bausteine nötig, die im ursprünglichen Umfang nicht vorgesehen waren:

\begin{enumerate}
    \item \textbf{Einbindung des Ouster-Treibers:}  
    Ergänzung und Anpassung des \texttt{sensor.launch.xml}, um den Ouster-Treiber in die bestehende Startlogik einzubetten.

    \item \textbf{Integration der Nodes der Verarbeitungskette:}  
    Die im Rahmen dieser Arbeit entwickelte Verarbeitungskette erfordert die Einbindung folgender \ac{ROS}~2-Nodes:
    \begin{itemize}
        \item \texttt{crop\_box\_node},
        \item \texttt{voxel\_filter\_node},
        \item \texttt{ground\_segmentation\_node},
        \item \texttt{cluster\_extraction\_node},
        \item \texttt{tracking\_node}.
    \end{itemize}

    Diese Nodes müssen im \texttt{sensor.launch.xml}, in den Aufzeichnungsskripten (\texttt{run\_aufzeichnung}) sowie in der MATLAB-App (\texttt{\ac{LiDAR}\_Embedded\_PC.mlapp}) hinterlegt werden.

    \item \textbf{Erweiterung der Betriebsmodi:}  
    Die GUI muss die neue Ouster-basierte Verarbeitungskette in allen drei vorhandenen Modi starten können.

    \item \textbf{Automatisches Laden aller Abhängigkeiten:}  
    Alle nötigen Umgebungsvariablen (\texttt{setup.bash}, Pfade, Domain-ID) müssen für die erweiterten Startabläufe hinterlegt werden.
\end{enumerate}

Damit lässt sich die neue Sensorkette ohne Medienbruch in die vorhandene GUI einbetten und steht anschließend in allen Betriebsmodi bereit.

\section{Umsetzung}

Die Integration des neuen Verarbeitungspakets \texttt{ouster\_cpp} in die bestehende IFZN-GUI vereinte mehrere gezielte Anpassungen an Dateistruktur, Workspaces und Startmechanismen der \ac{ROS}\,2-Nodes. Ziel war es, die gesamte Verarbeitungskette -- bestehend aus den Modulen \emph{CropBox}, \emph{VoxelGrid}, \emph{\ac{RANSAC}-Bodensegmentierung}, \emph{Cluster-Extraktion} und \emph{Tracking} -- in allen vorgesehenen Betriebsmodi nutzbar zu machen:
(1) Laptop,  
(2) Laptop mit Sensor,  
(3) Embedded-PC + Laptop.  
Im Folgenden werden die vorgenommenen Schritte beschrieben.

\subsection*{Vereinheitlichung der Workspace-Struktur}

Zu Beginn befand sich das neue Paket \texttt{ouster\_cpp} in einem separaten Workspace, der im Rahmen der GUI jedoch nicht berücksichtigt wurde. Da die IFZN-GUI ausschließlich den zentralen Workspace unter  
\texttt{Documents/Sensorik\_Astra/Ouster\_OS1/ros2\_ws}  
synchronisiert und aus diesem sowohl lokal als auch auf dem Embedded-PC alle relevanten Nodes startet, wurde das Paket vollständig in diesen Haupt-Workspace überführt.

Dies gewährleistet:
\begin{itemize}
  \item einen konsistenten Build-Prozess über alle Modi hinweg,
  \item die automatische Verfügbarkeit des Pakets auf dem Embedded-PC durch die GUI-Synchronisation,
  \item eine einheitliche Pfadstruktur für alle Startskripte.
\end{itemize}

\subsection*{Erweiterung des bestehenden Launch-Systems}

Damit die vollständige Verarbeitungskette gemeinsam mit dem Ouster-Treiber (\texttt{os\_driver}) gestartet werden kann, wurde die zentrale Launch-Datei \texttt{sensor.launch.xml} des Pakets \texttt{ouster\_ros} erweitert.  
Innerhalb des bestehenden Ouster-Namespaces wurde ein zusätzlicher Unter-Namespace \texttt{bench} angelegt und dort alle neuen Executables
\texttt{crop\_box\_node}, \texttt{voxel\_filter\_node}, \texttt{ransac\_ground\_node}, \texttt{cluster\_extraction\_node}, \texttt{tracking\_node}
als eigenständige \texttt{<node>}-Blöcke ergänzt.

Damit werden sämtliche Verarbeitungsschritte automatisch aktiviert, sobald die GUI den Ouster-Sensor startet -- unabhängig vom gewählten Betriebsmodus.

\subsection*{Anpassungen an den GUI-Startskripten}

Die GUI nutzt mehrere tmux-basierte Startskripte, welche die benötigten Nodes abhängig vom Betriebsmodus ausführen.  
Um die Funktionsfähigkeit der erweiterten Verarbeitungskette sicherzustellen, wurden folgende Anpassungen vorgenommen:

\begin{itemize}
  \item \textbf{Laptop- und Laptop-mit-Sensor-Modus:}  
  Das Skript \texttt{run\_ouster\_os1.bash} wurde so belassen, dass es den zentralen Workspace kompiliert und anschließend die erweiterte \texttt{sensor.launch.xml} ausführt. Somit laufen der Ouster-Treiber und alle \texttt{ouster\_cpp}-Nodes lokal.

  \item \textbf{Embedded-PC + Laptop:}  
  Da die GUI beim Aufbau der Verbindung den zentralen Workspace automatisch auf den Embedded-PC überträgt, stehen dort alle Nodes ohne weitere Änderungen zur Verfügung. Beim Start werden dieselben Launch-Dateien via~SSH ausgeführt, sodass die identische Verarbeitungskette auf dem Embedded-PC ausgeführt wird.

  \item \textbf{Aufzeichnungsmodus:}  
  Auch im Skript \texttt{run\_aufzeichnungen.sh} wird der zentrale Workspace kompiliert und anschließend ein Launch-File (benchmark.launch.py) ausgeführt. Durch die Integration von \texttt{ouster\_cpp} in den Haupt-Workspace stehen alle Nodes ebenfalls für Aufzeichnungen bereit, ohne dass die Skripte separat erweitert werden mussten.
\end{itemize}

\subsection*{Sicherstellung der \ac{ROS}\,2-Kompatibilität}

Damit die GUI alle relevanten Nodes zuverlässig erkennen und überwachen kann, wurden folgende strukturelle Anforderungen erfüllt:

\begin{itemize}
  \item alle Executables sind über \texttt{install(TARGETS ...)} korrekt in den \texttt{install/}-Baum eingebunden,
  \item sämtliche benötigten Abhängigkeiten (\texttt{rclcpp}, \texttt{sensor\_msgs}, \texttt{pcl\_conversions}, \texttt{vision\_msgs}, \texttt{Eigen3} usw.) sind im \texttt{package.xml} hinterlegt,
  \item konsistente Namespaces (\texttt{/ouster/bench/...}) ermöglichen eine eindeutige Identifikation der Nodes mittels \texttt{ros2 node list}.
\end{itemize}

\section{Ergebnis und mögliche Ursachen der Einschränkungen}

Die Integration des Verarbeitungspakets \texttt{ouster\_cpp} konnte für die Betriebsmodi \emph{Laptop} sowie \emph{Laptop mit Sensor} erfolgreich umgesetzt werden. In diesen Fällen wurden sämtliche Nodes korrekt kompiliert, durch die erweiterten Launch-Dateien gestartet und von der GUI erkannt, sodass die vollständige Verarbeitungskette funktionsfähig war.

Im Modus \emph{Embedded-PC + Laptop} zeigte sich jedoch eine Einschränkung: Obwohl der zentrale Workspace korrekt auf den Embedded-PC übertragen wurde und die Nodes technisch startfähig waren, wurde der Ouster-Sensor von der GUI nicht als vollständig aktiviert erkannt. Dadurch konnte der Betriebsmodus nicht in den regulären Zustand \enquote{aktiv} übergehen.

Eine mögliche Ursache für dieses Verhalten könnte in der internen Validierungslogik der GUI liegen. In der Klasse \texttt{\ac{LiDAR}\_Embedded\_PC} wird für jeden Sensor eine Liste erwarteter Node-Namen geführt, beispielsweise
\begin{quote}
\texttt{ousteros1nodes = \{'/ouster/os\_driver'\}}.
\end{quote}
Diese Liste dient dazu, sicherzustellen, dass der Sensortreiber korrekt läuft. Für die erweiterten Verarbeitungsschritte des Pakets \texttt{ouster\_cpp} wurden zusätzliche Nodes wie
\texttt{/ouster/bench/crop\_box\_node\_cpp},
\texttt{/ouster/bench/voxel\_filter\_node\_cpp},
\texttt{/ouster/bench/ransac\_ground\_node\_cpp},
\texttt{/ouster/bench/cluster\_extraction\_node\_cpp}
und
\texttt{/ouster/bench/tracking\_node\_cpp}
ergänzt.

Es ist denkbar, dass die Einbindung dieser zusätzlichen Nodes in die GUI-interne Prüfliste dazu führte, dass der Embedded-PC-Modus die Aktivierung des Sensors nicht mehr bestätigte. Mögliche Gründe hierfür könnten sein:
\begin{itemize}
    \item zeitlich verzögertes Starten einzelner Nodes über die SSH-Verbindung,
    \item unterschiedliche oder verschachtelte Namespaces, welche von der GUI nicht erwartungsgemäß ausgewertet werden,
    \item oder ein generelles Missverhältnis zwischen der ursprünglichen GUI-Logik (ausgelegt auf genau eine Node pro Sensor) und einer erweiterten Verarbeitungskette mit mehreren abhängigen Nodes.
\end{itemize}

Es handelt sich dabei um Hypothesen, die das beobachtete Verhalten plausibel erklären könnten; sie konnten im Rahmen der Arbeit jedoch nicht abschließend verifiziert werden. Entscheidend ist, dass die technische Integration des Pakets \texttt{ouster\_cpp} selbst erfolgreich war und die Einschränkungen ausschließlich die GUI-Validierung im Modus \emph{Embedded-PC + Laptop} betreffen.

Eine mögliche Weiterentwicklung der GUI könnte darin bestehen, die Aktivitätsprüfung flexibler zu gestalten oder lediglich die Minimalmenge notwendiger Nodes (z.\,B.\ den Treiber \texttt{os\_driver}) zu überprüfen, um komplexere Verarbeitungsketten künftig vollständig zu unterstützen.

\chapter{Test und Bewertung}
\label{chap:test_und_bewertung}

Dieses Kapitel bündelt die Testergebnisse der \ac{ROS}~2-basierten Verarbeitungskette und ordnet sie den zuvor definierten Anforderungen zu. Zunächst werden Versuchsaufbau und Parametervariationen beschrieben, anschließend folgen die Messungen im Fahrbetrieb. Damit entstehen klare Übergänge von der Laborparametrisierung zur Straßenerprobung. Die Verarbeitungskette ist vollständig in \ac{ROS}~2 umgesetzt, umfasst die in Abbildung~\ref{fig:ros2-verarbeitungskette-alltopics} dargestellten Module und wird über das Launch-File \textit{benchmark.launch.py} im gemeinsamen Namensraum \textit{/bench} gestartet.

\section{Parameterstudie}
\label{sec:parameterstudie}

Die gewählte Detektionskette reagiert empfindlich auf Parameter, die Segmentierung, Clusterbildung und Klassentrennung prägen. Die Parameterstudie untersucht deshalb systematisch die Kombinationen aus \texttt{voxel\_size}, \texttt{distance\_threshold} und \texttt{cluster\_tolerance}. Pro Durchgang wird jeweils nur einer dieser Werte angepasst und die übrige Konfiguration unverändert belassen, damit der Einfluss jeder Stellgröße klar nachvollziehbar bleibt.

Die Datensätze wurden bei 9\,°C und bedecktem Himmel aufgezeichnet, womit witterungsbedingte Ausreißer minimiert sind. Die Verarbeitung erfolgte auf einem MEDION 15~E1 Laptop mit AMD Ryzen~5~7430U (6~Kerne/12~Threads) und 32~GB RAM.

Der Fokus liegt bewusst auf den drei genannten Parametern, weil sie die größten Hebel entlang der Kette darstellen: \texttt{voxel\_size} steuert die Ausdünnung der Punktwolke (Rauschunterdrückung vs. Detailtreue), \texttt{distance_threshold} definiert die Inlier-Schwelle der Bodensegmentierung und \texttt{cluster_tolerance} bestimmt die räumliche Nachbarschaft beim Clustering. Weitere Einstellungen bleiben fix, da sie entweder durch die Versuchskonfiguration vorgegeben sind (\texttt{min_bound}/\texttt{max_bound} der CropBox), bereits konservative Robustheitsreserven bieten (\texttt{max_iterations} der \ac{RANSAC}-Suche), nur sekundären Einfluss auf die Klassentrennung haben (\texttt{min_cluster_size}, \texttt{max_cluster_size}, \texttt{max_clusters}) oder die Vergleichbarkeit der Wiederholungsmessungen beeinträchtigen würden (\texttt{gate_dist_max}, \texttt{max_missed}, \texttt{min_hits}). Dadurch bleibt der Suchraum handhabbar, ohne zentrale Qualitätstreiber zu vernachlässigen.

\subsection{Untersuchungsaufbau}
Die Messungen wurden in drei realen Szenarien durchgeführt:

\begin{itemize}
    \item \textbf{S1 – Geparkte Fahrzeuge auf einer ebenen Fahrbahn:} statische Umgebung mit eng stehenden PKW, geringer Verkehr, spiegelnde Oberflächen, hohe Gefahr von Übersegmentierung.
    \item \textbf{S2 – Kreuzung:} Objekte in dichter Nachbarschaft, teilweise verdeckt, dynamische Szene; Ziel: Minimierung von Falsch-Detektionen und stabiler Klassentrennung.
    \item \textbf{S3 – Unebene Fahrbahn:} geneigte Fläche mit parkenden Fahrzeugen; bestätigt Robustheit der Bodensegmentierung und der Clusterbildung bei variierender Höhengeometrie.
\end{itemize}

\begin{table}[H]
  \centering
  \caption{Übersicht der verwendeten Messaufzeichnungen}
  \begin{tabular}{|c|l|l|l|}
    \hline
    \textbf{Szenario} & \textbf{Beschreibung} & \textbf{Messaufzeichnung} & \textbf{Zeitraffer} \\ \hline
    S1 & Geparkte Autos & 2025-10-15-VK-OL-006 & Start: 5s; run 10s \\ \hline
    S2 & Kreuzung & 2025-10-15-VK-OL-007 & Start: 20s; run 10s \\ \hline
    S3 & Unebene Fläche & 2025-10-15-VK-OL-004 & Start: 145s; run 10s \\ \hline
  \end{tabular}
\end{table}

Als Bodenwahrheit (``Realwert'') dienten zum einen Kameraaufnahmen im Vorderbereich, zum anderen das 360° \ac{LiDAR}-Datenbild für rückwärtige Objekte. Bewertet wurden jeweils die Klassen \textit{PKW}, \textit{Mensch}, \textit{Fahrradfahrer} und \textit{LKW} sowie die Gesamtdetektionen, sodass sowohl Über- als auch Untersegmentierungen sichtbar werden.

Die in Tabelle~\ref{tab:startwerte_parameter} aufgeführten Werte blieben während der Variation von \textit{voxel\_size} unverändert und dienen als Referenz für die folgenden Auswertungen. Erst im Anschluss wurden \textit{distance\_threshold} und \textit{cluster\_tolerance} analog variiert, immer ausgehend von den ausgewählten Parameter.

\begin{table}[H]
  \centering
  \caption{Startwerte der Parameter vor der Parametrisierung}
  \label{tab:startwerte_parameter}
  \begin{tabular}{lll}
    \toprule
    \textbf{Node} & \textbf{Parameter} & \textbf{Wert} \\
    \midrule

    \texttt{crop\_box\_node\_cpp} \\
      & \texttt{min\_bound}  & \([-10.0,\ -6.0,\ -3.0]\) \\
      & \texttt{max\_bound}  & \([10.0,\ 6.0,\ 5.0]\) \\
    \midrule

    \texttt{ransac\_ground\_node\_cpp} \\
      & \texttt{distance\_threshold} & 0.15 \\
      & \texttt{max\_iterations}    & 1000 \\
    \midrule

    \texttt{cluster\_extraction\_node\_cpp} \\
      & \texttt{cluster\_tolerance} & 0.50 \\
      & \texttt{min\_cluster\_size} & 40 \\
      & \texttt{max\_cluster\_size} & 8000 \\
      & \texttt{max\_clusters}      & 200 \\
      & \texttt{bbox\_type}         & aabb \\
    \midrule

    \texttt{tracking\_node\_cpp} \\
      & \texttt{gate\_dist\_max} & 4.0 \\
      & \texttt{max\_missed}    & 10 \\
      & \texttt{min\_hits}      & 2 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{S1: Geparkte Fahrzeuge auf einer ebenen Fahrbahn}

\begin{table}[H]
\centering
\begin{tabular}{c|c|c|c|c|c}
\hline
\textbf{voxel\_size} & \textbf{PKW} & \textbf{Mensch} & \textbf{Fahrradfahrer} & \textbf{LKW} & \textbf{Gesamt} \\
\hline
0.10 & 12 & 3 & 3 & 0 & 18 \\
0.15 & 15 & 1 & 3 & 2 & 21 \\
0.20 & \textbf{15} & \textbf{0} & \textbf{1} & \textbf{1} & \textbf{17} \\
0.25 & 15 & 0 & 1 & 2 & 18 \\
\hline
Real & 15 & 0 & 1 & 3 & 19 \\
\hline
\end{tabular}
\caption{Detektionen pro Klasse für unterschiedliche voxel\_size-Werte (Geparkte Autos).}
\label{tab:geparkt_voxel}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{c|c|c|c|c|c}
\hline
\textbf{distance\_threshold} & \textbf{PKW} & \textbf{Mensch} & \textbf{Fahrradfahrer} & \textbf{LKW} & \textbf{Gesamt} \\
\hline
0.10 & 16 & 0 & 1 & 1 & 18 \\
0.15 & 15 & 0 & 1 & 1 & 17 \\
0.20 & \textbf{17} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{20} \\
0.25 & 16 & 0 & 1 & 1 & 18 \\
\hline
Real & 15 & 0 & 1 & 3 & 19 \\
\hline
\end{tabular}
\caption{Detektionen pro Klasse für unterschiedliche distance\_threshold-Werte (Geparkte Autos, voxel\_size = 0.15\,m).}
\label{tab:geparkt_distance}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{c|c|c|c|c|c}
\hline
\textbf{cluster\_tolerance} & \textbf{PKW} & \textbf{Mensch} & \textbf{Fahrradfahrer} & \textbf{LKW} & \textbf{Gesamt} \\
\hline
0.30 & 19 & 0 & 4 & 1 & 24 \\
0.50 & \textbf{17} & \textbf{0} & \textbf{1} & \textbf{1} & \textbf{19} \\
0.70 & 8  & 0 & 1 & 2 & 11 \\
0.90 & 7  & 0 & 1 & 2 & 10 \\
\hline
Real & 15 & 0 & 1 & 3 & 19 \\
\hline
\end{tabular}
\caption{Detektionen pro Klasse für unterschiedliche Werte der cluster\_tolerance (Geparkte Autos, voxel\_size = 0{,}15\,m, distance\_threshold = 0{,}20\,m).}
\label{tab:geparkt_cluster_tolerance}
\end{table}

Die Auswertungen für \textbf{S1} zeigen deutlich, wie sensibel die Verarbeitungskette auf Übersegmentierung reagiert. Eine Vergrößerung der \texttt{voxel\_size} von \SI{0.10}{m} auf \SI{0.20}{m} reduziert die Fehlklassifikationen von Menschen und Fahrradfahrern vollständig und nähert sich mit insgesamt 17 Detektionen dem Realwert von 19 Objekten an. Für die Bodensegmentierung verhindern Werte ab \SI{0.15}{m}, dass Fahrzeugdachpunkte als Boden eingestuft werden; bei \SI{0.20}{m} steigt die Gesamtdetektion zwar auf 20, allerdings mit einem zusätzlichen LKW-Falschpositiv. Am deutlichsten wird der Einfluss der \texttt{cluster\_tolerance}: Zu kleine Werte (\SI{0.30}{m}) führen zu einer Übersegmentierung von PKW (24 Gesamt\-detektionen), während zu große Werte (\SI{0.70}{m} und \SI{0.90}{m}) angrenzende Fahrzeuge zusammenziehen (nur 11 bzw. 10 Detektionen). Die Einstellung \SI{0.50}{m} liefert mit 19 Detektionen eine nahezu perfekte Übereinstimmung mit der Bodenwahrheit und stellt damit den besten Kompromiss zwischen Trennschärfe und Zusammenfassung dar. Insgesamt ergibt sich für dieses Szenario ein Optimum bei moderaten Einstellungen (\texttt{voxel\_size} \(\approx\) \SI{0.20}{m}, \texttt{distance\_threshold} \(=\) \SI{0.15}{m}, \texttt{cluster\_tolerance} \(=\) \SI{0.50}{m}).

\subsection{S2 – Kreuzung}

\begin{table}[H]
  \centering
  \caption{Anzahl Detektionen pro Klasse für unterschiedliche Voxelgrößen (Kreuzung).}
  \begin{tabular}{|l|c|c|c|c|c|}
    \hline
    \textbf{voxel\_size (m)} & \textbf{PKW} & \textbf{Mensch} & \textbf{Fahrradfahrer} & \textbf{LKW} & \textbf{Gesamt} \\
    \hline
    0.10 & 11 & 13 & 2 & 1 & 27 \\
    0.15 & 13 & 12 & 1 & 1 & 27 \\
    0.20 & 13 &  2 & 0 & 1 & 16 \\
    0.25 & 14 &  0 & 0 & 1 & 15 \\
    \hline
    Real &  9 &  3 & 1 & 1 & 14 \\
    \hline
  \end{tabular}
  \label{tab:kreuzung_voxel_size}
\end{table}

\begin{table}[H]
  \centering
  \caption{Anzahl Detektionen pro Klasse für unterschiedliche \texttt{distance\_threshold}-Werte (Kreuzung, voxel\_size = 0.15\,m).}
  \begin{tabular}{|l|c|c|c|c|c|}
    \hline
    \textbf{distance\_threshold (m)} & \textbf{PKW} & \textbf{Mensch} & \textbf{Fahrradfahrer} & \textbf{LKW} & \textbf{Gesamt} \\
    \hline
    0.10 & 15 & 12 & 1 & 1 & 29 \\
    0.15 & 14 & 12 & 1 & 1 & 28 \\
    0.20 & 14 &  9 & 1 & 1 & 25 \\
    0.25 & 14 &  9 & 1 & 1 & 25 \\
    \hline
    Real &  9 &  3 & 1 & 1 & 14 \\
    \hline
  \end{tabular}
  \label{tab:kreuzung_distance_threshold}
\end{table}

\begin{table}[H]
  \centering
  \caption{Anzahl Detektionen pro Klasse für unterschiedliche \texttt{cluster\_tolerance}-Werte (Kreuzung, voxel\_size = 0.15\,m, distance\_threshold = 0.20\,m).}
  \begin{tabular}{|l|c|c|c|c|c|}
    \hline
    \textbf{cluster\_tolerance (m)} & \textbf{PKW} & \textbf{Mensch} & \textbf{Fahrradfahrer} & \textbf{LKW} & \textbf{Gesamt} \\
    \hline
    0.30 &  8 &  8 & 4 & 1 & 21 \\
    0.50 & 14 &  9 & 1 & 1 & 25 \\
    0.70 & 17 &  8 & 1 & 1 & 27 \\
    0.90 & 16 &  8 & 1 & 1 & 26 \\
    \hline
    Real &  9 &  3 & 1 & 1 & 14 \\
    \hline
  \end{tabular}
  \label{tab:kreuzung_cluster_tolerance}
\end{table}

Die Ergebnisse der Szene \textbf{S2~–~Kreuzung} zeigen, dass eine dichte und dynamische Umgebung die Wirkung jedes Parameters verstärkt. Eine moderate Reduzierung der \textit{voxel\_size} (0,15--0,20\,m) senkt die Falschmeldungen in der Klasse \textit{Mensch} (von 13 auf 2~Detektionen), führt aber gleichzeitig zu einer leichten Untererfassung der tatsächlichen Fußgänger (3 erwartete Ziele) und weiterer Klassen \big(\autoref{tab:kreuzung_voxel_size}\big). Ein \textit{distance\_threshold} von 0,15--0,20\,m verhindert eine Explosion von Fehl-Clustern, ohne große Objekte zu verlieren; geringere Werte (0,10\,m) bewirken eine Übersegmentierung und fast doppelt so viele Detektionen wie real vorhanden \big(29 statt 14, \autoref{tab:kreuzung_distance_threshold}\big). Die Variation der \textit{cluster\_tolerance} belegt, dass ein Umfeld von 0,50--0,70\,m einen guten Kompromiss bietet: darunter (0,30\,m) werden Objekte übersegmentiert (21~Detektionen, davon 4~falsche Radfahrer), darüber (0,90\,m) beginnen sie zu fusionieren (26~Detektionen, 7 über dem Realwert), siehe \autoref{tab:kreuzung_cluster_tolerance}. In Summe bleiben die gewählten Einsatzwerte (\textit{voxel\_size} = 0,15--0,20\,m, \textit{distance\_threshold} = 0,15\,m, \textit{cluster\_tolerance} = 0,50\,m) die robusteste Wahl, um Fehlalarme zu begrenzen und gleichzeitig die wesentlichen Ziele in einer stark frequentierten Kreuzung zu erhalten.

\subsection{S3 – Unebene Fahrbahn}

\begin{table}[H]
\centering
\begin{tabular}{c|c|c|c|c|c}
\hline
\textbf{voxel\_size} & \textbf{PKW} & \textbf{Mensch} & \textbf{Fahrradfahrer} & \textbf{LKW} & \textbf{Gesamt} \\
\hline
0.10 & 15 & 7 & 3 & 0 & 25 \\
0.15 & 15 & 7 & 2 & 1 & 25 \\
0.20 & 15 & 1 & 2 & 1 & 19 \\
0.25 & 16 & 0 & 2 & 1 & 19 \\
\hline
Real & 13 & 0 & 1 & 5 & 19 \\
\hline
\end{tabular}
\caption{Anzahl Detektionen pro Klasse für verschiedene voxel\_size (Unebene Fläche).}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{c|c|c|c|c|c}
\hline
\textbf{distance\_threshold} & \textbf{PKW} & \textbf{Mensch} & \textbf{Fahrradfahrer} & \textbf{LKW} & \textbf{Gesamt} \\
\hline
0.10 & 18 & 1 & 2 & 1 & 22 \\
0.15 & 15 & 1 & 2 & 1 & 19 \\
0.20 & 14 & 1 & 3 & 1 & 19 \\
0.25 & 14 & 1 & 3 & 1 & 19 \\
\hline
Real & 13 & 0 & 1 & 5 & 19 \\
\hline
\end{tabular}
\caption{Anzahl Detektionen pro Klasse für verschiedene distance\_threshold (Unebene Fläche).}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{c|c|c|c|c|c}
\hline
\textbf{cluster\_tolerance} & \textbf{PKW} & \textbf{Mensch} & \textbf{Fahrradfahrer} & \textbf{LKW} & \textbf{Gesamt} \\
\hline
0.30 & 16 & 3 & 5 & 1 & 25 \\
0.50 & 16 & 1 & 2 & 1 & 20 \\
0.70 & 17 & 1 & 1 & 2 & 21 \\
0.90 & 13 & 1 & 0 & 2 & 16 \\
\hline
Real & 13 & 0 & 1 & 5 & 19 \\
\hline
\end{tabular}
\caption{Anzahl Detektionen pro Klasse für verschiedene cluster\_tolerance (Unebene Fläche).}
\end{table}

Im Vergleich zu den ebenen Szenarien fällt auf, dass die geneigte Fahrbahn deutlich mehr Fehlalarme bei kleinen Objekten erzeugt. Schon bei \(\textit{voxel\_size} = 0{,}10\,\text{m}\) entstehen insgesamt 25 Detektionen, obwohl das Realbild nur 19 Objekte enthält; die Höhenvarianz verstärkt hier das Rauschen. Größere Voxel glätten die Punktwolke und reduzieren fälschliche Personen-Detektionen (von 7 auf 0), senken aber zugleich die Lkw-Erkennung, weil schräge Fahrzeugflanken in groben Voxeln zusammenfallen. Ähnlich zeigt das Bodensegment \(\textit{distance\_threshold}\), dass Werte oberhalb von \(0{,}15\,\text{m}\) die Gesamtzahl auf das Realniveau (19) stabilisieren, während zu kleine Schwellen (0{,}10\,\text{m}) Objekte in Bodenausreißer zerlegen. Bei der Clusterung trennt eine mittlere \(\textit{cluster\_tolerance}\) von \(0{,}50\,\text{m}\) die Fahrzeuge trotz Steigung am zuverlässigsten und vermeidet zugleich die Übersegmentierung, die bei \(0{,}30\,\text{m}\) zu fünf Fahrrad-Fehldetektionen führt. Insgesamt bestätigen die geneigten Szenen, dass moderat gewählte Parameter die Höhenstreuung abfangen und die Detektionszahlen nahe an den Realwerten halten, ohne relevante Objektstrukturen zu verlieren.

\subsection{Fazit der Parameterstudie}

Die Studie belegt, dass moderate Werte für die Kernparameter die robustesten Ergebnisse liefern: Eine Voxelgröße von \(0{,}15\,\text{m}\) bis \(0{,}20\,\text{m}\) glättet Rauschen, ohne feine Objektstrukturen zu verlieren, \(\textit{distance\_threshold} = 0{,}15\,\text{m}\) verhindert Ausfransungen in geneigten Szenen und \(\textit{cluster\_tolerance}\) um \(0{,}50\,\text{m}\) trennt dicht beieinanderstehende Objekte zuverlässig, ohne sie zu übersegmentieren. Größere oder kleinere Abweichungen führten entweder zu übermäßiger Detektionszahl (Fehlalarme) oder zum Zusammenfallen benachbarter Objekte. Insgesamt bestätigt die Parameterstudie, dass die gewählte Verarbeitungskette mit diesen konservativen Mittelwerten ein stabiles Gleichgewicht zwischen Präzision und Robustheit erreicht. Sie erklärt zugleich, warum die in Tabelle~\ref{tab:benchmark_node_parameter_clean} zusammengefassten Einsatzwerte in die Fahrtests übernommen wurden.

Die Tabelle~\ref{tab:benchmark_node_parameter_clean} fasst die Endparameter zusammen und bildet die Grundlage für den anschließenden Test im Fahrbetrieb.
\begin{table}[H]
  \centering
  \caption{Parameterübersicht der Nodes im \texttt{benchmark.launch.py} für den Test im Fahrbetrieb}
  \label{tab:benchmark_node_parameter_clean}
  \begin{tabular}{lll}
    \toprule
    \textbf{Node} & \textbf{Parameter} & \textbf{Wert} \\
    \midrule

    \texttt{crop\_box\_node\_cpp} 
      & \texttt{min\_bound}  & \([-10.0,\ -6.0,\ -3.0]\) \\
      & \texttt{max\_bound}  & \([10.0,\ 6.0,\ 5.0]\) \\
    \midrule

    \texttt{voxel\_filter\_node\_cpp} 
      & \texttt{voxel\_size} & 0.20 \\
    \midrule

    \texttt{ransac\_ground\_node\_cpp} 
      & \texttt{distance\_threshold} & 0.15 \\
      & \texttt{max\_iterations}    & 1000 \\
    \midrule

    \texttt{cluster\_extraction\_node\_cpp} 
      & \texttt{cluster\_tolerance} & 0.50 \\
      & \texttt{min\_cluster\_size} & 40 \\
      & \texttt{max\_cluster\_size} & 8000 \\
      & \texttt{max\_clusters}      & 200 \\
      & \texttt{bbox\_type}         & AABB \\
    \midrule

    \texttt{tracking\_node\_cpp} 
      & \texttt{gate\_dist\_max} & 4.0 \\
      & \texttt{max\_missed}    & 10 \\
      & \texttt{min\_hits}      & 2 \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Test im Fahrbetrieb}

\subsection{Testumgebung}
\label{sec:testumgebung}

Für die Evaluierung der entwickelten Messkette wurden Tests im Fahrbetrieb gemacht.
Am Testtag herrschten \(-1\,^{\circ}\text{C}\)
mit gefühlten \(-4\,^{\circ}\text{C}\) bei überwiegend bewölktem Himmel.
Ziel dieser Datensätze war es, typische Objekte und Strukturen einer realen Verkehrsumgebung –
wie Personen, Fahrradfahrer, Pkw und Lkw – unter realistischen Bedingungen zu erfassen.
Für die Auswertung im Fahrbetrieb kam dabei der Laptop mit der bestehenden GUI zum Einsatz (\textit{Microsoft Surface Book~3}).

Um reproduzierbare Ergebnisse zu erhalten, wurden fünf klar abgegrenzte Fahrszenarien definiert und jeweils für eine Minute
aufgezeichnet. Jede Sequenz besitzt ein spezifisches Fahrprofil und Prüfziel, sodass Stabilität der Messkette ebenso wie die
Robustheit der Segmentierung und das ID-Handling beobachtet werden konnten:

\begin{enumerate}
    \item \textbf{Gerade Strecke, Stand (0\,km/h):} Referenzlauf ohne Fahrzeugbewegung zur Ermittlung von Grundlatenz und -frequenz.
          Ziel war eine stabile Hz/Latenz, Sensorstabilität, konsistente Boxen sowie ID-Kontinuität.
          
    \item \textbf{Kurve, 40\,km/h (Urban/Innenstadt):} Dynamische Fahrt durch eine urbane Kurve, um die Stabilität von Frequenz
          und Latenz ohne längere Aussetzer zu verifizieren.
          
    \item \textbf{Innenstadt mit Steigung, 30\,km/h:} Leichte Steigungsfahrt in urbanem Umfeld. Prüft, ob die Verarbeitungskette auch bei
          Gefälle-/Steigungslagen konstante Frequenzen und Latenzen liefert.
          
    \item \textbf{Kreuzung, 50\,km/h:} Schnellere Passage über unebenes Terrain, um die Robustheit der
          Verarbeitungskette gegenüber Karosseriebewegungen und vertikalen Lastwechseln zu bewerten.
    \item \textbf{Urbanes Szenario mit unebenem Boden, 50\,km/h:} Fahrt über unebenen Untergrund mit weiteren Verkehrsteilnehmern. Schwerpunkte
          waren stabile Hz/Latenz und eine robuste Bodensegmentierung trotz Unebenheiten sowie bewegter Objekte.
\end{enumerate}

\subsubsection{Gerade Strecke, Stand (0\,km/h):}
\label{gerade_strecke}
Im Szenario \enquote{Gerade Strecke, Stand} zeigte sich ein für rotierende LiDAR-Sensoren
typisches Echtzeitverhalten, das durch mehrere systemtechnische Effekte beeinflusst wurde.
Bei der Wiedergabe der Aufzeichnungen (2025-11-21-VK-OL-001) meldete der \texttt{rosbag2\_player} zeitweise eine
leere Read-Ahead-Queue (\enquote{Message queue starved}). Dies weist auf I/O-Limitierungen
bei der Bag-Wiedergabe hin und kann zu zusätzlichen zeitlichen Schwankungen in den
Eingangsfrequenzen führen, ohne jedoch die Verarbeitungslogik der Verarbeitungskette selbst zu
verändern. Die gemessenen Durchsatzraten des \texttt{frame\_loss\_probe} bestätigten dabei,
dass alle Verarbeitungsknoten die eingehenden Frames zuverlässig verarbeiten
(pass-Rate überwiegend $\geq 100\,\%$) und keine Sequenzlücken entstanden
(\enquote{gaps~=~0}). Die niedrige effektive Eingangsfrequenz der Punktwolke
($\approx 2$--$2{,}5\,\text{Hz}$) ist daher primär auf die Datenquelle bzw.\ die
I/O-Umgebung zurückzuführen und nicht auf eine Überlastung der implementierten
Verarbeitungskette.

Die Tracking-IDs waren im Echtbetrieb dennoch nur eingeschränkt stabil. Aufgrund der
geringen Eingangsfrequenz änderte sich die Clustergeometrie zwischen den einzelnen Frames
deutlich. Der eingesetzte Nearest-Neighbour-Tracker besitzt keine explizite Historie oder
Re-Identifikationslogik und reagiert daher empfindlich auf Geometrieänderungen sowie kleine
Positionssprünge. Dies führte zu häufigen ID-Neuzuweisungen: Der \texttt{ids\_probe}
registrierte bis zu 36–72 Track-Geburten pro Minute, wobei die mediane Lebensdauer
abgeschlossener Tracks bei $0{,}0\,\text{s}$ lag. Nur vereinzelt traten stabile Tracks mit
Lebensdauern im Bereich von $5$--$10\,\text{s}$ auf. Die Trackinginstabilität ist somit eine
direkte Folge der niedrigen Eingangsfrequenz und nicht auf Fehler innerhalb der
Pipeline zurückzuführen.

Zusätzlich war die Rotationsbewegung des Ouster-Sensors in Rviz deutlich sichtbar. Bei
niedrigen Bildraten erscheinen einzelne Lasersektoren zeitlich voneinander getrennt,
wodurch die typische Drehbewegung des rotierenden LiDAR-Bildes wahrnehmbar wird. Dieses
Phänomen ist ein normales physikalisches Verhalten des Sensors und stellt keinen Fehler der
Verarbeitungskette dar.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{Bilder/gerade_strecke_stand.png}
  \caption{Szenario \enquote{Gerade Strecke, Stand}: Links die Frontkameraansicht des Versuchsträgers,
  rechts die LiDAR-Punktwolke mit extrahierten Clustern und Fahrzeug-Track-IDs in der Echtzeitvisualisierung(2025-11-21-Messdaten/2025-11-21-VK-OL-001).}
  \label{fig:gerade_strecke_stand}
\end{figure}

\subsubsection{Kurve, 40\,km/h (Urban/Innenstadt)}

Ziel dieses Tests war die Überprüfung, ob die Verarbeitungskette auch bei Fahrt mit ca.\ 40\,km/h in einer innerstädtischen Kurve eine stabile Datenrate und Latenz ohne längere Aussetzer erreicht.

Die mit \texttt{resource\_probe.py} gemessene mittlere CPU-Auslastung der Verarbeitungsnodes liegt bei insgesamt
\(\approx 31{,}4\,\%\). Der \texttt{crop\_box\_node} weist mit rund \(17{,}9\,\%\) den höchsten Anteil auf, während
\texttt{voxel\_filter\_node}, \texttt{ransac\_ground\_node}, \texttt{cluster\_extraction\_node} und
\texttt{tracking\_node} jeweils unter \(5\,\%\) bleiben. Der durchschnittliche Hauptspeicherbedarf der Kette beträgt
\(\approx 198\,\text{MB}\). Damit liegen CPU- und Speicherlast im unkritischen Bereich, sodass auch bei höheren
Umgebungsanforderungen noch Reserven bestehen.

Das Latenztool \texttt{latency\_probe} ermittelt für die einzelnen Verarbeitungsschritte die folgenden mittleren
Laufzeiten:
\begin{itemize}
  \item \texttt{crop\_box\_node} $\rightarrow$ \texttt{voxel\_filter\_node}: \(14{,}05\,\text{ms}\) (\(n = 146\)),
  \item \texttt{voxel\_filter\_node} $\rightarrow$ \texttt{ransac\_ground\_node}: \(7{,}36\,\text{ms}\) (\(n = 146\)),
  \item \texttt{ransac\_ground\_node} $\rightarrow$ \texttt{cluster\_extraction\_node}: \(3{,}06\,\text{ms}\) (\(n = 145\)),
  \item \texttt{cluster\_extraction\_node} $\rightarrow$ \texttt{tracking\_node}: \(14{,}81\,\text{ms}\) (\(n = 145\)).
\end{itemize}
Für die gesamte Verarbeitungskette vom Eingang der zugeschnittenen Punktwolke bis zur Ausgabe der Tracks ergibt sich
damit eine mittlere End-to-End-Latenz von
\[
  t_{\text{crop}\rightarrow\text{track}} \approx 39{,}14\,\text{ms} \quad (n = 145),
\]
die deutlich unter einem typischen Zielwert von \(100\,\text{ms}\) liegt. Die Verarbeitung kann somit auch im
Fahrszenario \emph{Kurve} als echtzeitfähig eingestuft werden.

Die mittlere Verarbeitungsrate der Punktwolken bzw.\ Zwischenstufen beträgt im Test
\(\approx 3{,}7\,\text{Hz}\). Die gemessenen Perioden liegen zwischen \(0{,}154\,\text{s}\) und \(0{,}793\,\text{s}\)
bei einer Standardabweichung von \(\approx 0{,}095\,\text{s}\), was auf einen überwiegend gleichmäßigen Ablauf ohne
längere Aussetzer hinweist.

Für das Track-Topic (\emph{``/bench/tracks\_raw''}) ergibt sich eine mittlere Ausgaberate von etwa \(1{,}8\,\text{Hz}\).
Die Perioden liegen überwiegend im Bereich von \(\approx 0{,}3\,\text{s}\), erreichen vereinzelt jedoch Werte bis
\(1{,}44\,\text{s}\). Insgesamt bleibt die Track-Ausgabe damit hinreichend stabil, zeigt aber im Vergleich zur
Verarbeitungsrate der Punktwolken gelegentliche längere Intervalle, die auf interne Pufferungen oder
Verarbeitungsjitter im Tracking hinweisen können.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\textwidth]{Bilder/kurve_szene.png}
  \caption{Szenario \emph{Kurve} im urbanen Umfeld: Links die Fisheye-Kameraansicht der Fahrszene, rechts die zugehörige LiDAR-Visualisierung in RViz mit detektierten Clustern und PKW-Tracking-Boxen.((2025-11-21-Messdaten/2025-11-21-VK-OL-002)}
  \label{fig:kurve_rviz_kamera}
\end{figure}


\subsubsection{Innenstadt mit Steigung, 30\,km/h}

Die mit dem \texttt{latency\_probe} aufgezeichneten Messwerte zeigen für die einzelnen Verarbeitungsstufen mittlere Latenzen von etwa
\(21{,}1\,\text{ms}\) (\texttt{crop\_box\_node}~\(\rightarrow\)~\texttt{voxel\_filter\_node}),
\(2{,}7\,\text{ms}\) (\texttt{voxel\_filter\_node}~\(\rightarrow\)~\texttt{ransac\_ground\_node}),
\(0{,}23\,\text{ms}\) (\texttt{ransac\_ground\_node}~\(\rightarrow\)~\texttt{cluster\_extraction\_node}) und
\(6{,}0\,\text{ms}\) (\texttt{cluster\_extraction\_node}~\(\rightarrow\)~\texttt{tracking\_node}).
Die End-to-End-Latenz von \texttt{crop\_box\_node} zu \texttt{tracking\_node} liegt im Mittel bei rund
\(30\,\text{ms}\) und nimmt während der Messung leicht ab, sodass der geforderte Grenzwert von
\(100\,\text{ms}\) mit deutlicher Reserve eingehalten wird.

Der Ressourcenverbrauch der Kette bleibt mit einer durchschnittlichen CPU-Last von
\(\approx 12{,}3\,\%\) und einem Gesamtspeicherbedarf von rund \(200\,\text{MB}\) auf dem Versuchsrechner moderat. Keiner der Nodes sticht als Engpass hervor; die Last verteilt sich im Wesentlichen auf
\texttt{crop\_box\_node}, \texttt{ransac\_ground\_node} und \texttt{cluster\_extraction\_node}.

Die mit \texttt{ros2 topic hz} gemessene Frequenz der Verarbeitungsdaten (z.\,B.\ \texttt{/detections\_raw}) liegt stabil bei etwa \(3{,}3\) bis \(3{,}4\,\text{Hz}\) mit geringen Schwankungen
(Intervalldauer \(\approx 0{,}3\,\text{s}\), maximale beobachtete Lücke \(\approx 0{,}36\,\text{s}\)).
Das Tracking-Topic erreicht nach der Initialisierungsphase eine effektive Aktualisierungsrate von
rund \(2\,\text{Hz}\). Ein in den Messdaten ausgewiesener maximaler Abstand von
\(55{,}5\,\text{s}\) ist als einzelner Start-Ausreißer zu interpretieren und deutet nicht auf wiederkehrende Aussetzer im laufenden Betrieb hin. Insgesamt erfüllt die Verarbeitungskette damit das Ziel einer stabilen Latenz und Datenrate ohne längere Unterbrechungen auch unter innerstädtischen Fahrbedingungen.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\textwidth]{Bilder/steigung_szene.png}
  \caption{Szenario \emph{Steigung}: Links die Fisheye-Kameraansicht einer leicht ansteigenden, engen Straße im urbanen Umfeld; rechts die entsprechende LiDAR-Visualisierung in RViz mit erkannten PKW-Objekten und einem Fahrradfahrer(2025-11-21-Messdaten/2025-11-21-VK-OL-005).}
  \label{fig:steigung_rviz_kamera}
\end{figure}

\subsubsection{Kreuzung, 50\,km/h:}
Die Visualisierung der Daten in diesem Fall ist fehlgeschlagen (2025-11-21-Messdaten/2025-11-21-VK-OL-005). 

Im Szenario \emph{Kreuzung} mit einer Fahrgeschwindigkeit von \(50\,\text{km/h}\) wurde das Verhalten der Verarbeitungskette unter kombinierter Rotation, Fahrbahnunebenheiten und wechselnder Objektgeometrie untersucht. Die Verarbeitungsgeschwindigkeit zeigt über die gesamte Versuchsdauer eine konstante Frequenz von rund \(3.21{-}3.24\,\text{Hz}\). Gleichzeitig nimmt die Standardabweichung der Intervallzeiten kontinuierlich ab (von ca.\ \(0.072\,\text{s}\) auf \(0.057\,\text{s}\)), was auf eine zunehmende Stabilisierung der Verarbeitungskette trotz dynamischer Fahrsituation hinweist.

Die gemessenen Latenzen der einzelnen Verarbeitungsschritte bleiben ebenfalls über den gesamten Versuch sehr stabil. Die Hauptanteile entfallen erwartungsgemäß auf die Kombination aus \texttt{CropBox} und \texttt{VoxelGrid}, die im Bereich von \(15.4{-}17.3\,\text{ms}\) liegen. Die folgenden Schritte --- RANSAC-Bodensegmentierung und Cluster-Extraktion --- weisen jeweils sehr geringe Laufzeiten auf (unter \(2\,\text{ms}\) bzw.\ \(0.5\,\text{ms}\)). Die resultierende End-zu-End-Latenz zwischen Eingangspunktwolke und Tracking-Output beträgt konstant \(21{-}24\,\text{ms}\), was für die getestete Geschwindigkeit ein robustes Reaktionsverhalten sicherstellt.

Bei den Tracking-Outputs (\texttt{tracks\_raw}) treten kurzzeitige Frequenzabfälle sowie erhöhte Varianzen auf, die zeitlich mit Bodenwellen und Steigungen zusammenfallen. Diese Effekte sind plausibel auf temporäre Punktwolkenverzerrungen zurückzuführen, welche die Stabilität der Clusterbildung beeinflussen. Der schnelle Wiederanstieg der Frequenz zeigt jedoch, dass die Trackingkomponente insgesamt reibungsfrei weiterarbeiten kann und keine nachhaltige Instabilität entsteht.

Eine Ausgabe von \texttt{resource\_probe.py}  konnte wegen eines zu früh gekillten Prozesses nicht gespeichert werden.

\subsubsection{Urbanes Szenario mit unebenem Boden, 50\,km/h}
\label{sec:urban_50kmh}

Der Test im urbanen Umfeld bei etwa \(50\,\text{km/h}\) diente der Überprüfung der Robustheit der Bodensegmentierung unter realistischen Bedingungen mit Fahrbahnunebenheiten. Die Messung der Ressourcen mittels \texttt{resource\_probe.py} zeigte eine sehr geringe Gesamtauslastung der Verarbeitungskette: Die CPU-Last aller Knoten lag im Mittel bei lediglich \(11{,}7\,\%\), der gesamte Speicherbedarf bei rund \(200\,\text{MB}\). Einzelne Knoten wie \texttt{crop\_box\_node} oder \texttt{cluster\_extraction\_node} erreichten jeweils nur wenige Prozentpunkte Auslastung, sodass ausreichende Leistungsreserven bestehen.

Die Latenzmessungen mit \texttt{latency\_probe.py} verdeutlichen, dass die Ende-zu-Ende-Verarbeitung selbst in diesem dynamischen Szenario stabil bleibt. Die Verzögerung von \texttt{crop\_box} bis \texttt{tracking} bewegte sich im stationären Zustand im Bereich von etwa \(20\)–\(26\,\text{ms}\). Zwar steigen die Latenzen einzelner Schritte wie \texttt{ransac\_ground} oder \texttt{cluster\_extraction}, wenn durch unebene Fahrbahn oder zusätzliche Objekte mehr Punktdaten verarbeitet werden müssen, jedoch bleiben diese Effekte moderat und beeinträchtigen die Gesamtlatenz nicht kritisch.

Auch die Topic-Frequenzen zeigten ein stabiles Verhalten. Die Eingangsfrequenz des Sensors \texttt{/ouster/points} pendelte sich auf etwa \(3{,}2\,\text{Hz}\) ein, wobei die Standardabweichung im Zeitverlauf abnahm und somit eine zunehmende Stabilität der Datenrate belegte. Die Ausgangsfrequenz der Tracks (\texttt{/bench/tracks\_raw}) lag im Mittel bei rund \(3{,}0\,\text{Hz}\). Größere maximale Intervalle sind hier durch das Publikationsverhalten bedingt: \texttt{tracks\_raw} sendet nur dann Nachrichten, wenn tatsächlich verfolgte Objekte vorhanden sind, sodass Phasen ohne erkannte Ziele als lange Intervalle in der Frequenzmessung erscheinen.

Insgesamt zeigt dieses Szenario, dass die Bodensegmentierung und die gesamte Verarbeitungskette auch bei Fahrbahnunebenheiten und im urbanen Umfeld robust und performant arbeiten. Weder die Latenzen noch die Datenraten weisen kritische Schwankungen auf, sodass die Anforderungen an eine stabile, echtzeitfähige Verarbeitung erfüllt werden.

Die Abbildung zeigt die Robustheit der Verarbeitungskette bei realen Straßenprofilen und variierender Bodenhöhe.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{Bilder/urban_unebener_boden.png}
    \caption{Urbanes Szenario mit unebenem Boden: Links die Fisheye-Kameraansicht einer städtischen Straße mit leicht unregelmäßiger Fahrbahnoberfläche; rechts die entsprechende LiDAR-Visualisierung in RViz mit detektierten PKW-Clustern.(2025-11-21-Messdaten/2025-11-21-VK-OL-007)}
    \label{fig:urban_unebener_boden}
\end{figure}


Die im Fahrbetrieb gemessenen Kenngrößen zu Ressourcennutzung, Latenz sowie Eingangs- und Ausgangsfrequenzen der Verarbeitungskette sind in Tabelle~\ref{tab:fahrbetrieb_ueberblick} zusammengefasst.

\begin{table}[H]
  \centering
  \footnotesize
  \setlength{\tabcolsep}{4pt}
  \renewcommand{\arraystretch}{0.95}
  \begin{tabular}{|p{3.4cm}|p{3.1cm}|p{2.1cm}|p{2.1cm}|p{2.1cm}|}
    \hline
    \textbf{Szenario} & \textbf{CPU / RSS} & \textbf{mittlere Latenz} & \textbf{f\_in} & \textbf{f\_out} \\
    \hline

    Gerade Strecke, Stand &
    32\,\% / 197\,MB &
    86\,ms (stabil 81\,ms) &
    2.5\,Hz &
    1.8\,Hz \\
    \hline

    Kurve, 40\,km/h (Urban) &
    31.4\,\% / 198\,MB &
    39\,ms &
    3.7\,Hz &
    1.8\,Hz \\
    \hline

    Innenstadt + Steigung, 30\,km/h &
    12.3\,\% / 200\,MB &
    30\,ms &
    3.3--3.4\,Hz &
    2\,Hz \\
    \hline

    Kreuzung, 50\,km/h &
    n.\,a. &
    21--24\,ms &
    3.21--3.24\,Hz &
    kurze Einbrüche bis \(\approx 3{,}15\,\text{Hz}\)  \\
    \hline

    Urban, 50\,km/h (Unebene Fläche) &
    11.7\,\% / 200\,MB &
    20--26\,ms &
    3.2\,Hz &
    3.0\,Hz \\
    \hline
  \end{tabular}
  \caption{Vergleichende Übersicht der Messgrößen je Szenario}
  \label{tab:fahrbetrieb_ueberblick}
\end{table}

Die Übersicht verdeutlicht, dass die gewählte Parametrierung im Fahrbetrieb zu stabilen Verarbeitungsketten führt:
Selbst in dynamischen Szenarien wie Kurvenfahrten oder innerstädtischen Abschnitten bleibt die mittlere Latenz deutlich
unter 90\,ms, während die Ausgangsfrequenz nur moderat hinter der Eingangsfrequenz zurückbleibt. Auffällige Abweichungen
treten lediglich im Kreuzungsszenario auf, wo kurzzeitige Frequenzeinbrüche beobachtet wurden. Insgesamt zeigen die
Messwerte, dass die Verarbeitungskette auch unter variierenden Umweltbedingungen performant arbeitet und genügend Reserven für
weitergehende Funktionalitäten bietet.

\section{Bewertung des Algorithmus}
Die in Kapitel~\ref{chap:systemarchitektur} formulierten Anforderungen an den Algorithmus (Tabelle~\ref{tab:anforderungen_messkette}) lassen sich anhand der Messergebnisse weitgehend bestätigen. Im Folgenden werden die zentralen Kriterien zusammengefasst und den Beobachtungen aus den Fahrversuchen gegenübergestellt:
\begin{itemize}
  \item \textbf{Echtzeitfähigkeit (Anforderung~6):} Die gemessenen End-to-End-Latenzen liegen mit \(20{-}40\,\text{ms}\) deutlich unter der Zielmarke von \(100\,\text{ms}\). Selbst im ungünstigsten Szenario (Stillstand) bleibt die mittlere Latenz stabil bei rund \(81\,\text{ms}\), sodass die vorgegebene Ausführungsrate von \(10\,\text{Hz}\) eingehalten werden kann.
  \item \textbf{Ressourceneffizienz und Stabilität (Anforderungen~7~\&~8):} Die CPU-Auslastung bleibt über alle Szenarien unter \(35\,\%\), der Speicherbedarf stabilisiert sich bei etwa \(200\,\text{MB}\). Damit werden die Grenzen für Standardhardware klar unterschritten und es besteht Reserve für zusätzliche Funktionen oder Debug-Ausgaben.
  \item \textbf{Datenkonsistenz und Robustheit (Anforderungen~1--5):} Die Topics \texttt{/ouster/points}, \texttt{/detections\_raw} und \texttt{/tracks\_raw} werden ohne Paketverluste im Bereich \(3{-}3{,}7\,\text{Hz}\) publiziert, die Track-IDs bleiben in Bewegungsszenarien über mehrere Frames stabil. Downsampling und Bodensegmentierung erfüllen die geforderten Restbodenanteile (\(<5\,\%\)) und erhalten ausreichend Stützpunkte für robuste Bounding-Boxes, sodass die geometrische Genauigkeit der Objektrepräsentation gewahrt bleibt.
  \item \textbf{Visualisierung und Integration (Anforderungen~9~\&~10):} Rohdaten, Bodenfilter, Cluster und Tracks lassen sich in \texttt{RViz2} sowie der bestehenden GUI ohne spürbare Verzögerung darstellen. Die Verarbeitung bleibt auch bei zwei Betriebsmodi (Sensor, Sensor~+~PC) stabil, und muss bei einem verbessert werden.
\end{itemize}

Insgesamt erfüllt die entwickelte Verarbeitungskette die definierten funktionalen und nicht-funktionalen Anforderungen: Sie arbeitet echtzeitfähig, ressourcenschonend und robust gegenüber wechselnden Umgebungsbedingungen. Optimierungspotenzial besteht vor allem im Feintuning der Tracking-Schwellen, um kurzlebige Track-Geburten in statischen Szenen weiter zu reduzieren, ohne die Reaktionsfähigkeit in dynamischen Situationen zu beeinträchtigen.

% ====== 8 Zusammenfassung und Ausblick ======
\chapter{Zusammenfassung und Ausblick}
\section{Zusammenfassung}
Die vorliegende Arbeit entwickelte eine \ac{LiDAR}-basierte Verarbeitungskette, die alle Schritte von der Vorverarbeitung bis zur
Objektverfolgung integriert. Ausgehend von einer \ac{ROS}~2-basierten Systemarchitektur wurden zunächst robuste Verfahren zur
Normalisierung und Filterung der Punktwolken aufgebaut, die eine verlässliche Grundlage für nachgelagerte Algorithmen schaffen.
Darauf aufbauend ermöglicht die Bodensegmentierung eine stabile Trennung zwischen Fahrbahn- und Objektpunkten, während die
Clustering-Verfahren mit datengetriebenen Bounding-Boxen eine kompakte und interpretierbare Objektrepräsentation liefern.
Die anschließende Verfolgung stellt durch geeignete Datenassoziation und modellbasierte Prädiktion sicher, dass auch bei
temporären Sensorausfällen oder Abschattungen konsistente Trajektorien berechnet werden. In umfangreichen Versuchsfahrten
bestätigte die Verarbeitungskette ihre Echtzeitfähigkeit: Die Verarbeitungslatenzen blieben im typischen Fahrbetrieb deutlich unter
der Eingangstaktung des Sensors, sodass ausreichend Reserven für zusätzliche Funktionen bestehen. Insgesamt zeigt die Arbeit,
dass die Kombination aus klassischen Verfahren und sorgfältiger Parametrierung eine zuverlässige 360°-Umfelderfassung mit
hochaufgelösten \ac{LiDAR}-Daten ermöglicht.
\section{Ausblick}
Die erzielten Ergebnisse legen mehrere Ansatzpunkte für weiterführende Arbeiten nahe. Kurzfristig bietet sich eine adaptive
Parametrierung der Filter- und Segmentierungsstufen an, um die Verarbeitungskette dynamisch an unterschiedliche Wetter- oder
Verkehrssituationen anzupassen. Perspektivisch könnten probabilistische Filter oder graphbasierte Optimierungsverfahren die
Tracking-Genauigkeit in komplexen Szenen erhöhen und robuste Reidentifikation ermöglichen. Darüber hinaus lässt sich durch
Sensorfusion mit Kamera- oder Radardaten sowohl die semantische als auch die geometrische Konsistenz steigern, etwa über
gemeinsame Objektmerkmale oder \ac{BEV}-Repräsentationen. Ergänzend sollte geprüft werden, wie \ac{SLAM}-Verfahren auf Basis der
\ac{LiDAR}-Daten eine lokale Karte aufbauen und die Eigenbewegung präzisieren können, um die Objekttrajektorien in ein konsistentes
Weltkoordinatensystem einzubetten. Für eine spätere Integration in hochautomatisierte Fahrfunktionen sind zusätzliche
Safety-Mechanismen wie Zustandsüberwachung, Plausibilitätsprüfungen und Fallback-Strategien wünschenswert. Eine kontinuierliche
Evaluation mit erweiterten Datensätzen und realen Fahrversuchen wird entscheidend sein, um die Übertragbarkeit auf
unterschiedliche Fahrzeugplattformen und Einsatzgebiete zu sichern und das volle Potenzial der \ac{LiDAR}-basierten
Umfelderfassung auszuschöpfen.

% ====== Literaturverzeichnis ======
\cleardoublepage
\printbibliography[title=Literaturverzeichnis]

% ====== Anhang ======
\chapter*{Digitaler Anhang}
\label{chap:digitaler_anhang}
\addcontentsline{toc}{chapter}{Digitaler Anhang}

Alle im Rahmen dieser Arbeit verwendeten Codes und Skripte sind im
institutsinternen Dateisystem des Labors Fahrzeugtechnik abgelegt.
Der digitale Anhang befindet sich unter folgendem Basispfad:

\begin{quote}
\textit{V:/Fak\_MBVS/Projekte/Labor\_Fahrzeugtechnik/
FAS\_Studenten\_Austausch/FAS\_Sensorik\_am\_Astra/BA\_Nodem\_Ouster/}
\end{quote}

\section*{1. Implementierter ROS-2-Code}
\label{anhang:code}

Ablageort:

\begin{quote}
\textit{V:/Fak\_MBVS/Projekte/Labor\_Fahrzeugtechnik/
FAS\_Studenten\_Austausch/FAS\_Sensorik\_am\_Astra/BA\_Nodem\_Ouster/
Ouster\_OS1/ros2\_ws/src/ouster\_cpp/src/}
\end{quote}

Enthalten sind:
\begin{itemize}
  \item \texttt{crop\_box\_node.cpp}
  \item \texttt{voxel\_filter\_node.cpp}
  \item \texttt{ransac\_ground\_node.cpp}
  \item \texttt{cluster\_extraction\_node.cpp}
  \item \texttt{tracking\_node.cpp}
  \item \texttt{CMakeLists.txt}, \texttt{package.xml}
  \item verwendete Launch-Dateien (z.\,B. \texttt{benchmark.launch.py} sowie das Ouster-Launch-File)
\end{itemize}

\section*{2. Skripte zur Messung und Analyse}
\label{anhang:skripte}

Ablageort:

\begin{quote}
\textit{V:/Fak\_MBVS/Projekte/Labor\_Fahrzeugtechnik/
FAS\_Studenten\_Austausch/FAS\_Sensorik\_am\_Astra/BA\_Nodem\_Ouster/
Ouster\_OS1/ros2\_ws/src/ouster\_cpp/scripts/}
\end{quote}

Enthalten sind:
\begin{itemize}
  \item \texttt{frame\_loss\_probe.py}
  \item \texttt{ids\_probe.py}
  \item \texttt{latency\_probe.py}
  \item \texttt{resource\_probe.py}
\end{itemize}

\section*{3. Messaufzeichnungen}
\label{anhang:messaufzeichnungen}

Die vollständigen Messdaten der Fahrversuche (\texttt{rosbag2}) werden zentral im Labor
Fahrzeugtechnik gespeichert.

Ablageort der Rohdaten:

\begin{quote}
\textit{V:/Fak\_MBVS/Projekte/Labor\_Fahrzeugtechnik/Messdaten\_Astra/}
\end{quote}

Relevante Versuchsordner:
\begin{itemize}
  \item \texttt{2025-10-15-Messdaten}
  \item \texttt{2025-11-21-Messdaten}
\end{itemize}


\label{LastPage}
\end{document}