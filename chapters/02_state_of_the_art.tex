% ====== 2 Wissenschaftliche Grundlagen ======
\chapter{Stand der Technik und Wissenschaft}
\label{chap:stand-der-technik}

Moderne Fahrerassistenzsysteme und autonome Fahrfunktionen kombinieren Kamera-, Radar- und \ac{LiDAR}-Sensorik. Kameras liefern hochauflösende Bildinformationen für semantische Aufgaben, Radar überzeugt durch robuste Distanz- und Geschwindigkeitsmessungen bei widrigen Wetterbedingungen. \ac{LiDAR} ergänzt diese Systeme um präzise, dreidimensionale Punktwolken mit hoher Winkelauflösung, die auch bei wechselnden Lichtverhältnissen konstant verfügbar sind \parencite{beuth2021handbuch}. Dadurch entsteht ein Sensorverbund, der semantische, dynamische und geometrische Informationen gleichermaßen abdeckt.

Das Akronym \emph{\ac{LiDAR}} beschreibt optische Messverfahren, die Lichtimpulse aussenden, deren Laufzeit bis zur Reflexion erfassen und daraus Entfernungen sowie Strukturen in drei Dimensionen rekonstruieren \parencite{mcmanamon2015fieldguide}. Nach ersten Anwendungen in Geodäsie und Fernerkundung seit den 1960er-Jahren ermöglichte der Fortschritt in Lasertechnik und Miniaturisierung die Übertragung in die Automobilindustrie, wo \ac{LiDAR} heute eine Schlüsselrolle für hochautomatisierte Funktionen einnimmt \parencite{beuth2021handbuch}. Im automobilen Sensorsystemverbund liefert \ac{LiDAR} die geometrische Präzision, mit der Kamera- und Radarinformationen redundant oder ergänzend genutzt werden können.

Damit ist \ac{LiDAR} insbesondere für Hinderniserkennung, Bahnplanung und robuste Umfeldrepräsentation wertvoll. Der folgende Abschnitt beschreibt den Sensoraufbau, anschließend werden Wellenlängenbereiche und Messprinzip erläutert. Diese Bausteine bilden die experimentelle Grundlage der Ouster-Plattform, auf der die Arbeit aufsetzt. Dieses Kapitel stellt die wissenschaftlichen Grundlagen der \ac{LiDAR}-Sensorik zusammen, verortet sie im automobilen Kontext und leitet daraus die für die Verarbeitungskette relevanten Anforderungen ab. Im Mittelpunkt stehen die Messprinzipien, der historische Entwicklungspfad, die Abgrenzung zu Kamera und Radar sowie der Aufbau des in dieser Arbeit genutzten Sensors.

\section{Sensoraufbau}
\label{sec:sensoraufbau}

Ein \ac{LiDAR}-Sensor umfasst Sendeoptik und Strahlablenkung, Empfangsoptik und Lichtsensor sowie Auslese- und Steuerungselektronik (vgl. Abb.~\ref{fig:sensoraufbau}). Die nachfolgenden Abschnitte skizzieren den Beitrag der jeweiligen Komponenten zum Gesamtsystem.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{Bilder/Sensoraufbau.png}
    \caption{Schematischer Aufbau eines \ac{LiDAR}-Sensors mit Sendeoptik, 
    Empfangsoptik, Strahlablenkung sowie Steuer- und Auswerteeinheit (\cite{beuth2021handbuch}).}
    \label{fig:sensoraufbau}
\end{figure}

Vom Ausgang der Laserquelle gelangt das Licht zunächst in die Sendeoptik. 
Diese formt das Intensitätsprofil des Strahls und bestimmt somit indirekt 
die erreichbare räumliche Auflösung des Sensors. Häufig werden refraktive 
Linsen oder diffraktive optische Elemente (DOE) eingesetzt, welche die 
gewünschte Strahlform erzeugen. Für rasternde Systeme übernehmen Spiegel 
oder Prismen die Strahlablenkung durch rotierende oder oszillierende 
Bewegungen. Im Gegensatz dazu nutzen sogenannte Flash-Systeme DOEs oder 
Diffusoren, um die Szene pro Aufnahme vollständig auszuleuchten (\cite{mcmanamon2015fieldguide}).

Die Strahlablenkung erzeugt die Winkelinformation und legt fest, welcher Raumwinkel zu einem bestimmten Zeitpunkt erfasst wird. Sie ermöglicht die sukzessive Abtastung der Umgebung und beeinflusst mit ihrer Genauigkeit und Synchronisation direkt die geometrische Konsistenz der resultierenden Punktwolke (\cite{schuldt2020phd}).

Die Empfangsoptik übernimmt die Aufgabe, das vom Zielobjekt diffus oder spiegelnd 
zurückgestreute Licht möglichst verlustarm einzusammeln und dem Detektorsystem 
zuzuführen. Entscheidend dafür sind \emph{Aperturgröße} und \emph{Transmission}, 
die gemäß der \ac{LiDAR}-Gleichung direkt auf die empfangene Strahlungsleistung wirken 
und damit die Reichweite sowie die Detektierbarkeit schwacher Signale bestimmen. 
Um Reflexionsverluste zu reduzieren, werden optische Komponenten mit 
Antireflexbeschichtungen (AR-Coatings) versehen. Ergänzend sorgen 
sperrbandselektive Filter -- deren Durchlässigkeit exakt auf die Laserwellenlänge 
abgestimmt ist -- für eine wirksame Unterdrückung von Umgebungslicht und erhöhen 
somit die Robustheit gegenüber Sonnenlicht (\cite{wolter2021optoelektronik}).

Als Detektoren kommen in automobilen \ac{LiDAR}-Systemen typischerweise Avalanche-Photodioden (APD) oder Single-Photon-Avalanche-Diodes (SPAD-Arrays) zum Einsatz. APDs bieten durch ihren internen Lawineneffekt eine hohe Verstärkung und eignen sich zur Detektion schwacher Rückstreusignale bei gleichzeitig guter zeitlicher Auflösung. SPAD-Arrays erfassen einzelne Photonen und erreichen extrem kurze Anstiegszeiten, wodurch sie eine sehr präzise Laufzeitmessung unterstützen. Diese Eigenschaften sind entscheidend für die Genauigkeit des Time-of-Flight-Prinzips (vgl. Kap.~\ref{sec:dTOF} und ~\ref{sec:iTOF}) und wirken sich direkt auf die präzise Rekonstruktion der Objektentfernung aus (\cite{lemkin2020spad}).

Die vom Detektor erzeugten elektrischen Signale werden anschließend durch die 
Ausleseelektronik verarbeitet. Diese umfasst Verstärker, Analog-Digital-Wandler 
(ADC), Zeit-zu-Digital-Wandler (TDC) und Multiplexer. Ihre Aufgabe besteht darin, 
die Signale zu digitalisieren und die Laufzeitinformationen für jeden 
Messpunkt zu bestimmen. Moderne Systeme berechnen bereits auf Sensorebene 
Distanzwerte und können weitergehende Vorverarbeitungen wie die Bildung von 
Objektboxen oder erste Klassifikationen durchführen (\cite{behroozpour2017adc}).

Eine zentrale Rolle spielt die Steuerungselektronik, welche die Synchronisation 
aller Komponenten sicherstellt. Sie sorgt dafür, dass der Detektor nur während 
des Aussendens eines Laserpulses aktiv ist, um unnötige Rauschsignale durch 
Umgebungslicht zu vermeiden. Ebenso steuert sie die Strahlablenkung, sodass 
jeder Messpunkt dem korrekten Raumwinkel zugeordnet werden kann (\cite{beuth2021handbuch}).

Der \ac{LiDAR}-Sensoraufbau vereint optische, elektronische und algorithmische 
Bausteine zu einem Gesamtsystem, das die präzise Abbildung der Fahrzeugumgebung 
in Echtzeit ermöglicht (\cite{schuldt2020phd}).

\section{Wellenlängenbereiche}
\label{sec:wellenlaenge}
Zur Umsetzung dieses Prinzips werden überwiegend Laser im nahen Infrarotbereich
(NIR) eingesetzt, typischerweise mit Wellenlängen zwischen $870\,\text{nm}$ und
$950\,\text{nm}$. Dieser Bereich ist besonders geeignet, da die spektrale
Strahlungsintensität des Sonnenlichts dort vergleichsweise gering ist und somit
ein günstiges Signal-Rausch-Verhältnis erzielt werden kann. Gleichzeitig stehen in diesem Wellenlängenbereich sowohl kosteneffiziente Halbleiterlaser als auch empfindliche Siliziumdetektoren zur Verfügung. Für spezielle Anwendungen werden
auch Wellenlängen um $1064\,\text{nm}$ oder $1550\,\text{nm}$ eingesetzt, da hier
höhere Laserleistungen innerhalb der Augensicherheitsgrenzen möglich sind. Diese
erfordern jedoch alternative Detektormaterialien, was den technischen Aufwand und
die Kosten erhöht(\cite{beuth2021handbuch}).

Abbildung~\ref{fig:spektrum} zeigt das Spektrum der elektromagnetischen Strahlung mit den für LiDAR-Systeme relevanten Wellenlängenbereichen.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Bilder/Spektrum der elektromagnetischen Strahlung.png}
    \caption{Spektrum der elektromagnetischen Strahlung(\cite{beuth2021handbuch})}
    \label{fig:spektrum}
\end{figure}

\section{Messprinzip}
\label{sec:messprinzip}

\subsection{Direct Time‑of‑Flight (dToF)}
\label{sec:dTOF}

Das Messprinzip moderner \ac{LiDAR}-Systeme basiert überwiegend auf dem 
\emph{Time-of-Flight} (\ac{ToF})-Verfahren, das sich aufgrund seiner Robustheit und 
hohen Genauigkeit als Standard etabliert hat. Beim \ac{ToF}-Verfahren wird ein kurzer 
Laserimpuls ausgesendet, der sich mit Lichtgeschwindigkeit 
$c \approx 3 \cdot 10^{8}\,\text{m/s}$ ausbreitet, von einem Objekt reflektiert wird und schließlich den Empfänger erreicht. Aus der gemessenen Zeitdifferenz $\Delta t$ zwischen Emission und Detektion ergibt sich die Entfernung $d$ nach folgender Gleichung:

\begin{equation}
    d = \frac{c \cdot \Delta t}{2}
\end{equation}

Der Faktor $\tfrac{1}{2}$ berücksichtigt, dass der Lichtimpuls den Weg zweimal zurücklegt – vom Sender zum Objekt und wieder zurück zum Empfänger.

Wie in Abbildung~\ref{fig:dtof} zu sehen, wird das dToF-Messprinzip durch die direkte Messung der Lichtlaufzeit realisiert.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Bilder/Schematisches Beispiel eines direct ToF.png}
    \caption{Schematisches Beispiel eines direct Time-Of-Flight (dToF) \parencite{beuth2021handbuch}
}
    \label{fig:dtof}
\end{figure}

\subsection{Indirect \ac{ToF} (iToF)}
\label{sec:iTOF}

Beim iToF-Messprinzip wird die Entfernung nicht über die direkte Messung der Lichtlaufzeit bestimmt, sondern indirekt über die Veränderung eines kontinuierlich modulierten ausgesendeten Signals. Die zurückkehrende Reflexion weist abhängig von der Distanz charakteristische Änderungen auf, aus denen die Entfernung rekonstruiert werden kann. In der Praxis kommen hierfür hauptsächlich \emph{Amplitudenmodulationen} und \emph{Frequenzmodulationen} zum Einsatz \parencite{beuth2021handbuch}.

\subsubsection{AMCW (Amplitude-Modulated Continuous Wave)}

Bei einem AMCW‑\ac{LiDAR} wird ein kontinuierlich ausgesendeter Lichtstrahl mit einer Amplitudenmodulation versehen, typischerweise sinus‑ oder rechteckförmig.  Das vom Objekt reflektierte Licht weist gegenüber dem ausgesendeten Signal eine Phasenverschiebung~$\Delta\phi$ auf, die proportional zur Laufzeit und damit zur Entfernung ist.  Die Distanz lässt sich aus der gemessenen Phasenverschiebung bestimmen.  Für die Entfernung $d$ ergibt sich in Abhängigkeit von der Modulationsfrequenz~$f_\mathrm{mod}$ des sinusförmig modulierten Strahls
\begin{equation}
    d = \frac{c}{4\pi f_\mathrm{mod}}\, \Delta\phi\,,
\end{equation}
wobei $c$ die Lichtgeschwindigkeit bezeichnet.  Die maximale eindeutige Messdistanz (unambiguous range) ist durch die Periodizität des Modulationssignals begrenzt und beträgt
\begin{equation}
    d_\text{max} = \frac{c}{2 f_\mathrm{mod}}\,,
\end{equation}
so dass eine eindeutige Zuordnung der gemessenen Phasenverschiebung nur innerhalb dieser Entfernung möglich ist.  Höhere Modulationsfrequenzen verbessern zwar die Tiefenauflösung, verkürzen aber die eindeutige Reichweite proportional dazu \parencite{yi2023digital}.

Abbildung~\ref{fig:amcw_schema} zeigt das schematische Prinzip eines amplitudenmodulierten LiDAR-Systems.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Bilder/Schema_AMCW_LiDAR.png}
    \caption{Schema eines amplitudenmodulierten LiDARs (AMCW) \parencite{beuth2021handbuch}.}
    \label{fig:amcw_schema}
\end{figure}

AMCW‑Systeme weisen als Verfahren mehrere Vorteile auf.  Die benötigte Hardware ist vergleichsweise einfach und kostengünstig zu realisieren, was hochintegrierte Sensordesigns ermöglicht.  Durch den kontinuierlichen Betrieb können niedrigere Spitzenleistungen genutzt werden, und bei kurzen Messdistanzen wird eine hohe Tiefenauflösung erreicht.  Aus diesen Gründen sind AMCW‑Sensoren in industriellen 3D‑Kameras, Smartphones und mobilen Endgeräten weit verbreitet \parencite{yi2023digital}.

Dem stehen charakteristische Nachteile gegenüber.  Die periodische Modulation führt zu einer Entfernungsambiguität, da eindeutige Distanzmessungen nur bis zur oben genannten unambiguous range möglich sind.  Zudem reagieren AMCW‑Systeme empfindlich auf Multipath‑Effekte: Mehrfachreflexionen unterschiedlicher Laufzeit überlagern sich und verfälschen die gemessene Phase, was insbesondere bei glänzenden Materialien oder in Innenraumumgebungen problematisch ist.  Weitere Fehlerquellen entstehen durch Phasenverkettung (2\,$\pi$‑Ambiguität), Alias‑Effekte höherer Harmonischer und Bewegungsartefakte, die durch die mehrfache Abtastung pro Bild entstehen \parencite{whyte2015application}.

Insgesamt eignet sich das AMCW‑Verfahren vor allem für kostensensitive und kompakte Sensorsysteme mit begrenzten Messdistanzen.  Seine Stärken liegen in der einfachen Realisierbarkeit und der hohen Präzision auf kurzen Entfernungen, während Nachteile wie begrenzte eindeutige Reichweite und Empfindlichkeit gegenüber Mehrwegeffekten den Einsatz in komplexen oder stark reflektierenden Umgebungen einschränken \parencite{yi2023digital}.

\subsubsection{FMCW (Frequency Modulated Continuous Wave)}
\label{sec:fmcw_lidar}

Ein FMCW‑\ac{LiDAR} nutzt kontinuierlich abgestrahlte Laserstrahlung, deren Frequenz zeitlich über einen sogenannten \emph{Chirp} linear variiert wird. Eine schematische Darstellung des grundlegenden Messprinzips ist in Abb.~\ref{fig:fmcw} gezeigt. Durch die Überlagerung des ausgesandten und des vom Objekt reflektierten Signals entsteht ein niederfrequentes \emph{Beat‑Signal}, dessen Frequenz proportional zur Laufzeitdifferenz ist. Aus diesem Beat‑Signal lässt sich die Entfernung mit hoher Präzision bestimmen\parencite{pinto2025fundamentals}.

Im Gegensatz zu klassischen Pulssystemen erlaubt FMCW zudem die gleichzeitige Ermittlung der Relativgeschwindigkeit: Eine durch die Objektbewegung verursachte Doppler‑Verschiebung verändert die Beat‑Frequenz, sodass Distanz‑ und Geschwindigkeitsinformation in einem einzigen Messvorgang separierbar sind. Dies führt zu einer besonders robusten Bewegungsanalyse, selbst bei schwachen Reflexionen oder geringem Signal‑Rausch‑Verhältnis\parencite{piggott2022physics}.

Ein zentraler Vorteil dieser Technik ist die hohe Störrobustheit gegenüber Fremdlicht und anderen \ac{LiDAR}‑Quellen. Da FMCW auf kohärenter Detektion basiert, werden ausschließlich Signale mit identischer Modulationscharakteristik ausgewertet. Dadurch können selbst sehr schwache Rückstreusignale zuverlässig erfasst werden, was die Performance bei schlechten Sichtbedingungen verbessert. Zudem ermöglicht die phasenbasierte Auswertung eine hohe Entfernungsauflösung bis in den Millimeterbereich \parencite{piggott2022immunity}.

Demgegenüber erfordert FMCW‑\ac{LiDAR} eine aufwändigere opto‑elektronische Architektur, einschließlich laserfrequenzstabiler Quellen, präziser Modulation, schneller Detektoren und leistungsfähiger digitaler Signalverarbeitung. Diese Anforderungen führen aktuell zu höheren Herstellungskosten sowie erhöhtem Rechenaufwand in der Echtzeit‑Auswertung \parencite{bianconi2025challenges}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Bilder/fmcw.png}
    \caption{Schematisches Messprinzip eines FMCW‑LiDARs (\cite{beuth2021handbuch}).}
    \label{fig:fmcw}
\end{figure}

\subsection{Fehlerquellen}
\label{sec:fehlerquellen}

Die Leistungsfähigkeit optischer \ac{LiDAR}-Sensoren wird von mehreren systematischen und stochastischen Fehlerquellen beeinflusst. Eine zentrale Rolle spielen Schwankungen in der Zeitmessung (\emph{Zeitjitter}) innerhalb der TDC- bzw. ADC-Elektronik, die direkt die Laufzeitbestimmung und damit die Distanzauflösung beeinträchtigen \parencite{rapp2020lidar}. Weitere typische Fehlerursachen sind \emph{Mehrwegeffekte (Multipath)}, bei denen reflektiertes Licht über indirekte Wege zum Detektor gelangt und so zu verfälschten oder doppelten Entfernungswerten führt \parencite{glennie2013multichannel}. 

Auch der Einfallswinkel des Laserstrahls sowie die optische Reflektivität der Oberfläche wirken sich auf die Signalstärke aus: Flache Winkel oder dunkle, absorbierende Materialien reduzieren die Signalenergie und erhöhen damit das Rauschen bzw. die Wahrscheinlichkeit von Fehlmessungen \cite{beuth2021handbuch}. Zusätzlich begrenzt die Strahldivergenz des Lasers die räumliche Auflösung, da der Lichtfleck mit zunehmender Entfernung größer wird und Reflexionen von benachbarten Objekten überlagert werden können.

Neben optischen Einflüssen spielen auch umwelt- und systembedingte Faktoren eine Rolle. temperaturbedingte Messabweichungen im Lasersystem oder in der Empfangselektronik können den Messoffset verschieben oder die Verstärkung verändern. Mechanische Komponenten – insbesondere bei rotierenden LiDARs – unterliegen Toleranzen und Vibrationen, beispielsweise durch Schwankungen der Rotordrehzahl, was zu nichtlinearen Abtastwinkeln oder zeitlichen Versätzen führt (\cite{levinson2011towards}). Diese Effekte wirken sich unmittelbar auf Reichweite, Präzision und Datenkonsistenz aus und können nachfolgende Verarbeitungsschritte wie Segmentierung, Objekterkennung oder Klassifikation beeinträchtigen.

Um in sicherheitsrelevanten Anwendungen robuste Umfeldmodelle zu gewährleisten, sind daher geeignete Maßnahmen zur Signalkonditionierung, Filterung und Fehlerkompensation notwendig. Dazu gehören Rauschreduktion, adaptive Schwellenwertverfahren, geometrische Konsistenzprüfungen sowie modellbasierte Korrekturen, um messtechnische Artefakte zu minimieren und Fehlinterpretationen im Gesamtsystem zu vermeiden.

\section{Typische Anwendungsszenarien im Fahrzeugumfeld}
\label{sec:anwendung}

\ac{LiDAR}-Sensoren spielen im Fahrzeugumfeld eine zentrale Rolle bei klassischen Fahrerassistenzsystemen (ADAS). Ein wichtiger Anwendungsbereich ist die \emph{Freiraum- und Belegtheitsdetektion}, bei der die Punktwolken genutzt werden, um befahrbare Bereiche zu bestimmen und Hindernisse im Nah- und Fernbereich präzise zu lokalisieren. Dies bildet die Grundlage für Funktionen wie automatische Notbremsung (AEB), Kollisionsvermeidung, Abstandsregelung sowie Querführung in niedrigen Automatisierungsstufen \parencite{Arnold2019Survey}.

Für höher automatisierte Fahrfunktionen (Level~3–4) dient \ac{LiDAR} als wesentliche Komponente zur \emph{3D-Umfeldmodellierung}. Dabei werden aus den Punktwolken statische Elemente der Infrastruktur (z.\,B. Leitplanken, Begrenzungslinien, geparkte Fahrzeuge) sowie dynamische Verkehrsteilnehmer wie Fahrzeuge, Radfahrer oder Fußgänger extrahiert und klassifiziert. Die hohe geometrische Auflösung von \ac{LiDAR} ermöglicht die Erstellung präziser Objektlisten, Konturen und Bewegungsmodelle, die von der Trajektorienplanung und Entscheidungslogik weiterverarbeitet werden \cite{beuth2021handbuch}.

Ein weiterer wichtiger Anwendungsbereich liegt in der \emph{Lokalisierung und Kartenerstellung (Mapping)}. \ac{LiDAR}-basierte \ac{SLAM}-Verfahren (Simultaneous Localization and Mapping) und Scan-Matching-Algorithmen nutzen die 3D-Punktwolken, um die Fahrzeugpose kontinuierlich zu schätzen und lokale Karten der Umgebung aufzubauen. Dies ist insbesondere in unbekannten oder unstrukturierten Gebieten relevant und dient als Ergänzung zu GPS, Inertialsensorik und Kamera \cite{cadena2016slam}.

Darüber hinaus wird \ac{LiDAR} in komplexen und dynamischen Verkehrsszenarien eingesetzt, etwa im \emph{Baustellen- und Engstellenmanagement}. Dort ermöglicht die präzise 3D-Geometrie das Erkennen veränderter Fahrkorridore, temporärer Barrieren, verschwenkter Fahrspuren oder unstrukturierter Objektumgebungen. Diese Information bildet die Grundlage für robuste Pfadplanung, sichere Trajektoriengenerierung und automatisierte Spurwahl \cite{levinson2011towards}.

Schließlich unterstützt \ac{LiDAR} Funktionen der \emph{Situations- und Kontextinterpretation}, etwa die Analyse von Verkehrsfluss, Abstandsdynamiken, Manövererkennung anderer Verkehrsteilnehmer sowie die Ableitung semantischer Strukturen der Umgebung (z.\,B. Fahrbahnränder, drivable space, Objekttypen). Die detaillierten 3D-Daten dienen dabei als hochpräzise geometrische Referenzschicht innerhalb der Sensorsuite \cite{mcauliffe2019scaling}.

\section{Algorithmischer Stand der Technik}
\label{sec:stand-der-technik}

Die algorithmische Verarbeitung hochauflösender 3D-\ac{LiDAR}-Daten folgt in der Literatur einem weitgehend etablierten Verarbeitungskettenmodell, das aus Vorverarbeitung, Bodensegmentierung, Segmentierung (Clustering), Merkmalsextraktion sowie Objektverfolgung besteht. Jedes Modul adressiert dabei spezifische Herausforderungen roher Punktwolken wie Rauschen, variable Punktdichten, Mehrwegeffekte oder dynamische Verkehrsszenen. Im Folgenden werden die einzelnen Bausteine detailliert beschrieben und jeweils mit relevanter Literatur untermauert.

% =====================================================================
\subsection{Vorverarbeitung}
\label{subsec:vorverarbeitung}

Die Vorverarbeitung dient der Reduktion der Datenmenge, der Stabilisierung der Punktwolke sowie der Vorbereitung der Daten für die nachfolgenden Module. Häufig eingesetzte Schritte umfassen:
\paragraph{Downsampling}
Ein verbreitetes Verfahren zur kontrollierten Reduktion der Punktanzahl ist das \emph{VoxelGrid}-Downsampling. Die Punktwolke wird in dreidimensionale Voxel unterteilt, wobei alle Punkte eines Voxels durch ihren Schwerpunkt ersetzt werden. Dadurch wird die räumliche Auflösung gezielt skaliert, ohne wesentliche geometrische Strukturen zu verlieren \parencite{RusuCousins2011}.

\paragraph{Ausreißerentfernung}
Zur Rauschreduktion werden in der Praxis der \emph{Statistical Outlier Removal (SOR)} sowie der \emph{Radius Outlier Removal (ROR)} eingesetzt.
SOR entfernt Punkte, deren mittlere Distanz zu ihren Nachbarn stark vom Erwartungswert abweicht, während ROR Punkte eliminiert, die innerhalb eines definierten Radius zu wenige Nachbarn besitzen \parencite{RusuCousins2011}. In der vorliegenden Implementierung wurde auf beide Verfahren verzichtet, weil die Punktwolken des Ouster-Sensors im Testumfeld ausreichend stabil waren und die zusätzliche Rechenzeit nicht gerechtfertigt erschien.

\paragraph{Bewegungskompensation (Deskewing)}
Da \ac{LiDAR}-Sensoren Punktwolken zeilenweise aufnehmen, entstehen bei Fahrzeugbewegungen Verzerrungen. Durch Deskewing unter Verwendung von IMU- oder Odometrie-Daten werden zeitbezogene Positionsfehler korrigiert \parencite{Behley2018}.

\paragraph{Intensitätsnormalisierung}
Die Intensität eines Punktes wird durch Materialeigenschaften und Auftreffwinkel beeinflusst. Eine Distanz- und Winkelkorrektur verbessert die Clusterqualität sowie nachfolgende Klassifikationsschritte \parencite{LevinsonThrun2011}.

\paragraph{Koordinatentransformationen}
Für bestimmte Verfahren werden die Punktwolken in Polarkoordinaten oder \emph{Bird’s-Eye-View}-Raster überführt, beispielsweise für scanline-basierte Segmentierer oder CNN-Detektoren \parencite{Lang2019PointPillars}.

\paragraph{Mehrfachechos}
Moderne LiDARs bieten mehrere Echos pro Laserpuls. Das erste Echo eignet sich zur Bodenerkennung, spätere Echos unterstützen die Erkennung hinter teiltransparenten Strukturen wie Vegetation \parencite{Glennie2010}.

% =====================================================================
\subsection{Bodensegmentierung}
\label{subsec:bodenseg}

Die Trennung von Boden und darüberliegenden Objekten ist ein entscheidender Schritt zur Reduktion des Suchraums. Zu den wichtigsten Verfahren gehören:

\paragraph{\ac{RANSAC}-Ebene}
Eine globale Bodenebene kann mittels \ac{RANSAC} robust geschätzt werden. Das Verfahren ist tolerant gegenüber Ausreißern, stößt jedoch bei geneigten Szenen oder unebenem Gelände an Grenzen \parencite{FischlerBolles1981}.

\paragraph{Progressive Morphological Filter (PMF)}
Der PMF nutzt morphologische Operationen auf einer Höhenkarte und wurde ursprünglich für Airborne-\ac{LiDAR} entwickelt. Er ist besonders robust gegenüber hügeligem Gelände \parencite{Zhang2003PMF}.

\paragraph{Grid-basierte Höhenkarten}
Hierbei wird die Punktwolke in ein zweidimensionales Raster projiziert, aus dem pro Zelle statistische Höhenmerkmale extrahiert werden. Diese Methode ist effizient und gut für Echtzeitsysteme geeignet.

\paragraph{Cloth Simulation Filtering (CSF)}
Ein virtuelles Tuch wird auf die invertierte Punktwolke “fallen gelassen”, wodurch die Bodenform approximiert wird. CSF hat sich als robust gegenüber Vegetation und komplexen Geländeformen erwiesen \parencite{Zhang2016CSF}.

\paragraph{Probabilistische Modelle}
Gaussian-Process-Modelle modellieren lokale Höhenverteilungen probabilistisch und sind besonders geeignet für urbane Szenarien mit Bordsteinen \parencite{Vetter2017}.

% =====================================================================
\subsection{Clustering und Segmentierung}
\label{subsec:clustering}

Nach Entfernung des Bodens wird die verbleibende Punktwolke segmentiert. Wichtige Verfahren sind:

\paragraph{Euklidische Clusterung}
Das klassische \ac{PCL}-Verfahren gruppiert Punkte, die innerhalb eines Distanzschwellwertes verbunden sind. Diese Methode ist einfach und schnell, reagiert jedoch empfindlich auf variable Punktdichten.

\paragraph{\ac{DBSCAN} und HDBSCAN}
Dichtebasierte Verfahren erkennen Cluster beliebiger Form und klassifizieren isolierte Punkte als Rauschen. \ac{DBSCAN} ist in vielen \ac{LiDAR}-Verarbeitungsketten Standard \parencite{Ester1996DBSCAN}.  
HDBSCAN erweitert das Konzept um hierarchische Dichteanalysen und ist robuster gegenüber Dichteunterschieden.

\paragraph{Region Growing}
Unter Verwendung lokaler Normale und Krümmungen segmentiert Region Growing größere, glatte Flächen. Dies ist besonders nützlich für Infrastruktur und Fassaden.

% =====================================================================
\subsection{Detektion und Klassifikation}
\label{subsec:detektion}

Ziel dieses Moduls ist die Identifikation potenzieller Objekte.

\paragraph{Geometrische Merkmale}
Aus den Clustern werden typische Merkmale wie Achslängen, PCA-Formfaktoren, Punktdichte oder orientierte Bounding Boxes extrahiert. Diese Verfahren benötigen keine Trainingsdaten und sind für Echtzeit geeignet \parencite{Douillard2011}.

\paragraph{Lernbasierte 3D-Detektoren}
Moderne Methoden wie \emph{PointPillars}, \emph{SECOND} oder \emph{CenterPoint} repräsentieren Punktwolken als \ac{BEV}-Raster und nutzen CNNs, um Fahrzeug-, Fußgänger- und Fahrradklassen zu erkennen \parencite{Lang2019PointPillars}.  
Diese Verfahren liefern hohe Genauigkeiten, erfordern jedoch umfangreiche Trainingsdaten und GPU-Ressourcen.

% =====================================================================
\subsection{Tracking}
\label{subsec:tracking}

Bewegte Objekte werden über mehrere Zeitschritte verfolgt. Dafür sind zwei Teilkomponenten notwendig:

\paragraph{Bewegungsmodelle}
Mehrere Filterverfahren kommen zum Einsatz:
\begin{itemize}
    \item Kalman-Filter (\ac{KF}) für lineare Bewegungsmodelle,
    \item Extended Kalman Filter (EKF) für leicht nichtlineare Modelle,
    \item Unscented Kalman Filter (UKF) für stärker nichtlineare Bewegungen.
\end{itemize}
Sie modellieren Objektzustände wie Position und Geschwindigkeit \parencite{WelchBishop1995}.

\paragraph{Datenassoziation}
Um neue Detektionen bestehenden Objekten zuzuordnen, kommen Verfahren wie \emph{Nearest Neighbor}, \emph{Joint Probabilistic Data Association (JPDAF)} oder \emph{Multiple Hypothesis Tracking (MHT)} zum Einsatz.  
In vielen automotive-nahen Arbeiten wird der \emph{Ungarische Algorithmus} zur optimalen Zuordnung verwendet \parencite{Kuhn1955Hungarian}.

\paragraph{Track-Management}
Typische Regeln umfassen:
\begin{itemize}
    \item Erzeugung neuer Tracks ab stabiler Beobachtung,
    \item Löschen nicht bestätigter Tracks,
    \item Lebenszeitkriterien zur Validität.
\end{itemize}
Dies verhindert spontane Fehldetektionen und stabilisiert die Szeneinterpretation.

\section{Ouster OS1 am Opel Astra der TH Nürnberg}

\subsection{Spezifische Eigenschaften des Ouster OS1}
\label{sec:eigenschaften}

Der \emph{Ouster OS1} (vgl. Abbildung~\ref{fig:ouster_os1}) ist ein rotationssymmetrischer 360°-\ac{LiDAR}, der eine hochauflösende dreidimensionale Erfassung der Umgebung ermöglicht. Die im Projekt verwendete Variante verfügt über eine vertikale Auflösung von 128 Kanälen. In der maximalen Konfiguration erreicht der Sensor eine horizontale Auflösung von 2{,}048 Abtastungen pro Umdrehung, was zu einer Punktwolke mit bis zu 262\,144 Messpunkten pro Scan führt.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Bilder/Ouster.png}
    \caption{Ouster OS1 \parencite{sensalyticsRetailAnalytics}}
    \label{fig:ouster_os1}
\end{figure}

In der derzeitigen Konfiguration sind 1{,}024 horizontale Winkelkanäle aktiv, sodass pro Umdrehung rund 131\,072 Datenpunkte generiert werden. Über das Webinterface des Sensors lassen sich sämtliche relevanten Parameter flexibel anpassen, darunter die horizontale Auflösung (512, 1{,}024 oder 2{,}048 Kanäle) sowie die Framerate (10 bis 20\,Hz). Die Datenübertragung erfolgt über eine Gigabit-Ethernet-Schnittstelle, über die sowohl die UDP-Punktwolkendaten als auch Konfigurationsbefehle übertragen werden \parencite{Ouster2024}.

Weitere technische Details zur Integration und Konfiguration des Sensors sind ausführlich in \textcite{Sagdic2025} dokumentiert.


\begin{table}[htb]
    \centering
    \label{tab:ouster_os1_specs}
    \begin{tabular}{p{0.35\textwidth}p{0.55\textwidth}}
        \hline
        \textbf{Hersteller:}              & Ouster Inc. \\ 
        \textbf{Gerätebezeichnung:}       & OS1 Mid-Range Imaging \ac{LiDAR} \\ 
        \textbf{Gewicht:}                 & ca.\ 0{,}50 kg (mit Gehäusekappe) \\ 
        \textbf{Abmaße (Ø x H):}          & 87 mm $\times$ 74{,}2 mm \\ 
        \textbf{Wellenlänge Laser:}       & 865 nm \\ 
        \textbf{Framerate:}               & 10 Hz oder 20 Hz (konfigurierbar) \\ 
        \textbf{Reichweite (80 \% Reflektivität):} 
                                          & 170 m bei $> 90\,\%$ Positivdetektion bei 100 klx Sonnenlicht \\ 
        \textbf{Reichweite (10 \% Reflektivität):} 
                                          & 90 m bei $> 90\,\%$ Positivdetektion bei 100 klx Sonnenlicht \\ 
        \textbf{Fehler Distanzbestimmung:}& typ. $\pm 2{,}5$ cm (Lambert-Ziel), bis $\pm 5$ cm (retroreflektiv) \\ 
        \textbf{Horizontales FoV:}        & $360^\circ$ \\ 
        \textbf{Vertikales FoV:}          & $42{,}4^\circ \pm 1^\circ$ \\ 
        \textbf{Horizontale Auflösung:}   & 512, 1\,024 oder 2\,048 Winkelabtastungen pro Umdrehung \\ 
        \textbf{Vertikale Auflösung:}     & 128 Kanäle \\ 
        \textbf{Punktzahl pro Sekunde:}   & bis zu 5{,}24 Mio.\ Punkte/s (128 Kanäle) \\ 
        \textbf{Betriebsspannung:}        & 9{,}5 V bis 51 V DC \\ 
        \textbf{Leistungsaufnahme:}       & ca.\ 16 W (nominal, bis 28 W beim Kaltstart) \\ 
        \textbf{Datenschnittstelle:}      & UDP über Gigabit-Ethernet (1000BASE-T / 1000BASE-T1) \\ 
        \hline
    \end{tabular}
    \caption{Spezifikationen \ac{LiDAR}-Sensor \emph{Ouster OS1}}
\end{table}


\subsection{Sensorpositionierung am Opel Astra}
Der im Rahmen dieser Arbeit verwendete Ouster OS1 ist auf dem Versuchsträger Opel Astra des Instituts montiert. Der Sensor, laut \textcite{Sagdic2025}, wurde mittig auf einer Trägerstruktur über der Fahrzeugdachkante positioniert. Diese erhöhte Montage ermöglicht ein möglichst großes 360°-Sichtfeld ohne Abschattungen durch Fahrzeugkarosserie oder Anbauteile. Die Ausrichtung des Sensors gewährleistet, dass sowohl Bereiche vor dem Fahrzeug als auch seitliche und rückwärtige Zonen erfasst werden, was für die spätere Bodenfilterung, Clusterbildung und Objektverfolgung essenziell ist. Die Abbildung~\ref{fig:sensorposition_astra} zeigt die Montageposition auf dem Fahrzeug im Labor des IFZN mit Abmessungen.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.83\textwidth]{Bilder/sensorposition_astra.png}
    \caption{Montageposition des Ouster OS1 auf dem Opel Astra mit Abmessungen (\textcite{Wendel2025}).}
    \label{fig:sensorposition_astra}
\end{figure}

Abbildung~\ref{fig:position_sensor} veranschaulicht den Versuchsträger Opel Astra sowie die auf der Trägerstruktur montierte Position des Ouster OS1.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.83\textwidth]{Bilder/position_sensor.png}
    \caption{Montageposition des Ouster OS1 auf dem Opel Astra}
    \label{fig:position_sensor}
\end{figure}

\section{Zusammenfassung}
Der Ouster OS1 erweist sich aufgrund seiner hohen Auflösung, der konfigurierbaren Scan-Muster und der robusten digitalen Schnittstelle als geeigneter Sensor für die geplante Umfelderfassung. Seine technischen Eigenschaften, insbesondere die präzise Distanzmessung und die stabile Punktwolkenstruktur, bilden eine solide Grundlage für die nachfolgenden algorithmischen Schritte.

Im nächsten Kapitel wird die darauf aufbauende Systemarchitektur vorgestellt, die den Sensor in eine modulare ROS~2-Verarbeitungskette integriert und die technische Basis für die Implementierung legt.